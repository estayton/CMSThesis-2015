\chapter{Narratives and Counternarratives}
\label{chap:1}

%% was sources \& ideologies
%% 2. Where is that coming from?
%% -Sci-fi and historical dreama
%% -a cultural vision of automation of work
%% -AI history
%% -NHTSA/SAE policy/standards (instantiated in)
%% -researchers in the field!

%Reconsider numbers in subsection headings


%%1) science fiction; feels disconnected from the meat of the chapter
%%could be introductory to whole thesis
%%or may need more depth MOVED;

%%2) designed dreams needs more work; 
%%signal connection: designers thinking about science fiction
%%possibly remove designed dreams from title

%%3) stronger punch/framing MY argument in the historical overview
%% around p30 ``the narrative of the automation of work''. . .

%%4) more to define what is at stake in engineering standards/policy
%%docs; why I quibble with their definitions
%% I may be able to do this drawing on the other subsections
%%as a CONCLUSION, wrapping up the other three things
%%and what is LOST/OVERLOOKED; the ability to deal with nuance and
%%REAL ACTIVITY
%%other part of an analysis of this kind of stuff: did it for a
%%reason; what is the work that it does (reactionary thing in some
%%cases; pushes for a particular image of what this can be)
%%--visions of human; SAE history of thinking as engineers; people as
%%predictable black-boxed inputs and outputs
%%--NHTSA as reactionary, as trying to cope with what Industry says;
%%and as history of dealing with people and responsibility in
%%particularly binary ways; REAL PRESSURE to get it OUT THERE to guide
%%development
%%maybe split into different sub-headers with context like NHTSA:
%%Agency Trying to Catch Up or w/e

%%5) not a good conclusion to the chapter so far; should have a sense
%%we have gotten somewhere and learned something

%%=====================================


% STAGE DIRECTION: if the development history is often forgotten, the
% imagined through-line of self-driving vehicles has other sources and
% other histories; AI, cousins in Air/Space, factory automation

%% The
%% reasons for why these stories take a particular form may be innumerable
%% and difficult to pin down in general.

%% . But while each,
%% in its turn, impacts the popular narrative in specific and important
%% ways,

The popular narratives about self-driving vehicles have not appeared
out of nowhere. But what are their roots? From what histories, fields,
and ideas about technology do these stories come? Journalistic
accounts of technological change are of 
course impacted by a wide variety of practices and perspectives, from
the market and readership needs of news organizations to a pervasive
culture of commodity scientism \cite{smithSelling}. But a number of
specific histories and
fields influence the way driverless cars are figured in the press as
autonomous machines: that the only image of the technology that bears
investigation is that of fully self-driving vehicles, an imagined
technological peak. Factory automation history,
artificial intelligence and robotics, and automation experience in
air and space each impact the way automated vehicles are
conceptualized, but the dominant narratives of automation ignore the finer
points of these histories. A fuller exploration of them is necessary to
answer the question:  Why does vehicle
automation appear, at least on the surface, in the way that it does?
And what relationship does this bear to actual history?


%% But some has to do, instead, with other historical images
%% that are part and parcel of
%% popular autonomous vehicle narratives, ones that are not connected
%% with vehicles at all. 

%% 1.1) deep history is the mechanization/automation of work (4 p)
%% --going back to factories/IRev: replacing human competencies with
%% machine competencies and moving the people into new roles
%% --which engages old debates about the role of the human

Human technological progress since antiquity has
involved continual re-negotiations of human labor and the roles of
animals and mechanisms in the labor process. Due to a confluence
of factors---the miniaturization of computing technology, new
advances in machine learning and artificial intelligence algorithms, a
gradual increase in battery capacities, faster wireless networks---the
horizons for everyday automation are broader now than ever before. But
though media focus is on the future, our past is deeply involved 
in its presentation. News articles fret
about what will happen when no one knows how to drive manually any
more \cite{pross}, a classic fear of ``de-skilling'' that is implicated in so many
other implementations of computers. Coexistence with
autonomous or automated systems is sometimes presented as a fundamentally new
situation, as if human beings had never before had to work and live
with and next to automated systems---at the same time, robotic cars
are sometimes situated as 
the next step for robots after the factory, their final emergence into
the real world having conquered the factory floor.\footnote{See for
  example ``Robot Vehicles'' in \emph{RobotWorx} \cite{robotworx}, which describes automated cars as having the sensors
  industrial robots have had for many years.} 
Automation already has a deep 
history in the industrial sector, presenting new benefits and
dangers, and requiring new roles for human laborers. Current debates
and fears about de-skilling, human jobs, 
and the role and value of human labor return us to questions that have
plagued factories, and labor's relationship to machinery, since the
early Industrial Revolution. 

%% But as we have already seen with the common
%% elision of the actual research history of self-driving vehicles, the
%% stories that get told about driverless cars and factory automation are
%% primarily those rooted in the cultural consciousness, rather than in
%% nuanced, factual histories.

\section{The Automation of Work}

%% One of the commonly referenced histories of automation is that of the
%% textile mills of the early 1800s, in which a large
%% proportion of hand-labor was replaced by steam- or
%% water-powered industrial machinery, and which has entered our collective
%% consciousness.
%% there are even lessons to be
%% learned by the research community. This past 

%% \footnote{The process both increased efficiency and
%%   decreased the costs 
%% of production, so much so that the same basic machinery is still in
%% use today in some smaller milling operations \cite{wyegrist}.}

The
story of early 1800s textile mills is a familiar one \cite{pewPositive}: skilled artisans made obsolete by the
lower cost and higher productive capacity of mechanical labor. By
analogy, new types of skilled labor (taxi and limousine drivers, or
even bus drivers) are now under threat, and their fate should be no
different than that of the workers who came before. But just what the
processes of standardization, mechanization, and automation (or
``automatization'' as the cyberneticists referred to it) have done to
the factory, and to laborers in it, is 
not clearly understood among many who write about autonomous cars. This
elided history---which is substituted for by a non-existent person-less
factory fabricated by the collective imagination---is relevant, perhaps
more than ever, to the future of transportation.

A search for the beginnings of industrial automation takes us to the middle of
the 18th century: Vaucanson's mechanical loom dates to 1741, and formed the basis of
 later developments in weaving by Joseph Marie Jacquard \cite[p. 9]{dieboldImpact}. 
But the first example of ``complete'' industrial automation originating in the
United States does not come until Oliver Evans's work in the 1780s on
automated grist mills \cite[p. 5]{roesmithYankee}. Through a series of elevators and descenders,
horizontal screws, spreaders and rakes, his mill moved grain from raw
agricultural commodity to finished product: sifted flour. And ideally,
all parts of the process would occur without human
intervention.\footnote{Evans cited a reduction in
  labor and expense of ``fully one-half,'' which is not the same as
  its complete elimination. Behind his concession to manual switching
  of machines lies a great volume of labor that is excluded
  from the traditional narrative: switching machines on and off;
  tending and configuring the machines during their operation; examining machines for
  wear, degradation or failure; fixing the machines when they break
  down \cite{evansMillguide}.}
Evans's innovation was to place these devices 
in succession so as to allow continuous production, and the
elimination of many slow human jobs that degraded the quality of the
product by tracking dirt and contaminants around inside the
mill \cite[p. 203]{evansMillguide}. 
%None of his individual
%% inventions\footnote {He lists these as the elevator, ``conveyer,'' hopper-boy,
%% drill, and descender in his 1795 miller's guide.} was a particularly
%% groundbreaking achievement, but w

%% and the mill may find its
%% closest cousins in the ``automatic'' factories of the 1950s and 1960s
%% and the roboticized factories of today,

It took some time for the automation found in the Evans
mill to spread across other industries, but Evans's contemporaries
were not uninterested in increasing efficiency and output.\footnote{Paul
Revere, one of America's early industrialists, applied shifts in
manufacturing techniques to transition himself from an
artisan worker to manager and overseer of others over his long
metalworking career \cite[p. 187]{martello}.} Manufacturing itself was a site of public
debate, pitted against the ``inherent virtue'' of agricultural pursuits.
Tench Coxe, a political economist, wrote in 1810 that ``new machines
and power sources allowed even greater productivity with less labor,
further underscoring the connection between technology and republican
virtue'' \cite[p. 217]{martello}. To Coxe's romantic view, these machines  worked ``as if they
were animated beings, endowed with all the talents of their inventors,
laboring with organs that never tire, and subject to no expense of
food, or bed, or raiment, or dwelling'' \cite[p. xxv]{coxe}. Though we
may have lost this romanticism, we haven't lost this perceived
animism. Automated machines, like self-driving cars, continue to
excite, impress, and cause fear due to this transformative, if
mechanistic, aliveness.

But these romantic words did not represent the whole reality of
industrial machine labor. Human labor of
maintenance and supervision is implicit in these manufacturing
machines---even the Evans Mill---but it is rendered invisible by the
rhetoric that the 
machines themselves require no bed or board. At the same time, Coxe's
use of the word ``endowed'' should focus our attention on exactly which
``talents'' of the inventors have been automatized, and the human
labor necessarily involved in that conferring of capabilities. 

%% the ``lean
%%   production'' techniques largely responsible for industrial
%%   efficiency of Japanese automotive companies are not primarily about
%%   automation. Though automation is involved,

%%TODO (DONE): Article about expected impacts in UK, includes lots of jobs and
%%predictions by 2030:
%%
%footnote it

%% Nash's
%%  barrel-turning machine in part mechanized the production of barrels
%%  of standardized dimensions: it consisted of a lathe on a wooden
%%  frame, with human-operated props to hold the barrel in
%%  place \cite[p. 119]{roesmithHarpers}. The worker also had to continually
%%  measure the barrel with a caliper, and adjust the device's chisel
%%  appropriately \cite[p. 121]{roesmithHarpers}. This gradual implementation of
%%  mechanized labor was continued in further machines produced by Hall.
%%  His straight-cutting machine, an early version of a milling machine,
%%  had as its distinctive feature the ability to be

%% : while the ``American system'' of interchangeable
%% parts perhaps drove increases in mechanization via the tight
%% tolerances necessary for this production 
%% method, armorers and managers resisted the
%% mechanization of their craft.

%% \footnote{Blanchard's many automatic
%%   machines for making gunstocks were of particular
%%   importance \cite[p. 56]{roesmithHarpers} but most
%%  machines still required significant human labor, even if only tending
%%  by ``common
%%  hands'' \cite[p. 239]{roesmithHarpers}.}


Driven by the plight of those displaced---like Detroit
autoworkers who faced unemployment with the rise of
Japanese (popularly read as ``highly automated'') automotive
might\footnote{It is worth mentioning here that the core of
  \emph{kaizen} ``lean'' manufacturing is not automation but increased trust between management
  and labor, assisted by continual communications and high job
  security \cite[p. 198--199]{nyeAmericas}.}---the history of
automation that gets mobilized is one of a teleological progression
toward complete automation of all sectors of work. Actual historical 
circumstances are less important, popularly, than the general
perception that automated 
systems, such as the robotic arms used for painting and assembly, are
reducing the overall pool of available jobs.\footnote{A report produced for the Society of Motor
Manufacturers and Traders in the UK suggests an alternative: automated
cars could be  
expected to create 320,000 new jobs in the United Kingdom by 2030, only 25,000 of
which would be in automotive manufacturing directly
\cite{toveyCreate}, which has everything to do with new labor involved in their
development, production, testing, and use.}
Modern automotive manufacturing is not alone in presenting conflicting ideas
of what automation can do. Military manufacturing and assembly line
history is rife with contradictions. While certain competencies were
transferred from the skilled worker
to the technical apparatus, human oversight and operation were still
integral to the production of weaponry using the new technology of the
``American system'' of interchangeable parts. And it was
not clear until after the fact that more mechanization was necessarily
better.\footnote{For example, Harper's Ferry, where armorers resisted the
  mechanization of their craft, remained ``competitive'' with
costs at the more highly automated Springfield Armory through the mid
1830s \cite[p. 324]{roesmithHarpers}.} The same exchange of
competencies characterized Ford's assembly line, which began
true mass production in America \cite[p. 217]{hounshell}. Fixtures
and gauges were designed to allow for use by unskilled machine
tenders.
 But while the gauge simplifies the
assurance  of quality, it does not
automate it:  it simply changes the effort from a more complex judgment
of quality and measurement to a simpler one.\footnote{As Donald
Norman writes of this fundamental principle: ``the world remembers things for us, just by being
there'' \cite[p. 147]{normanThings}.} Ford also attracted a wide
variety of well-educated skilled mechanics 
to his automobile plants \cite[p. 223]{hounshell}. Like Evans, Blanchard, Hall, and others
before them \cite{roesmithHarpers}, these mechanics applied their skills to design machines,
and simplify and standardize work processes. The individual judgment
of the assembly line laborer was displaced into standardized tools and
fixtures, built into these technologies by the labor of skilled
machinists and designers. But the new labor generated by automation
has often
been made structurally invisible.

%While he instituted the
%five-dollar day to attempt to solve labor problems at the factory, and to
%compensate laborers for becoming part of the ``production machine,''

Also around the turn of the century, Taylorism in factories created ``new managerial
functions'' performed by ``new classes of people with new titles and
more clearly specified responsibilities'' \cite[p. 120]{aitken}. A focus on the
people---who are they? where are they? what are they doing?---shows that
one of the fundamental and enduring characteristics of Taylor's
system, the expansion of management roles and the further division of
labor, is not about automation but about new and altered
types of human work: industry continued
the removal of strategic decision-making from the
workers most physically engaged in product production, installing it
instead within formal organizational structures and the managers that
constituted them. 

%% Numerical control (NC),
%% developed in the 1940s and 1950s as an outgrowth of World War II
%% research into feedback systems, slowly began to produce industrial
%% robots that could perform factory tasks without direct human
%% intervention.

%% , in his 1950 book \emph{The Human Use of
%% Human Beings},
%% , which
%% would be mechanized
%% ``Only Automation Plant in the World'' in 1953 (

Machines ``coupled to the
outer world through the mechanical equivalents of sense organs''
allowed the the Rockford
Ordinance Plant to operate in 1953 with a ``largely automatic process of
turning steel bars into 155 millimetre
shells'' \cite{wienerMachineThreat}. Around the same time, in Ford
Motor Company's Brooks Park engine plants, near Cleveland, forty-two
automatic machines ``linked together by transfer devices that
automatically move the blocks through the complete process, perform
530 precision cutting and drilling operations'' \cite[p.
  9]{dieboldImpact}: through 1,545 feet of assembly line, no human
touched the parts. Such is the contemporary vision of the automated factory. In
truth an operator stood by each machine, ensuring its continued
operation. This human labor, menial as it may be, is not often
recognized.\footnote{One worker described his experience: ``I don't do nothing but
press those two buttons . . . Sometimes I use my thumbs, sometimes I
use my wrists and sometimes I lay my whole arm across'' \cite[p.
  10]{dieboldImpact}. And yet,
despite the meniality of his labor, it is still integral to the
process of production.} The worker
no longer makes choices about how to bore a part---choices now built into the industrial equipment 
she oversees---but chooses to turn the machine on and off.%% Instead, he monitors the state of the line and chooses to
%% turn the machine on or off.

%% , while automating tasks,

Patterns of contingent change repeat for
numerical control (NC): adoption of robots to replace assembly line jobs such
as spray painting and welding was gradual, with only
about 6,000 robots in use in American factories by the mid
1970s \cite[p. 159]{nyeAmericas}. Industrial robots
had a way of generating large contingents of skilled human laborers
who still needed to be paid for their services:  to tend them, and
to repair them when the broke down.\footnote{These early experiments did not
increase profits because of the volume of highly skilled labor needed
to keep the machines operating \cite[p. 162]{nyeAmericas}.} The development of NC machines
proceeded with a specific interest in eliminating skilled workers, but
the jobs that disappeared were largely unskilled or semi-skilled
laborers \cite[p. 164]{nyeAmericas}. And while Norbert Weiner
prophesied in 1950 the end of ``deadly uninteresting'' jobs within 20
years, such changes have still not 
totally come to pass \cite[p. 161]{nyeAmericas}. To compound the problem, new industries of
skilled workers---record-and-playback machine designers, and NC machine
programmers---sprang up to furnish factories with their tools. This
historical thread should focus our attention on what is added, rather
than removed, by
autonomous vehicles: more complex computer systems may make the driving
task simpler for a given level of safety, but make the system
engineering task, and the tasks of maintenance and repair, more and more complicated.

%% Norbert Wiener, the famous cyberneticist, was asked to contribute to an
%% anniversary supplement on automatic machinery for the \emph{St. Louis
%% Post-Dispatch} in 1953. The piece that he wrote, ``The Machine as Threat
%% and Promise'' \cite{wienerMachineThreat} hailed the coming of the
%% robotic factory with his usual 
%% caution and good sense, but confirmed that ``automatic machinery of a
%% new sort is assuredly here to stay,'' 

%% : ICAM
%% attempted to aid shop floor automation by automating certain 
%% the creative work that the management had attempted to
%% place in their own hands, in the first place, through earlier
%% processes of rationalization \cite[p. 330]{nobleForces}.

As control is further constituted within management, the roles of
management may be rendered more and more menial themselves: the Air
Force's Integrated Computer-Aided Manufacturing program (ICAM)
attempted to automate
management functions, ``to try to reduce the enormous indirect costs
that have resulted from the effort to reduce labor costs and remove
power and judgment from the shop floor,'' costs that have continued to
dog new rationalization strategies \cite[p. 330]{nobleForces}. Automation
can be used both to routinize work---for the manual laborer---and to
eliminate the routine in favor of the creative---for managers and newly
generated classes of creative workers.\footnote{ICAM, like the mythical
ouroboros, sought to offer automation as the ``solution to the problems
generated by automation,'' providing automated scheduling functions,
inventory control, and design tools to ``provide better management
control'' and ``free management from excessive routine duties to do
creative work'' \cite[p. 330]{nobleForces}.} This is analogous to the
current process of self-driving vehicle design: automated systems
substitute the mechanical process of controlling the vehicle inputs
with the new task of the intellectual supervision of the driving
system. This new task is not necessarily simpler or easier, though it
may be intended to be, but can be seen as representing a management,
rather than labor, role in vehicle operations.

%%LOSING Thread of argument

%% The narrative of automation progress was once the province of
%% blue-collar labor only, but is now moving into white-collar work as
%% well, mobilizing the popular concern about next-generation
%% automation technologies that perform more difficult and complex
%% informational tasks. 
Self-driving vehicles make sense today because of
a general climate that believes in the possibility of automating
knowledge work, and their development feeds back into the perception
that other complex tasks will soon yield themselves to automation. Bill
Gates, speaking at the American Enterprise Institute,
suggested that a large portion of the workforce will find itself
displaced by robots in the next 20 years, including accountants and
other white-collar
jobs \cite{gatesRobots}.\footnote{Gate's suggested remedies (low-to-no taxes and decreasing minimum
wages) are painfully biased toward corporate profits.}
His comments play directly into
contemporary fears about automation, and support only the
narrative that job-loss due to automation is inevitable, and workers
(or drivers)
should just get out of the way. To this view, the last two hundred
years of innovation in automation is unidirectional and largely
undifferentiated:  from steam power to the assembly line, from
Taylorism to roboticization, each was yet another nail in the coffin
of the human worker. But
while it is undeniable that automation has changed the
character of human labor, this perceived uniformity in automation
processes is a figment of the collective imagination. Control and
rationalization of
the work processes of the individual---in a way mechanizing her---also
created new classes of worker and expanded the role of human managers
in the labor process. Automation may look very
different depending on where in the hierarchy 
a person happens to fall, but the historical lesson is that human
involvement remains, though altered in space, time, and kind. As John
Diebold pointed out in 1953, there will be ``no worker-less
factories as a result of automation'' \cite[p. 63-64]{dieboldNew}
precisely because human beings will be needed to construct, to repair,
to manage, and to oversee.

%% took out paragraph about labor creativity, now in chap3



%% Why might this be? To understand it, we must understand something
%% about AI history, and the ideologies intertwined with artificial
%% intelligence and robotics research.

%% This approach
%% presents serious risks. It may be some time before the human inside
%% the car can be entirely disengaged with the driving task,2 if that is
%% even something we want as a culture, which means an interim period of
%% operation potentially characterized by “hours of boredom punctuated by
%% moments of terror.”3 The danger of human inattention,4 which has shown
%% up in aircraft automation with sometimes disastrous results, is
%% actually pushing aircraft manufacturers to more fully involve the
%% human in the process of flying, even as cockpits become more and more
%% computerized.5 And by focusing on ever more automation, rather than
%% appropriate automation, we may also be removing some of the parts of
%% driving that are most enjoyable: by replacing the skilled craftsman
%% with the automaton and the machine tender, we risk making driving
%% sterile and dull. 

\section{Artificial Intelligence}
%% 1.2) place to introduce the brief overview of AI history, of what goes
%% into the different approaches
%% --starting from historical visions of automata (1 p)
%% --classical symbolic approaches / physical symbol system (3 p)
%% ----which in a way we have returned to today w/ explicit mapping
%% --Rodney Brooks subsumption architecture and ``world is its own best
%% model'' (3 p)
%% --Leaning heavily on H. R. Ekiba and his approach to illuminating the
%% unstated assumptions of AI research areas (2 p)

%% TODO
%% Newell and Simon, general intelligence, but we get Shakey
%% DARPA SC
%% different wave of research, Brooks's subsumption
%% keep most on prob robotics
%% but this isn't what's talked about; connected to machine learning/vision

Artificial intelligence has its own history, intertwined with that of the
automation of work, that feeds into driverless car narratives. Though many popular
portrayals of AI care little about the actual history of the
field,\footnote{They generally pull from major moments that garnered enough popular
attention to enter the cultural knowledge-base: ELIZA, Deep Blue,
Watson, and Siri.} advances in AI are seen to cross-pollinate: culturally accepted narratives of AI are used
as models of progress and evidence of the technological inevitability
of self-driving vehicles. 

%% The history
%% of AI as conveyed through news coverage about self-driving vehicles is
%% not a complete one, but it is one side of the truth. And even
%% engineers ignorant of this history may make similar mistakes of understanding.

%% Automata, 
%% or rather semblances of automata, appear in Hellenic Egypt, with
%% priests as puppeteers pulling their strings \cite[Ch. 1]{mccorduck}. The history of
%% artificial life is intertwined with that of autonomous machines: the
%% creation of Pygmalion's Galatea echoes the same practices and
%% concerns, as does the story of Mary Shelley's ``modern Prometheus.'' (Of
%% particular note for the purposes of this argument, Shelley's vision of
%% artificial life is inspired and physically mobilized by static
%% electricity, the infusing of a ``spark of being'' into the creature; and
%% electrostatics were a highly public research topic and indeed public
%% spectacle, complete with live demonstrations, in the 1700s and early
%% 1800s) \cite[p. 44]{shelley}.

Intelligent machines are not a new idea. Just as automation has long
been a part of human history, dreams of artificial life suffuse our
legends---though as Minsoo Kang
rightly notes, these early automata are of diverse types and kinds,
some readable as automata only in hindsight due to their
similarities with contemporary robots \cite[p. 15]{kang}.
Nonetheless, that we continue 
to retell these stories should tell us something about the
lure of the automated, its power as a ``hybrid entity'' that can
mediate between living and nonliving worlds \cite[p. 19]{kang}.
The myths about Hephaestus and 
his creations, notably Talos, a golden female automaton, come to us
from antiquity \cite[Ch. 1]{mccorduck}, but continue to be cited as
historical antecedents in 
literature on autonomous robots and their ethical issues \cite[p.
  3]{patrickLin}. Judah Loew ben Bezalel, a Talmudic
scholar, is in legend the 
creator of the golem, a being animated from clay who functioned as a
spy against the Gentiles \cite[Ch. 1]{mccorduck}---the rabbi occupies
a special position among the most 
prominent AI researchers of the 20th century: Marvin Minsky and Joel
Moses grew up with a ``family tradition 
that they are descendants of Rabbi Loew,'' and Joel Moses claims a number of
other American scientists (including John von Neumann and Norbert
Wiener) also consider themselves among the descendants \cite[Ch.
  1]{mccorduck}. This is all 
to say that ancient myth and legend continue to subtly underpin
research in autonomy and artificial intelligence.

%% Makers of automata strove to imitate the very materials of life,
%% hoping to ``make the parts of their machine work as much as possible
%% like the parts of living things and thereby to test the limits of
%% resemblance between synthetic and natural life'' \cite[p. 606]{riskinDuck}.
%% Automata of this era ``bled,'' ``defecated,'' and ``breathed,'' though some
%% of these functions were themselves faked, such as in the case of
%% Vaucanson's Duck. Nevertheless, imitation was central to the project,
%% and in this way these early automata prefigured at least some of the
%% developments in AI and Alife. These early simulacra, though little known and
%% rarely referenced today, provide critical background for the attempts
%% that followed. 

 %% Indicative of Norbert Wiener's later research into cybernetics, such
 %% corrective feedback mechanisms had been studied since at least the
 %% late 18th century, when James Watt incorporated a governor into his
 %% steam engine. Watt himself had pulled from earlier applications of
 %% governors in windmills, which had been used since at least the 17th
 %% century \cite{richardhills}.

%% Many of these automata, despite being surprisingly accurate
%% mimeses of life, did little in terms of interaction with the
%% environment. M

%% Drosz and Vaucanson %% Steam governors  κυβερνήτης
As Jessica Riskin chronicles in her studies of eighteenth and
nineteenth century automata, clever inventors like Vaucanson, interested in going
beyond mere representation, created a variety of impressive
simulations of life.\footnote{\emph{Simulation} in its modern sense, meaning
``experimental models that can elucidate the natural,'' rather than its
contemporary sense which would have connoted artifice \cite[p.
  605--606]{riskinDuck}.} But meaningful interaction requires closing the loop between
sensing and acting with corrective feedback.\footnote{This began in
  the 17th century in windmills \cite{richardhills}, before James
  Watt's famous steam engine governor
(late 18th century). James Clerk Maxwell's 1868 paper on centrifugal
 governors in steam engines became one of the central papers in early
 control theory \cite{ottomayr}.} Bringing together a number of existing
 areas including control theory,
 cybernetics---from the Greek \emph{cybernetes}\footnote{This is as
   written in Wiener's contemporary papers. It could more properly be written
   as \emph{kybernetes}, or 
   $\kappa\upsilon\beta\varepsilon\rho\nu\acute{\eta}\tau\eta\varsigma$
   \cite[p. 11]{cybernetics}.} meaning 
 ``steersman'' \cite[p. 6]{wienerMainIdeas}---extended automation's reach to more complex
 systems: ``control and communication in the animal and machine.''
 Cybernetics envisions the world in 
 terms of feedback mechanisms: all systems that pass and respond to
 messages internally are within its explanatory sphere \cite[p.
   10-15]{wienerMainIdeas}.\footnote{Homeostasis, balance, and motion
 disorders like locomotor ataxia and Parkinsons all are cited by Wiener.}
 AI is interested in re-creating many of the same
 self-regulating systems within computers---and claims a
 similarly broad explanatory role, whether the goal is playing chess,
 investing in the market, or driving a vehicle. 

%% This replacement of
%%  competencies tends toward the obsolescence of the human being, and
%%  again is interpreted in many stories as a teleological process, a
%%  perpetual advancement of technological competencies.

%Control, or the feedback mechanism, is necessary for the extension of
%information  theory into communication theory.

%% Integral to the history of AI as a field is that it was fundamentally
%% interdisciplinary from the start. Like its forebear cybernetics, it
%% brought together researchers from physics, mathematics, biology, and
%% early cognitive science. The field began in earnest with the


%% The research areas of
%% the ``Dartmouth Summer Research Project'' included language learning
%% and use, ``neuron'' networks,\footnote{Neural networks are one of the
%%   intriguing long-term stories of AI research, subject of much
%%   controversy over the years regarding whether or not they would
%%   actually work. A couple of theoretical developments altered them
%%   from a curiosity to one of the main techniques in modern AI. This
%%   half-century journey was presaged by the one sentence: ``Partial
%%   results have been obtained but the problem needs more theoretical
%%   work'' \cite{dartmouthconf}.} self-improving machines, and computational 
%% creativity \cite{dartmouthconf}. Early successes spurred romantic
%% predictions, and by 1960, human-level intelligence was foreseen by
%% some to be only a decade away \cite[p. 3]{winston}. 

\subsection{Logical Beginnings}

The Dartmouth
Conference in 1956---hosted by John McCarthy, who originated
  the term ``Artificial Intelligence''---assembled many who would continue to
be preeminent researchers over the next decades:\footnote{Marvin Minsky, Nathaniel Rochester, and
Claude Shannon also co-hosted, and attendees included Trenchard More, Oliver Selfridge,
Ray Solomonoff, Allen Newell, and Herbert Simon.} all were united by
``the idea that there was a rigorous and objective way of explaining
the human intellect'' \cite[Ch. 5]{mccorduck}. General purpose
intelligence was the dream motivating ``Dartmouth Summer Research
Project'' work in ``neuron'' networks,
self-improving machines, and computational creativity
\cite{dartmouthconf}. Representative of this first age of AI, Allen Newell and Herb Simon's
physical-symbol system hypothesis states that ``symbols lie at the root of 
intelligent action'' \cite[p. 109]{newellsimon}.\footnote{Such
constructs are \emph{symbol systems} in that they contain symbols and processes
that act upon symbols. And they are \emph{physical} in that they obey
physical laws and are realizable through engineering. They operate via
heuristic search, pruning a tree of possibilities in a hopefully
``intelligent'' way \cite[p. 124]{newellsimon}.} A symbolic
approach rooted in the physical symbol-systems hypothesis did not
yield results as quickly as expected.\footnote{It did produce
  expert systems useful in particular domains, but there is good
  reason to think we do not spend most of our time processing symbols:
  symbolic logic takes a lot of mental capacity, so we generally use
  other sorts of shortcut processes to come to decisions. Pollock
  calls these ``quick and inflexible'' or ``Q\&I'' models \cite[p.
    120]{pollock} and Dennett refers to them as ``habitual methods''
  or mechanical routines \cite[p. 157]{dennett}.
It may well be that introspection on thought is a large part of why the logical theory seemed so
compelling: researchers like Newell and Simon used ``think aloud''
experiments to identify problem-solving techniques \cite[Ch.
  10]{mccorduck}, which seems naturally to suggest a logical mode.} As
Pat Winston put it: ``Everyone searched for a kind of philosopher's
stone, 
a mechanism that when placed in a computer would require only data to
become truly intelligent'' \cite[p. 4]{winston}---they did not find one.
%%While we certainly may apply logic
%% and process symbols, there is good reason to think that is not how we
%% spend most of our time.\footnote{} We rely on other processes to do
%% much of our day-to-day reasoning because it is ``very important for
%% humans to be able to make rough probability judgments'' \cite[p.
%%   120]{pollock}, and accepting the output of such approximate models
%% is not at all unreasonable in the absence of evidence for their
%% inappropriateness to a situation. 

The failures of logic-based robotic systems like Shakey---the
DARPA-funded, SRI
robot named for its tendency to shake when in motion \cite[Ch.
  10]{mccorduck}---to achieve intelligent results disillusioned many
researchers. While limited processing power made responses and reactions slow, the
uncertainty inherent in the real world made them unreliable. Far from
the generally accepted story of continued, natural 
progression (that while AI may not have been possible before, it must
certainly be possible now due to continued development and greater
computing power), artificial intelligence proceeded in fits and
starts. Achieving humanlike intelligence via logic 
systems was the archetypal dream of early AI, but while logic is
part of the puzzle it is not the entireity of it.

\subsection{Robotics, Embodiment, Probability}

Persistence, funding, and new approaches did yield results.
DARPA's massive 1983 US Strategic Computing initiative had a 10-year
funding plan, including work on ``image understanding'' and
interpretation, an autonomous land vehicle (ARV), a pilot's associate,
and computerized battle management software \cite{rolandShiman}. It
was canceled without 
achieving its stated goals, but this was not truly failure: ``AI now
performs miracles unimagined when SC 
began,'' especially in areas of natural language processing, ``though
it can't do what SC promised,'' \cite[p. 328]{rolandShiman}. Other
approaches to building intelligent robots were developed in the 1980s. Rodney 
Brooks's subsumption architecture \cite[p.
  353]{mobilebrooks} attempted to cut out 
cognition altogether, focusing on sensing and reaction and iteratively
building small systems that could cope with uncertainty:
insects, for example, lack cognition, but respond more adroitly to the
world than Shakey and its contemporaries. Instead of modeling the
world, this approach treats
the world as ``its own best model'' and focuses on embodiment and action
within the environment \cite[p. 256]{ekbia}.

%% The 1980s continued with divides in 
%% the field about approaches to artificial intelligence, the decade
%% actually resulted in a wide variety of successful projects based on
%% improvements to expert systems, machine learning, natural language
%% processing, and computer vision \cite[Afterword]{mccorduck}.
%% Divides in the field continued, but The 1983 US Strategic Computing
%% initiative, led by Robert Kahn at DARPA, had AI as its third focus
%% area, with ``image understanding'' and interpretation---made possible by
%% the digitized image---as long range project goals. In its revised
%% 10-year plan, the initiative even included an autonomous land vehicle,
%% alongside a pilot's associate, and computerized battle management
%% software \cite{rolandShiman}. The project suffered serious management problems, and
%% was eventually canceled, precipitating the general crash in AI
%% funding and another ``AI Winter'' beginning in 1987 or
%% 1988\footnote{These are roughly the dates Russell and Norvig give in
%%   \emph{Artificial Intelligence} \cite{russellnorvig}.}---through which research quietly continued, waiting for another
%% up-tick in public interest. But the Strategic Computing project,
%% whatever its lofty goals, was no failure. McCorduck cites Roland and
%% Shiman as saying that ``AI now performs miracles unimagined when SC
%% began, though it can't do what SC promised,'' which speaks to the
%% important developments that were made in the service of DARPA's vision
%% \cite[Afterword]{mccorduck}.


%% Overexpectation, however,
%% led to a first AI ``winter'' from about 1965 to 1970, in which the
%% grand promises of AI were shown to be much further off: the current
%% techniques simply did not yield advances at the required rate. 

%% The early to mid 1980s were also a time of great developments in
%% Artificial Intelligence, an era of ``celebrity science,'' high hopes,
%% big investments, and subsequent great public disappointment with t

%% DARPA SCI
%% Despite the
%% earlier warnings of Roger Schank and Marvin Minsky that overoptimistic
%% expectations for AI would result in another winter like the previous
%% one in the 1970s, overall expectations were high, especially within
%% the business community, which funded companies and assimilated AI
%% techniques into real applications \cite[Afterword]{mccorduck}.

%%TODO make sure it is 3/4 appropriately

%% \subsection{Physical Symbol Systems}

%% Classical symbol approaches (3)
%%specifically w/ reference to ideologies
%% The first important AI paradigm is the classical symbolic system
%% approach. Associated with Allen Newell and Herb Simon, the main idea of the
%% physical-symbol system hypothesis is that ``symbols lie at the root of
%% intelligent action'' \cite[p. 109]{newellsimon}. Therefore not only
%% does intelligence require symbolic manipulation, it may indeed be
%% coextensive with physical-symbol systems; in other words, a
%% physical-symbol system has ``necessary and sufficient means'' for
%% intelligence and intelligent action \cite[p. 111]{newellsimon}.
%% These symbol systems would arrive at answers through a technique known as
%% heuristic search: by looking through a tree of possibilities in an
%% intelligent way, they arrive at the appropriate answer.\footnote{The key
%%   point of heuristic search is that such answers are approximate, but
%%   arrived at quickly, rather than exact, but arrived at slowly or,
%%   perhaps, never at all.} Intelligence is applied in heuristic search
%% by the pruning of the tree: rather than having to apply brute force to
%% search the entire space, an intelligent system applying heuristic
%% search makes decisions at each node as to which branches are most
%% likely to produce a good result and searches those \cite[p.
%%   124]{newellsimon}.\footnote{As Newell and Simon wrote, what makes a problem a
%% problem is ``not that a large amount of search is required for its
%% solution, but that a large amount \emph{would} be required if a requisite
%% level of intelligence were not applied'': the task of intelligence is
%% to ``avert the ever-present threat of the explosion of
%% search'' \cite[p. 125]{newellsimon}.}

%% \footnote{This is somewhat ironic in that
%%   modern approaches to AI focus particularly on search, which has
%%   often produced results that surpass logic-based approaches.} 

%% The conceit, then, of the physical-symbol system hypothesis is twofold.
%% First, it assumes that
%% human beings essentially operate in this manner: that we apply
%% symbolic logic and heuristic search to provide for our intelligent
%% actions. Second, it assumes that computers can be true physical-symbol
%% systems. The validity of the second assumptions is not necessary
%% clear, but presents a philosophical question of whether or not computers are capable of
%% true intelligence, not whether or not they can
%% convincingly imitate intelligence.\footnote{Searle essentially rejects
%%   the computer as physical-symbol system in his Chinese room example
%%   \cite{chineseSearle}, though other philosophers of AI do not take
%%   his view \cite{escapingBoden}. Indeed, I find the Chinese room example to be
%%   lacking in several respects, particularly that it rests on some
%%   na\"{\i}ve assumptions about what can or cannot be intelligent that seem to beg
%%   the question it tries to pose. What is important about the
%%   argument is that, right or wrong, it challenges the basis of
%%   computers as intelligent systems.}

%% John Searle
%% essentially rejects the computer as a physical-symbol system in his
%% Chinese room example. Instead, the computer (room) is seen as a cheap
%% imitation of such a system: a room into which strange symbols are
%% passed, the appropriate responses looked up in a book,
%% and then passed out again, all without anything in the room having
%% access to their meaning \cite{chineseSearle}. Though meaningless
%% symbols are being processed by such a contraption, his view is that no
%% electronic computer ``can really manipulate symbols, nor really
%% designate or interpret anything at all'' \cite{escapingBoden}. This is

%% :  when
%% asked to describe how we came to some decision, basing it in logic
%% seems the most acceptable alternative

%% The physical-symbol system hypothesis
%% is just that, a hypothesis; and h

%% The former point proved harder for symbol-system researchers to dodge.
%% While it may be true that
%% machines can think because ``we are precisely such machines,'' \cite[p.
%% 83]{chineseSearle}, this does not guarantee that symbol systems are
%% the way to achieve intelligence. 


%\subsection{Embodied Cognition}

%% Above and
%% beyond the problems that physical-symbol-system-based AI had in
%% relatively controlled domains, robots controlled with these
%% techniques, overwhelmed by the complexity of the real world, responded
%% slowly and ineptly.
%% Uncertainty everywhere in
%% the physical world makes world-modeling extremely difficult: the
%% model has a tendency to get out of sync, and there needs to be
%% some reliable, calibrating stimulus to return it to accuracy.

%% Rodney Brooks (3)

%% level zero competence might avoid objects,
%% while the next level would wander aimlessly, and the next would
%% attempt to wander to places it had not been before \cite[p.
%%   351--352]{mobilebrooks}. Each layer is separate (and can actually run 
%% on its own processor), and can read data from
%% and write data into the layer below it (hence the term \emph{subsumption}: 
%% the higher levels subsume the lower).

%% The subsumption approach is by definition modular. More complex behaviors
%% are built out of simper ones level-by-level; when a higher level fails or
%% cannot run, lower levels continue to operate, and basic behavior is
%% maintained \cite[p. 355]{mobilebrooks}.\footnote{This approach has tempting
%% biological connections as well: .} And each layer is built to be robust in the inevitable
%% event of uncertainties and lost messages. Theoretically, one could
%% imagine building systems of great complexity (e.g. cars) using these approaches,
%% but this architecture, the radical relativist response to the
%% structural realism of the physical-symbol system, did not alone manage
%% to allow the creation of robots that exceeded insect intelligence.

The layered subsumption approach is modular, building complex
behaviors on top of and out of simpler ones, and presents tempting
biological similes.\footnote{One can imagine the human being as a
robot with a subsumption 
architecture, where breathing and heartbeat are lower layers than balance,
which is lower than voluntary motion, which is lower than logical
thought.} But while it could theoretically be used to engineer greatly
complex systems (cars), this architecture did not alone manage to allow the
creation of such robots.
Brooks himself abandoned the project of building intelligence from
these humble, subsumptive blocks.\footnote{Instead, his later research, for example
the Cog robot, shifts focus from ``emergence'' to ``integration,'' and
reversed some of his initial fervor to avoid representation \cite[p.
  258]{ekbia}.} He skipped the middle of the evolutionary tree, straight
to humanoid forms, because the evolutionary approach was too slow: 
Brooks reported, it was ``starting to look like, if I was really
lucky, I might be remembered as the guy who built the best artificial
cat,'' a distinction he apparently did not desire \cite[p.
  65]{brooksflesh}. But to do this, bootstrapped knowledge from other sources was
necessary, bringing back some of the world modeling inherent in older,
symbolic approaches. 

%%\subsection{Probabilistic AI}

Dealing with uncertainty in a more pragmatic way, statistical AI
approaches dominate the modern robotics space, typified by Sebastian 
Thrun, Wolfram Burgard, and Dieter Fox's book
\emph{Probabilistic Robotics}.\footnote{The book begins with a nod to
  the self-driving car project: ``Wouldn't it be great if all our cars
were able to safely steer themselves, making car accidents a notion of
the past?'' Utopian vision of technological possibility are not
limited to press accounts \cite[p. 3]{thrunProb}.} This research,
organized around algorithms based on Bayes rule,
distinguishes itself from prior model-based (logical) or ``reactive
behavior-based'' (subsumption) approaches by dealing gracefully with
uncertainties in both the world model and sensor inputs \cite[p.
  9]{thrunProb}. Probabilistic robotics depends on mapping the world
and modeling the robot's relation to it.

%% \footnote{It is
%%   possible to create robots that navigate without any modeling of the
%% environment, as Fern\'{a}ndez-Madrigal et al.\ point out. However, this
%% severely limits the range of behavior that is possible. Without
%% reference to the environment, errors slowly accrue, and a system that
%% navigates in a known space without localization will find itself
%% getting further and further off-track.}

Localization has been a fundamental issue for robotics: In order to figure out how to act, a robot
needs to know where it is; and in general, robotic environments can be
expected to change rapidly with motion, and to vary significantly from
place-to-place \cite[p. 4]{SLAMbook}. Motions must continually be reevaluated in new and
emerging situations. Key to this puzzle, the robot must be able to determine its own
relationship to some target location: this
``target-robot relation'' is an ``unavoidable'' component of successful
navigation \cite[p. 5]{SLAMbook}.\footnote{This relation can be expressed
in different forms---either quantitative representations such as maps,
or logical, prepositional statements---and these representations carry
their own techniques of interpretation.} While
localization is the usage of environmental elements to
estimate a robot's position, those elements themselves must be known
in order to perform this process. Mapping is the complementary
process, the estimation of ``\emph{unknown} spatial relations that
exist between environment elements'' to allow for subsequent
navigation \cite[p. 5]{SLAMbook}. This presents a
precedence problem:  to build maps with
autonomous systems, the systems must know where they are; in order to know
where they are, they must have maps to measure
against \cite[p. 6]{SLAMbook}.

%% Consider the history of AI we already reviewed.
%% Localization was part of the cause of the poor performance of the
%% robot Shakey: world modeling through a symbolic approach was slow,
%% which was rendered even more problematic by the relative lack of
%% computing power at the time \cite[Afterword]{mccorduck}.

%% By
%% contrast, Brook's subsumption architecture involved an
%% attempt to avoid the localization problem by pure sensory response to
%% the environment; but building a robot that exhibits complex and
%% predictable behaviors in complex environments through this approach
%% is exceedingly difficult, as evidenced by Brooks's shift to hybrid
%% modeling approaches. But new sensors providing new kinds of data, and
%% new algorithms and techniques to cope with uncertainty---such as those
%% of probabilistic robotics \cite{thrunProb}---have allowed
%% significant progress in the field.

Though one way to determine robot position in the world is through
simultaneous localization and mapping (SLAM)---a serious research area in
robotics, as it is ideal for regions that are impractical to map
beforehand, such as ones that are always changing---it is easier to localize by comparing
measurements to a known map. However, that implies that the map is
pre-made, and therefore sets limits on the rate at which the
environment can change and still allow the robot to operate. Today, localization and
mapping problems alone are both considered ``satisfactorily solved in practical
situations''---though no single algorithmic approach is ideal for all purposes---given sufficient
computational resources and environmental data \cite[p.
  5-6]{SLAMbook}. SLAM techniques, however,
are still somewhat less reliable or developed.



%% Deep learning


%Ours is the age of
%% The new source 
%% of bootstrapping information is data, ``Big'' and ``small.''
\subsection{Learning and ``Knowing''}

But despite that probabilistic robotics fundamentally
underlies the major automated vehicle innovations reported by the
popular press, it is rarely, if ever, mentioned. What receives attention in the
self-driving car narrative is a different gloss on statistical
systems: today's buzzword
AI technique, ``deep learning,'' appears to yield
radical new possibilities everywhere it is 
applied---though it really just means a return of neural
networks,\footnote{These systems, also known as
``multilayer perceptrons,'' still have little or nothing to do
  with actual neurons: they are brain-like in only the barest
  toy-model sort of way. Consisting of webs of interconnected
nodes, they may have multiple layers (hence ``deep'' in deep learning)
or may be shallow. Each node 
corresponds to a particular feature or combination of features in the
input \cite{neuralJordan}.} which, armed with some improvements in weight generation,
more layers of
nodes, and more data to train with, have been able to eclipse many of
the previous techniques. 


%%EDIT: this could all use some clarity
%% such networks have an input layer and
%% output layer, with one or more intervening hidden layers.
%% , and therefore works to classify inputs into different
%% categories. Each node is also connected to all the nodes in the
%% following layer with some tunable weight. T

Training the
neural network involves adjusting the connections' weights so as to
be more sensitive in distinguishing different types of
inputs. This tuning,
traditionally, has been done using supervised method and
backpropagation of errors: the output result is compared to an
expected classification result (determined by humans), and the error used to tune the weights
more appropriately.\footnote{These techniques are actually used in the
probabilistic robotics field, to train learning algorithms to develop
maps out of noisy sensor data \cite[p. 284-297]{thrunProb}.} Deep
learning extends this technique, decreasing 
the amount of ``feature-engineering'' (identifying the
features the network should use to distinguish inputs) required to
train the system. The ``ideal'' training situation is entirely
unsupervised: the network independently ``learns'' the features of
unlabeled and unclassified input, without any human input. This ideal
prospers both due to convenience in terms of human effort (less
programmer effort required to build labeled sets of training
data\footnote{This labeling is considered ``inhumane'' work, and contributes to
a reluctance to use machine learning in automotive applications,
according to G\"{o}de Both \cite{bothpt2}.}) and
due to the deep-seated ideology, which reappears in the imagined
operations of self-driving cars, that machine independence is
paramount. Unsupervised learning is often seen as more impressive and
valuable research.

%% ``self-aware'' mario, image recognition and other
%% claptrap (1) (incl. John von Neumann & ALife (See CMS790 paper)??)
%% Proliferating AI hype impact the way all other AI work is
%% perceived by the public (especially viewed in the light of
%% popular concern about autonomous vehicle ethics).

%% The real
%% history of artificial intelligence and robotics is far more nuanced.

%% Since vision is the primary sense involved in human driving
%% and one of the main research areas for automated vehicles, s

Popular claims
about the utopic promise of deep learning, to
learn about the world ``on its own,''
abound. Google's and Stanford's recent
improvements in image recognition, driven by deep learning,
\cite{markoffImage} triggered a wave 
of popular speculation about computer vision meeting or surpassing
that of humans. Such
advances might seem to translate automatically into the self-driving
car space. Though some articles note that current
state-of-the-art image recognition is still considerably less capable
than people are, many articles still present the uncritical idea that we have solved
the image-recognition problem---or rather, that increases in computing
power mean it will necessarily soon be solved. By this narrative, seemingly limitless
possibilities are open before us: a new universal problem-solver, like
feedback control before it, seems to have broadened the frontiers
of the future. 

%% A number of conceptually different approaches to AI have been tried
%% over the past 50 years, often with similarly deep-seated
%% ambivalences.\footnote{Ekbia's book does an excellent job
%%   exploring and cataloging many of these, more than I have space for.
%%   The logical and neorobotic approaches appear most relevant as
%%   broad-strokes historical precursors for inclusion here. But the rest
%% of his book is worth reading for more detail on other schools of AI
%% thought.} Ideas fade 
%% and resurface as fashions change, and greater computational power
%% allows ``failed'' techniques to be tried anew. But much of this
%% history is presently unappreciated outside of the discipline. What the
%% physical-symbol system and embodied cognition 
%% have in common is their attempt to start from first-principles or zero
%% knowledge, and to build inexorably toward intelligence. The difficulty
%% of this road led Brooks to recombine some of the old manner of
%% knowledge representation into his robotic techniques. Today, multiple
%% strains of AI research play out the same tensions. While Tomaso Poggio
%% and others at the Center for Brains, Minds \& Machines attempt to
%% emulate human learning,\footnote{Speaking at the CAST Symposium at
%%   MIT, September 26, 2014, Poggio noted that their approach is
%%   actually stepping away from ``Big Data'' and AI approaches applied
%%   by Google, among others.} other researchers bootstrap their machines
%% with more fully-formed models. 

%%\subsection{What it Means to ``Know''}
%%It is difficult to
%% define intelligence in ourselves \cite{huntIntelligence} and yet another thing to define it
%% in relation to other entities. 

Though computer science and philosophies of AI have been using
``intelligence,'' ``knowledge,'' and ``understanding,'' among other
words, to talk about computers since the beginning of the field, these
uses should not be taken at face value. ``Intelligence'' is slippery,
and its definition is not constant over time. Weaving was once considered to be a
peculiarly human capability, a sign of an advanced, intelligent
mind \cite[p. 627]{riskinDuck}, but after Vaucanson's loom allowed mechanical devices to weave
seemingly on their own, this capacity was no longer seen as uniquely
human, and was no longer a marker of intelligence. The same process
occurred with chess in the 1990s: when IBM's Deep Blue beat Gary
Kasparov, chess ceased to be the standard by which intelligence could
be judged, precisely because it had been achieved. Real intelligence
had to lie elsewhere: for example, in the game Go, mastery of which
has continued to elude machines.\footnote{``When IBM's Deep Blue beat
  Gary Kasparov in 1997, most Artificial Intelligence researchers and
  commentators decided that chess playing did not require intelligence
  after all and declared a new standard, the ability to play Go'' \cite[p. 623]{riskinDuck}.}

Nevertheless, machines manage to do things that \emph{seem}
intelligent. Our heuristics for understanding the observed 
behaviors of machines slip slowly over time from self-conscious
scare-quoted use into casually accepted statements. While automatic
translation may seem ``intelligent,'' or a system that can define \emph{\'{e}toile}
as ``star'' may seem to possess ``knowledge,'' this intelligence or
knowledge is perhaps very different than our own. A deep
epistemological question presents itself: how do we know, and how do
machines ``know''? Many AI systems operate via statistical pattern
recognition, so we may ask  whether we believe human intelligence is
also merely pattern recognition: Does a system that can associate \emph{star}
with its definition really know what a star is? Is linguistic
association sufficient for knowledge?

%% As we have
%% seen, Searle's thought experiment of the ``Chinese room''
%% argues that symbol processing and pattern recognition alone is not
%% intelligence, though from the outside the results may appear to be
%% intelligent. Despite the faults of his argument, his caution
%% about ascribing overly ambitious human ideas to computational
%% processes is warranted. Even Pamela McCorduck, a colleague of several
%% notable AI researchers and a believer in the field in general, hedges
%% on how intelligent some of the programs she discusses in \emph{Machines Who
%% Think} actually are.

H. R. Ekbia and others remind us that we should be skeptical of the
applications of these terms to computational processes \cite{ekbia}.
Ornstein, Smith, and Suchman, in their 1984 
article ``Strategic Computing,'' warn of the difference between domain
capabilities and ``common sense,'' and suggest that ``unwarranted
optimism'' and a particular funding climate (issues also present today)
push researchers to mask the shortcomings of AI with ``semantic
shifts'' \cite[p. 14]{ornstein}. We alter the definitions of ``knowledge'' and ``understanding''
to fit what our machines can do, and these claims, taken literally,
``give rise to unrealistic confidence in the power of the
technology'' \cite[p. 15]{ornstein}.
The two-way process of linguistic and technological change---that
intelligence gets applied to describe whatever researchers manage to
achieve, while real ``intelligence'' retreats away from each
computational advance---leaves these terms poorly defined. The ``understanding''
involved in even the most impressive current systems is limited. Image
recognition systems, for example, are not
able to answer questions about the scenes that have to do with
the material properties of the objects, or likely results of various
actions \cite{gomesJordan}.\footnote{Another recent slew of articles focuses on
the ``self-aware'' Mario created by researchers at the University of
T\"{u}bingen. Any pretense to worry about a ``self-aware AI . . . with
an insatiable desire for material wealth'' that knows ``how to kill'' \cite{vincentMario}
is simply journalistic excess, as the
  researchers themselves well know.} But as technical stories spiral out of the
lab, such subtleties are often lost, and rhetoric or appearance of
intelligence outweighs the actual technological substance behind them.

%% These statistical techniques leverage existing data
%% sets by training machine-learning systems; but
%% machine-learning systems have properties that make them ill-suited for
%% safety-critical systems, and autonomous vehicles are designed with a
%% mix of approaches that allows for more introspection into their
%% workings.\footnote{See chapter \ref{chap:2}.} 

%%  This case is just a convenient example of how
%% such stories spiral out from the lab and acquire new meanings.} and suggests
%% significantly more care must be taken in the use of such terms. Mario
%% is programmed with emotional states similar to the way a word-processing program is
%% programmed with different modes or display parameters. These states
%% are simply caricatures of emotions, and that, plus the program's
%% ``natural-language'' interface, makes it appear more eerily
%% science-fictional than it actually is.

We would do well to remember that 
progress in AI has been slow, contingent, and heterogeneous, a product of a
wide variety of concepts and techniques. Logical, precise models,
reactive-subsumptive control, and probabilistic re-framings of the
navigation problem all had their parts to play in this evolution. It
is difficult to argue with techniques that ``work'' and
spawn numerous real-world systems, from personal assistants to
self-driving test vehicles. However, the limits of each new paradigm are rarely
obvious \emph{a priori}, and it remains to be seen how far current
algorithms can carry the field, and whether subsequent algorithmic
advances will allow progress to continue unabated. This is the
environment that autonomous cars, as a 
research area of artificial intelligence, find themselves in today:
part of a new surge of interest in the field, driven by new
or newly extended techniques. But no one has yet found Winston's
philosopher's stone.


\section{Aircraft and Autopilots}

%% ---and which, when poorly designed from
%% a human factors 
%% angle, as they have often been, can behave in ways that are unexpected
%% by the pilots and antithetical to their wishes.
%% . AF
%% 447 took off from Rio de Janiero, Brazil on its way to Paris, crashing
%% over the Atlantic Ocean at 02:14 UTC on June 1, 2009. The official
%% report, published in 2012, identifies the cause of the accident as a

%% it is worth examining how they
%% have treated human and machine components and interactions. A

Autopilots have a strong rhetorical role as
models of ground vehicle automation, but autopilot design and use differs 
significantly from its popular representation. It is not a fully automated
system that controls the entire aircraft, but a set of specific tools
leveraged by pilots to increase their agency. Autopilots are also complex systems, with many modes
that require deep technical knowledge and training to use
appropriately \cite{harrisPsych}. Stories of accidents
due to autopilot mode confusion abound. The official report for
the Air France Flight 447 crash cites a
failed hand-over of control from the autopilot system to the pilots as
the cause:
when the autopilot system shut down due to a failure of its airspeed
indicators, the pilots were not prepared to so suddenly resume the
task of flying a large airliner in poor conditions \cite{AF447}.
%% Seemingly confused by the situation they were in, the pilot and
%% co-pilot provided contradictory control inputs to the aircraft and
%% stalled the plane, causing it to crash. 

%cite removed
%airflow required electronic solutions\cite[p. 33]{DM}

 But this concern with cockpit automation is not
new, and pilots and aircraft designers have been negotiating human
roles in flying machines since the very early days of aviation. The
tension between stable and unstable aircraft design---will the
aircraft ``fly itself'' in a given orientation or does it require
constant control inputs to maintain course---goes back as far as the
Wright Flyer, as do the competing professional identities that
accompany them: are you simply a chauffeur, or a ``true
airman'' \cite[p. 21]{DM}? The Wright's focus on the operator's
skill created the unique profession of the
``pilot,'' and unstable aircraft were the standard for years, until
human fatigue over long periods of flight changed the importance of
stability to aircraft operation \cite[p. 22-24]{DM}. Stability,
this subsequent virtue of a well-engineered airframe, had to be again
renegotiated in the context of supersonic flight, where stability
problems related to supersonic airflow required electronic
solutions---pilots were still skeptical of black boxes seemingly out
of their control.\footnote{See J. O. Roberts, ``The Case Against
  Automation in Manned Fighter Aircraft,'' which argues for emphasis on
information display rather automated control \cite[p. 35]{DM}} Today, 
many airlines 
have guidelines that require automation systems to be used
whenever possible \cite[p. 38]{PARCCAST}, but automation in the
cockpit has long been a contested technology. 
%% But there was deep
%% concern among the 
%% flying community about black boxes that are out of the pilot's
%% control, and some saw these devices as ``band-aids'' for engineers not
%% ``skilled'' enough to make the aircraft stable:  J.
%% O. Roberts' ``The Case Against Automation in Manned Fighter Aircraft''
%% argued for more emphasis on displaying information to pilots, rather
%% than using automation to assume control \cite[p. 35]{DM}.
%% Nevertheless, cockpit automation has continued to increase.

%% The risks of a hand-over of control at high
%% speed would negate, by this reasoning, the benefits of computer
%% control on the highway.

While the AF 447 example shows the
perils of human interaction with automated systems, an issue which will
affect automated vehicles as well, its significance
within aircraft development does not seem to be broadly appreciated
outside of that field. It gets taken---including by one of the
automated vehicle researchers I interviewed---as meaning that human
interaction should \emph{never} be expected, and that until automated
vehicles can be fully self-driving in all circumstances, they will not
be sufficiently safe. But aircraft companies are learning a
different lesson: Boeing and Airbus, the two
most prominent manufacturers of commercial airliners, are distancing
themselves from complete automation, while increasing the computerization of
their cockpits through digital displays and tools to assist pilots in
the task of flying \cite{787dream} \cite{brownFuture}. The focus for
future development is on adaptive or adaptible automation
rather than complete computer control. Both types of systems allow for
balancing the operator's load---adaptive systems automatically,
adaptible systems by operator request---taking on tasks during
high-stress, busy situations, but handing back tasks during periods of
limited load in order to keep the operator informed of and engaged in
the operation of the system \cite{dahai}. Such systems have their own
engineering challenges, but represent a very different answer to the
question ``what should the role of pilots be?'' than does the further
complete automation of aircraft operations \cite{kaber}. Asking people to be mere machine
tenders, present only to ensure the continued operation of the
machinery, is indeed untenable, as it produces boredom,
inattention, and the risk of catastrophic failures like the AF 447
crash. Aircraft experience shows that sustained human engagement
with automated systems is possible, but has to be designed into
the system from the beginning.%%  However, joint human-machine systems research provides avenues
%% to engage operators in the operation of the system in ways that
%% increase overall system safety, without necessitating that the human
%% be entirely eliminated. There is a deep history of
%% work in human supervisory control (HSC), which I keep alluding to,
%% that has important implications 
%% for the real-world design of automated systems, and which tells a
%% potentially very different story about what automated vehicle
%% operation will look like from a ``driver's'' or operator's point of
%% view.


\section{NASA and High Technology}

%% And these are all senses with which the narrative of automation in
%% spaceflight is inflected today. Certainly unmanned spaceflight had a
%% strong interest in automation since at least the 1950s, with serious
%% research being done on simple automated probes to do a fly-by of
%% Mars \cite[p. 1]{battin}.

NASA, as a purveyor of high technology, also
gets leveraged in the popular narrative as a source for
  automated vehicle design inspiration: ``And so Google's new vehicle
  design 
takes a leaf out of NASA's design book to cope with such
eventualities. `It doesn't have a fallback to human---it has redundant
systems,' said Fairfield. `It has two steering motors, and we have
various ways we can bring it to a stop.''' \cite{simonite}. This is a
reductive view even of NASA's unmanned space
systems, let alone manned systems with which the Google vehicles must share
the critical characteristic of containing human occupants.
Spaceflight, both manned and unmanned, provides myriad reasons to
invest in automated systems: time delays prevent or at least greatly
inhibit remote control from Earth; conditions where humans are not
uniquely equipped to survive suggest the use of mechanical explorers
instead; and precise control functions with redundant backup systems
allow the use of the unique capacities of computerized systems to
monitor constantly and act immediately in the event of an emergency.
 But the story of automation in spaceflight is
not nearly so simple as it appears. Rather than being the ultimate and
obvious endpoint of a progression of human engineering in space, the
roles and implementations of automation remained fraught and highly
contested. 

%% by astronauts themselves due to professional pride, national
%% politics, evidence for the capability of the human being to be in
%% control, and the dogma among certain groups that such control is
%% necessarily more reliable than computerized automation---which has not
%% a little to do with the fragility of pre-integrated-circuit computer
%% technologies.

\subsection{Manned Spaceflight}

%% Numerous 
%% articles focused, not surprisingly, on the people who would do the
%% voyaging, how they would be selected, tested, and kept alive. But

%% may have been the most polemical, he 
%% \footnote{In an
%%   earlier speech to the SETP (October 4, 1957) on automation and 
%% the ``survival'' of the test pilot as a profession, Richard Horner
%% took a balanced and moderate view on human involvement, but warned
%% that technology would progress faster than human beings change:
%% the ``link'' that improves the least ``is the man himself'' \cite[p. 19]{DM}.}

Manned spaceflight might seem to be a story having little to do with
automation, as its public picture has often focused on the skill and
bravery of human astronauts. But it did not begin this way: from 1952
to 1954, \emph{Collier's} 
magazine published a series of articles 
titled \emph{Man Will Conquer Space Soon} which described Wernher von
Braun's vision for manned spaceflight \cite{scribdColliers}
\cite{dreamsofspace}.\footnote{The illustrations from this magazine
  are reputed to be some of the ``most influential images of the early
space age'' \cite[p. 9]{marketingMoon}.} Lurking behind this majestic
picture of manned spaceflight was a dark 
realization for prospective astronauts: to von Braun himself, the
astronaut would be a mere passenger, ferried into space by automated
rockets.\footnote{In a speech to the Society of Experimental Test Pilots in
August of 1959, a hostile audience for this sort of rhetoric, von
Braun emphasized that human control in rocketry is ``actually
undesirable'' because human beings are ``outrageously slow and
cumbersome'' in missile terms \cite[p. 66-67]{DM}.} Von Braun
was not alone in his cautions to
test pilots about the limits of their capacities: Richard Horner
warned that technology would progress
faster than human beings: the ``link'' that improves the least ``is
the man himself'' \cite[p. 19]{DM}. 

%% Not only was their
%% personal pride at stake---as military test pilots, they possessed a
%% particular standing in part by virtue of being in control of dangerous
%% and cutting-edge vehicles like the X-15\footnote{Kelly Johnson, the leading aircraft
%% designer in the US, working at Lockheed, objected to the manned
%% component of the X-15
%% project, but his objections were overruled by the NACA \cite[p.
%%   46]{DM}. Though it necessarily
%% incorporated significant automation to deal with hypersonic flight and
%% the transition from atmospheric to extra-atmospheric operation,

%% Though it included much automation, the existing X-15 kept
%% the pilot in command through fly-by-wire controls \cite[p.
%%   55, 61--62, 77]{hypersonics}, pilots had  
%% reason to doubt the capabilities of computerized guidance and
%% control systems.\footnote{Even the fly-by-wire control was somewhat
%%   controversial. Milt Thompson, a test pilot in the X-15 program, said
%%   of it: ``you would like him [the engineer responsible] to be in the
%% airplane with you to be exposed to any adverse results'' \cite[p.
%%   55]{DM}.}

Pilots---prospective astronauts---were not about to find themselves
cut out of the control loop. As Mindell chronicles in \emph{Digital
  Apollo}, their resistance involved professional pride and concern
about reliability. Al Blackburn's response to von Braun was
particularly empassioned, and recounted his many experiences with 
``brain-dead autopilots, broken fire control systems, and failed cockpit
computers'' \cite[p. 68]{DM}. Pilots had  
reason to doubt the capabilities of computerized guidance and
control systems. Existing X-15 fly-by-wire control \cite{hypersonics} was even
  controversial. Milt Thompson, a test pilot in the X-15 program, said
  of it: ``you would like him [the engineer responsible] to be in the
airplane with you to be exposed to any adverse results'' \cite[p.
  55]{DM}. But human resourcefulness was one answer to automated
technology's lack of reliability. 

%%TODO2 can you find this exact TERM used?
%%the shibboleth ``third world man'' was waiting on the outcome of the
%%space race to decide whether to be communist or capitalist 

%%  Mercury, which began the
%% manned space program in the United States, had an ambivalent
%% relationship to its astronauts. They were pilots, rather than von
%% Braun's ``missile riders,'' but only just.

%% Like the warheads of the ballistic missiles from
%% which the program spawned---and which were in fact used to convey the
%% Mercury astronauts into space---the astronauts were launched by
%% automated rockets, and returned to the ground on a purely ballistic
%% trajectory.

%% Apollo was, among many
%% other things, a nationalist (and even perhaps imperial) PR campaign.
%% Spurred on by Russian successes (Sputnik, Gargarin), which were
%% originally of little political interest until the media reporting on
%% the events presented them in a frantic light as a possible sign of
%% Soviet technological dominance,\footnote{There was little coverage in
%%   Russian papers about the Sputnik launch (and even Gargarin was not
%%   greatly covered). However, the media frenzy that ensued in the United
%% States and other countries worldwide made clear the political
%% importance of being first in space. See \cite{bessonov}.} Kennedy pushed for a greater
%% investment in space research.

%% While Mercury preserved an
%% aeronautical joystick, which controlled the reaction control thrusters
%% (RCS) and thereby allowed the astronauts attitude control over their
%% space ``capsule''---nomenclature that would soon find itself under
%% siege \cite[p. 85]{kauffman}---they could not fly the Mercury craft in any
%% aeronautical sense. But the presence of the human astronaut was greatly
%% important to the Mercury project, even if he (they were all men)
%% largely flew as ``Spam in a tin can'' \cite[p. 60]{wolfe}. Putting a man in
%% space was a political venture more than a scientific one or a matter
%% of national defense.\footnote{Scientists held and continue to hold
%%   that unmanned missions are more effective in generating scientific
%%   discoveries than manned ones.} In the eyes of the world---or at
%% least as interpreted by Washington---the
%% reputation of the United States depended on it. Project : ``other countries of the free
%% world---troubled and restless'' were waiting outcome of the space race to
%% decide whether to be communist or capitalist \cite{KennedySep7}. 

%% Kennedy he was ``not that interested in
%% space''\footnote{He continued: ``I think it's good, I think we ought
%%   to know about it,'' but balked at the level of funding necessary
%%   for the manned space program unless it led to a US political and PR
%%   victory.}, but project had to
%% be manned, precisely because human spaceflight captured people's
%% imaginations, and because the Soviets were certain to continue their
%% piloted space flight ventures \cite{KennedyNov21}. Soviet spacecraft continued to be
%% more automated than their Western counterparts, and
%% this automation was seen as a feminization of space:  Soviet
%% spacecraft required no skill to operate, and were therefore,
%% rhetorically at least, less impressive \cite[p. 90]{DM}. 

Such debates over control did not go away, but lasted through each
of the Mercury, Gemini, and Apollo programs. Some of this was
political: Apollo was
conceived of by Kennedy\footnote{Famously ``not that interested in
space'' \cite{KennedyNov21}.} as a program of diplomacy,
which would help to ``win the battle that is now going on around the
world between freedom and tyranny'' by impressing the ``minds of men
everywhere'' \cite{KennedyMay25}. Space missions had to be piloted
because the Soviets were sure to continue their program. Human
roles were important: Soviet spacecraft continued to be
more automated than their Western counterparts, and
this automation was seen as evidence of a lack of skill \cite[p.
  90]{DM}.
Compared to the highly-automated Mercury capsules,\footnote{A change in nomenclature,
  from space ``capsules'' to spacecraft, responded to the national
  importance of human involvement \cite[p. 85]{kauffman}.} the Gemini
craft---intended to demonstrate capabilities necessary 
for a mission to the moon---engaged human pilots in new kinds of
operations. While pilots wanted a role in launch vehicle
guidance,\footnote{Simulator studies were commissioned to identify the
  capacity of humans to 
actually guide boosters into orbit. Spun in a
centrifuge in Johnsville, PA to simulate the immense acceleration of
takeoff, some pilots managed to fly
simulated rockets into orbit, serving as evidence for the importance of
pilots to the ``reliability and flexibility'' of launch vehicles
\cite[p. 72]{DM}.} no 
human would fly a rocket off the launch pad. The importance of orbital maneuvers
 (rendezvous and docking) for the Gemini missions suggested returning
the role of piloting to the human, but as spacecraft
pilots soon discovered, orbital
rendezvous could not be achieved by the
traditional manner of flying.
``Numbers, equations, and calculations'' would be  
required, and were bootstrapped to the pilot's senses with a new
readout, the IVI or Incremental Velocity Indicator \cite[p.
  86-87]{DM}.\footnote{The IVI could be
programmed with particular velocity changes in different axes and
would show visually when those particular burns had been achieved,
so that the human pilot would know when to stop accelerating.}


%% and rendezvous was
%% indeed performed with manual control
%%%%WARNING CITATION STYLE CHANGE%%%%%%%%
%%cites removed below
%  with manual control\cite[p. 84]{DM}
% a backup for automated systems\cite[p. 83]{DM}

%% The development of the Gemini spacecraft---actually started after the
%% Apollo project, and intended to demonstrate capabilities necessary for
%% a mission to the moon to take place---set out to extend the
%% capabilities of Mercury, and incorporate lessons learned from the
%% precursor program. One
%% reorchestration of space flight in the Gemini program involved the
%% human's interaction with the spacecraft through control
%% systems.\footnote{Another change was specifically technical, a
%% re-engineering of the space capsule to allow systems to be installed
%% and serviced from the outside of the craft \cite{NASAGeminiConcept}.}
%% Pilots wanted a role in launch vehicle guidance, and a number of
%% simulator studies were commissioned to identify the capacity of humans to
%% actually guide boosters into orbit. Spun in a
%% centrifuge in Johnsville, PA to simulate the immense acceleration of
%% takeoff, pilots flew
%% simulated rockets: while some tests failed, with astronauts losing
%% control during stage changes, some were able to launch the vehicle
%% into orbit, and these tests served as evidence for the importance of
%% pilots to the ``reliability and flexibility'' of launch vehicles,
%% the same phrasing used to justify the pilot's role in the X-15
%% testing \cite[p. 72]{DM}.\footnote{A few skeptics even argued that the lack of pilots
%% explained the high failure rate of automated rockets \cite[p. 73]{DM}.}
%% Though no American would ever fly the launch stage of
%% the rocket, pilots continued to push for control. 

%% The Gemini pilot
%% interfaced with an inertial navigation system, a digital computer, and
%% an optical star tracker, responsible for tasks that would be too heavy
%% or too difficult to automate, and remaining as a backup for automated
%% systems. The importance of orbital maneuvers,
%%  such as rendezvous and docking, for the Gemini missions suggested returning
%% the role of piloting to the spacecraft pilot, and rendezvous was
%% indeed performed with manual control. But, as
%% pilots (particularly, Jim McDivitt) soon discovered, orbital
%% rendezvous could not be achieved by the
%% traditional manner of flying: in orbit, flying toward an object is not
%% an appropriate way to achieve rendezvous, which requires not only
%% passing close to the target but remaining there for an extended
%% period. Matching velocity means matching orbits, and so ``catching
%% up'' to a target may require going slower to change orbits \cite[p.
%%   86]{DM}. ``Numbers, equations, and calculations'' would be 
%% required, and were bootstrapped to the pilot's senses with a new
%% readout, the IVI or Incremental Velocity Indicator \cite[p.
%%   86-87]{DM}.\footnote{The IVI could be
%% programmed with particular velocity changes in different axes and
%% would show visually when those particular burns had been achieved,
%% so that the human pilot would know when to stop accelerating.} Pilots
%% did not fight the implementation of such  
%% technology, realizing that anything that ``helped them manage the
%% chaos [of space flight] would extend their ability'' \cite[p. 88]{DM}. Gemini is 
%% a triumph of manual control, aided of course by automated read-outs.

The Apollo program started too soon to learn lessons from
Gemini---the designs were largely complete by the time Gemini
missions flew---but arguments about human roles also shaped this
parallel program. Due to requirements of weight, space, and
reliability\footnote{Serious
research was being done in the 1950s on simple automated probes to do a fly-by of
Mars \cite[p. 1]{battin}. But self-contained navigation capability was
largely removed from Apollo due to memory restrictions
\cite{tindallMay12}.} the human beings were placed at
the nexus of 
many devices, including a digital computer and manual controls. Human beings were
again counted on for sextant readings, for initiating appropriate
program modes and monitoring automation systems \cite[p.
  4]{BennettExperience}. The LM's approach
to the surface also involved a complex dance of manual and automated
capabilities \cite{BennettCheatham}. As Allan Klumpp describes his
``hybrid'' system: ``The essence of the
approach phase guidance system is that the LM commander can manually
steer the LM to the selected landing site, yet the trajectory he flies
is produced by an automatic guidance system'' \cite[p.
  129--130]{Klumpp}. These 
historical examples are about neither heroic pilots nor
automated spacecraft, but a successful combination of the capabilities
of both via large-scale networks of people and organizations.

%% Klumpp's self-described ``hybrid'' system accounts
%% for the ``obvious desire of any man to control his craft'' while
%% providing both flexibility and safety \cite[p. 129-130]{Klumpp}.

%% One major locus of such controversy was the Apollo
%% guidance computer: Should it allow fully autonomous (from Earth)
%% operation? What functions should be fully automated? The idea of an
%% autonomous navigation system was not fundamentally new.\footnote{Serious
%% research was being done in the 1950s on simple automated probes to do a fly-by of
%% Mars \cite[p. 1]{battin}.} 
%% Though self-contained navigation capacity was a goal
%% for the engineers at MIT responsible for the guidance system---working
%% under Charles Stark Draper, who was also involved in developing
%% guidance systems for ICBMs---much of this functionality was ultimately
%% removed due to computer memory restrictions \cite{tindallMay12}. The Apollo
%% spacecraft would have to be in contact with Earth for guidance
%% corrections and instructions over the long term.
%%   Armstrong,
%% famously, took manual control on the Apollo 11 
%% lander to seek out a cleaner landing spot and avoid a potentially
%% dangerous crater \cite[p. 3]{DM}. But even this story is
%% simplistic. 
%% When conceived of as a model for automated vehicle
%% engineering, the history of manned space programs does not fit the
%% popular narrative of automation: the missions depended on human and
%% automated systems both, and the successful integration of their roles
%% and capabilities. 


%% ---Gargarin could not
%% operate manual backup functionality without unlocking a combination
%% lock, the
%% code to which was only reluctantly placed in the cockpit in a sealed
%% envelope rather than being radioed from the ground \cite[p.
%% 89]{DM}--
%% This reading was
%% supported some years later in 1963 by the flight of Valentina
%% Tereshkova, the 
%% first woman in space (a milestone not followed by the United States
%% until the 1980s). This automated approach perhaps fit a ``communist ethos,'' sending
%% the representatives of every-man and every-woman into
%% space,\footnote{Neither Gargarin nor Tereshkova had aristocratic
%%   family backgrounds.} but did
%% not fit with the ideally meritocratic culture and the political aims
%% of the US program, not to mention the flying culture of the astronauts
%% ultimately selected for the program.\footnote{All those selected for
%%   the Mercury program were military test pilots, despite NASA's
%%   existence as a civilian agency. Issues of secrecy alone
%% should not have disqualified other military personnel, and a greater
%% emphasis on science in the program would have suggested actually
%% including scientists on flight teams, as Mindell notes. Though over the recruitment
%% rounds the emphasis on test-pilot status decreased in favor of greater
%% engineering knowledge (and higher degrees), and though one geologist
%% set foot on the moon during the final Apollo mission, the astronaut
%% culture was dominated by influences from the test-pilot community.}


%% Robert Voas,
%% responsible for astronaut selection and training, was a firm believer
%% in the human role both for monitoring and managing automated systems,
%% but also 
%% navigation, communications, and performing research \cite[p. 77]{DM}.
%% Monitored automation, which prevailed in Mercury---with
%% the human ``more than secondary, if still less than primary'' \cite[p.
%%   77]{DM}---continued into the Gemini program. 

%% But Mindell holds that advancing
%% technology was seen to be served by the inclusion of more human
%% control, which Mercury (in association with simulator tests) had proved was
%% feasible \cite[p. 84]{DM}. 

%% and overall
%% liked piloting the Gemini craft, which they saw as expressing the
%% ``national character'' through the importance of the individual pilot
%% to the vehicle's operations


%cites removed
% missions actually flew\cite[p. 93]{DM}
%% despite the
%% ultimately high levels of automation present with the Apollo
%% spacecraft,

%% The Apollo
%% project pulled from previous experience in digital computer design and
%% development, both from military ICBM programs which often used
%% self-contained inertial guidance units, and from work by Laning and
%% Trageser from the Instrumentation Lab at MIT on a small Mars probe,
%% driven by ``self-dependent'' navigation, which could ideally use
%% complex logic to respond autonomously to problems \cite[p.
%% 99-100]{DM}.

%% and the spacecraft 
%% could operate in a self-contained fashion for some length of
%% time, albeit with greater positional errors than when a ground
%% reference is used \cite[p. 191]{BennettCheatham}.

%% While the initial
%% idea for lunar landing was that the 
%% computer would guide the spacecraft down, given appropriate landmark
%% fixes from pilots, this was not how the LM was flown: despite
%% possessing an autopilot capable of automatic landing, pilots used
%% their prerogative of ultimate control to take over at various stages
%% of landing:

%% While the Apollo engineers perhaps
%% got away with a lot of automation, in part because the astronauts were
%% busy with Gemini instead, manual controls still proved critical to the
%% overall operation of the system. But t

%% presidential science advisor and later president of MIT, and other
%% scientists were perennially against manned
%% spaceflight.\footnote{Wiesner not only disagreed with the importance
%%   of manned space flight, he took issues with the decision to perform
%%   Lunar Orbit Rendezvous, LOR, as the Apollo mission mode. He
%%   continued to oppose LOR after the official NASA announcement, and
%%   argued with von Braun in front of the
%%   press at Marshall Space Flight Center, until President Kennedy
%%   stepped in to end the argument \cite[p. 43]{Seamans}.} Unmanned exploration is
%% cheaper by orders of magnitude \cite[p. 66]{coxMurray}. To serve
%% scientific goals, humans are not required, and in his role as chair of
%% the Presidential Science Advisory Committee, Wiesner felt that his
%% duty was to argue against manned missions on scientific
%% grounds \cite[Chapter 2]{Levine}.\footnote{While Killian's list of goals for the
%%   space program included the human need to explore, national defense,
%%   national pride, and scientific experiment, and while the
%%   congressional hearings about the space program said little about
%%   defense, trading it for discussion of propaganda value and a
%%   ``vertiginous rhetoric'' of scientific exploration, manned space
%%   exploration is in general not justifiable by scientific ends \cite[p.
%%     194-197]{smithSelling}.} Instead of a manned program, scientists

\subsection{Researchers and ``Robot Geologists''}

 While these manned missions were occurring, scientists
 pushed for unmanned exploration,\footnote{Jerome
 Wienser, chair of the Presidential Science
   Advisory Committee, and felt that his duty was to argue against
   manned missions on scientific grounds \cite[Chapter 2]{Levine}.}
 which is cheaper by orders of magnitude
 \cite[p. 66]{coxMurray}. But even for
unmanned exploration systems in space, autonomy is not the
end-all-be-all goal. More important is the link between human
operators and scientists on the ground, and the remote science
platform responsible for carrying out instructions. 
While people may
think of the Mars rovers \emph{Spirit} and \emph{Opportunity} as
autonomous robots remotely carrying out science experiments---Clancey
describes them often showing up anthropomorphized as a ``robot 
geologist'' or 
``explorer'' in news coverage \cite[p. 7]{clancey}---they are
in reality telerobotic systems, remotely carrying out human commands.
The rovers (MER) are not 
sent out to wander or find goals by themselves. They drive mostly
blind, with only their immediate obstacle detection, following manual
waypoints entered by human navigators on Earth: their autonomous
path-finding systems are much slower, and use more power, and are
therefore generally avoided \cite[p. 118]{clancey}. While the rovers
are out of contact for the span of two weeks during solar
conjunctions, the instruments lie dormant and the rovers sit
still \cite[p. 25]{clancey}. Though the rovers are technically capable of
carrying out pre-planned science sequences during this time, including
automatic navigation, this capability is not used: human observation
is too important and
autonomous operation too risky. Human
scientists define sites of interest, locations to take samples, and
the paths to reach them most safely and effectively. Scientists
contact the rovers at least once per day to relay plans, and again
to retrieve results, with more intense schedules during the early
parts of the program \cite[p. 58]{clancey}. Small sets of
operations are requested, and the outcomes monitored, with new plans
being made by humans to account for new data at each step.

%% This in some ways
%% represents greater engagement between scientists and the remote
%% machine than during the earlier \emph{Viking} mission, in which weeks
%% were spent writing programs to run the robot \cite[p. 58]{clancey}.
The recent missions step back
from the programmatic operation of the earlier \emph{Viking} landers, toward closely
coupled human control and
supervision. The Mars rovers could
certainly have been made more autonomous, but this would have opposed
their function. The quality of the mission depends on ``aspects of the
MER's design that promote the \emph{agency} of the scientists''
themselves, rather than automated operation specifically \cite[p.
  xii]{clancey}. Scientific work in the field is ``opportunistic, 
serendipitous, and incremental'' \cite[p. 32]{clancey}, yielding
not so much to \emph{a priori}
plans of great detail, but Suchman's ``situated
action'' \cite{suchmanSA}. Researchers on the ground actually
experience a sense of ``telepresence'' through these ``synergistic''
machines, created 
by virtue of their closely coupled operations and the MER's
semi-anthropomorphic bodies \cite[p. 55]{clancey}. Scientists see
as if they are the rover, and have to ``retool'' their thinking, to
become the ``mind'' of the rover and plan its work in a ``symbiotic''
way \cite[p. 106, 110, 118]{clancey}. The system's autonomy does not replace the
scientists, but allows them to do more of what they want to do (and are
best at), and less of what they don't: some types of automation could reduce the
autonomy of the scientists, and their ability to act creatively and
spontaneously to capitalize on new findings \cite[p.
  118-119]{clancey}. This is a subtle point about automation design
and human agency, one
worth considering when evaluating designs of automated cars.

%%\subsection{Unmanned Exploration and the Researcher}

As Clancey suggests, autonomy is not an ``inherent property of
technology'' but ``a relation between people, technology, and a
task-environment'' and should be considered in those terms \cite[p.
  119]{clancey}. We might say, colloquially, that 
``\emph{Opportunity} encountered'' something \cite[p. 8]{clancey}, but
this is convenient shorthand for scientists on Earth encountering it
through the telerobotic platform. Human knowledge,
perception, and common sense are integral to rover missions, and
organizations operating expensive 
technology in a high-risk environment quite reasonably want humans to
be responsible
for the well-being of the equipment. The same issues play out with
robotic vehicles for underwater exploration. The most famous of these,
Alvin and Jason, deeply involve human scientists as operators in real
time, either directly from within the vehicle or through a tethered
link \cite{NOAA1} \cite{NOAA2}. Most autonomous underwater
vehicles (AUVs) are not 
truly autonomous. This is largely an issue of risk
aversion. While it has often been assumed, even sometimes by system
designers, that long duration missions will be performed autonomously,
this is not the primary mode in which AUVs are operated: the last
thing you want to happen, as a scientist presiding over an expensive
piece of equipment, is to lose the vehicle irretrievably or have an
instrument fail on the first day and not know about it for weeks. So
instead of long, autonomous missions, most operations involve many
shorter missions with low-bandwidth acoustic links coupling the device with
shipboard researchers.\footnote{David Mindell, discussion with the
  author, September 3, 2014.} 

%% , but are operated remotely using low-bandwidth data
%% links via acoustic communications.\footnote{David Mindell, discussion
%%   with the author, September 3, 2014}

%% , similarly to how
%% geologists use the automated capabilities of the Mars rovers to
%% increase theirs.

%% Similarly to how pilots use autopilot systems to increase their agency!

%% having
%% one's vehicle go missing on the way to or from a parking garage while
%% operating autonomously would be a significant issue, and the risk of
%% this kind of failure

%% ---allowing them to do more of what they
%% want and less of what they don't---

While self-driving cars would not
be much like Mars rovers or underwater robots, telerobotic exploration
provides potential lessons
to be learned in terms of human and machine roles, and the level of
autonomy one wants from a machine in a human-machine relationship.
Automation can be a tool to increase 
human agency, both for pilots in the air and geologists on Earth.
Considering what type of interaction promotes the agency
of human owners and occupants is particularly important,
especially while recognizing that the answer to what humans want to do
is not necessarily ``nothing.'' And while local streets are not as
remote and inaccessible as Mars or the bottom of the ocean, risks of
loss and failure must still be considered in the human-machine system
design. Cars are capital-intensive pieces of equipment, and we
generally want to know where they are at all times. Whether vehicles
are monitored
from within or without, it seems unlikely designers will be able to
escape the need to loop the human into the decision-making process.

%% But there is reason to suggest that even this picture of supervision
%% and monitoring on a long timescale may be too ambitious for automated
%% vehicles of the near future. As we have discussed, a number of major
%% engineering problems require solutions before we can build cars that
%% are capable of fully automated operation in all reasonable situations,
%% which implies more than occasional supervision is still required:
%% human engagement in
%% sensing and reacting, on reasonable time scales, remains necessary.
%% The
%% specter that I have been dodging so far, throughout this chapter and
%% indeed through much of the thesis, is the issue of human attention.
%% Aircraft automation represents a prominent public face for automated
%% vehicle design, conceptually connected to the creation of
%% ``autopilots'' for cars. It has long had to tangle with these issues of
%% monitoring. 

\section{From the Driver's Seat}

Aerospace experience with automation parallels some developments
in the automotive realm. Computerized aids are sites of contradictory
feelings and pressures: 
cruise control, when first introduced commercially as Chrysler's ``Auto-Pilot,''
was described as ``faintly
ominious'' by \emph{Popular Science}, but nevertheless seemed like a
``genuine help'' for reducing
fatigue \cite{rowsomePopsci}.
It is well enshrined within legal principles that drivers using
cruise control and even automated driver aids are legally responsible
for controlling their vehicles at all times, and yet these same aids
may reduce attention and ability to react in an emergency
situation---the other side of reducing fatigue.
Traction and stability control are now required for all cars in the US
market \cite{brookingsLiability}, while drivers' personal identities still modulate the
extent to which such
advancements are seen as valuable, or how often the features get
turned off in day-to-day use. 

Deep controversies exist among automotive
enthusiasts about the proper roles of all-wheel-drive, traction
control, and automatic transmissions in the driving process. Even ABS,
generally accepted today as a positive technology that enhances safety and
performance, is in some senses controversial; this skepticism is
motivated by the idea that 
a professional driver utilizing fully manual ``threshold braking,''
developed through deep experience and human skill, can outperform the
computerized system.\footnote{There are innumerable forum posts on
  this topic scattered around automotive forums, and arguments over
  physical principles and the capabilities of the technology in
  different road conditions are
  intertwined with issues of masculinity and implicit driving skill.}
Human and automated capabilities take on a special role in
  high-performance racing, which, despite involving technology, puts the
  human being at the center. Modern racing cars depend on
much accumulated engineering expertise, but racing is as much about
the drivers as the cars, and so inventiveness is often tempered by
rules that disallow certain technologies:  ABS and traction control are
banned from Formula 1 racing, for example. As Carlos Martinez-Vela
describes, regarding NASCAR: ``as important as engineering science has
become ... given
that every team runs virtually the same technology, what makes a
difference in performance is the `human element.''' The human is the ``only data acquisition
system'' allowed at the track, and the abilities of human beings to
sense and describe performance characteristics, and to work together
as a team, are paramount in this community \cite[p.
  178]{martinezvela}. New consumer performance vehicles---like the 
highly computerized Nissan GT-R, with a plethora of all-wheel-drive
systems---are criticized by some as ``too easy to drive quickly'' \cite{edmunds}.
An overly computerized vehicle is to some soulless, unexciting, too much like
a video game: these vehicles may derisively be said to be for the
PlayStation generation, even while others argue for the superior
abilities that advanced technologies convey upon human
drivers,\footnote{This position stumbles upon a sort of hybrid
  identity: the  human gains through
interaction with the technology.}
turning an average driver into a hero or track-star. These gains and
losses are not all easily quantifiable: while accident rates or track
times are easy to measure, subjective judgments involving drivers'
identities are not, and may be similarly important.

% can cite http://www.ctvnews.ca/autos/nissan-s-new-gt-r-model-for-2014-is-this-a-supercar-for-the-playstation-generation-1.1550159

\section{Conclusion}

% STAGE DIRECTION: we have seen that the history is more nuanced than
% is often recognized in this discussion; the imagined narrative is
% not issuing only from here; much of it is about expectations and
% dreams, shaped by visions of the future from technologists and
% others; it behooves us to take a better look at these future
% visions, and examine their assumptions and outcomes in detail; while
% we do so, we try to answer the question: Why do these kinds of
% narratives and assumptions matter in our dreams?

This historical journey has attempted
to shed light on some of the unspoken assumptions behind autonomous
vehicles that shape the popular narrative toward
particular visions of automation and away from others. From factory
automation, through AI and robotics, and applied automation in
multiple types of vehicles, we have seen that history does not support
an assumed trend toward ever increasing automation and complete
replacement of the human being. Instead, automation is partial,
contingent, oriented toward particular tasks and the extension of
human agency in specific ways. It shifts the role of labor, allowing
the concentration of creative control in management and in those who
design the automation systems. It emerges from a multiplicity of ideas
about designing effective or ``intelligent'' systems, which provide no
simple answer for replacing human abilities. It extends the capacities of
pilots, geologists, and astronauts, without replacing the necessity of
judgment and decision-making in when and how to apply automated
capabilities. And it is already contested within enthusiast
communities due to their particular desires and identity politics.
These specificities are often absent
from our dreams of self-driving vehicles, and these dreams deserve
more careful examination for the consequences of the futures they envision.



%% Now, however,
%% let us turn to consider the stakes of the 
%% visions we have investigated. Why do these kinds of narratives and
%% assumptions matter? And what 
%% reasons might we have for investigating alternatives?

%%CONCLUDE


%% 3) (or worked in to each of the previous) current research
%%--not historical, but actual last-20-years papers about this
%%--examples of where ideologies bleed through!!!!
