\chapter{Ideologies and Their Stakes}

Ideologies (and their stakes) (32 p)

\subsection{Automation of Work}
%% 1.1) deep history is the mechanization/automation of work (4 p)
%% --going back to factories/IRev: replacing human competencies with
%% machine competencies and moving the people into new roles
%% --which engages old debates about the role of the human

Human technological progress since antiquity has
involved continual re-negotiations of human labor and the roles of
animals and mechanisms in the labor process. Due to a confluence
of factors---the continuing miniaturization of computing technology, new
advances in machine learning and artificial intelligence algorithms, a
gradual increase in battery capacities, faster wireless networks---the
horizons for everyday automation are broader than ever. 

The popular visions of this technology focus on the future: the
idea that in just two decades the majority of cars on the road will be
fully autonomous. Even respected business information and consulting
bodies have bought into this dream.\footnote{For example, the IHS
  predicts 54 million such vehicles by 2035, which is not as extreme,
  but still a sizable fraction of road vehicles \cite{IHSstudy}} In these vehicles, the users would
step in, select a destination, and would then be free to read, sleep,
watch a movie, answer emails, or otherwise occupy themselves without
needing to pay any attention to the operation of the vehicle. While this
vision has its benefits, it makes many people nervous about
ceding their driving agency to a computer system.\cite{clytton} News articles fret
about what will happen when no one knows how to drive manually any
more,\cite{pross} a classic fear of ``de-skilling'' that is implicated in so many
other implementations of computers. Furthermore, coexistence with
autonomous or automated systems is presented as a fundamentally new
situation, as if human beings had never before had to work and live
with and next to automated systems, presenting new benefits and
dangers, and requiring new roles for their human tenders.

However, automation already has a deep
history in the industrial sector. Current debates and fears about de-skilling, human jobs,
and the role and value of human labor return us to questions that have
plagued factories, and labor's relationship to machinery, since the
early Industrial Revolution.

Robotic cars are sometimes situated as
the next step for robots after the factory, their final emergence into
the real world having conquered the factory floor.\footnote{See for
  example ``Robot Vehicles'' in RobotWorx \cite{robotworx}, which describes automated cars as having the sensors
  industrial robots have had for many years.} But just what the
processes of standardization, mechanization, and automation (or
``automatization'') have done to the factory, and to laborers in it, is
not clearly understood among many who write about autonomous cars. Within this
forgotten history---which is substituted for by an imagined person-less
factory that does not exist in the real world---there are even lessons to be
learned by the research community. This past is relevant, perhaps
more than ever, to the future of transportation.

A search for the beginnings of industrial automation takes us to the middle of
the 18th century: Vaucanson's mechanical loom dates to 1741, and formed the basis of
 later developments in weaving by Joseph Marie Jacquard.\cite[p. 9]{dieboldImpact} 
But the first true example of industrial automation originating in the
United States does not come until Oliver Evans' work in the 1780s on
automated grist mills.\cite[p. 5]{roesmithYankee} Through a series of elevators and descenders,
horizontal screws, spreaders and rakes, his mill moved grain from raw
agricultural commodity to finished product: sifted flour. And ideally,
all parts of the process would occur without human intervention. 

In reality, the process both increased efficiency and decreased the costs
of production, so much so that the same basic machinery is still in
use today in some smaller milling operations.\cite{wyegrist} None of his individual
inventions---which he lists as the elevator, ``conveyer,'' hopper-boy,
drill, and descender in his 1795 miller's guide---was a particularly
groundbreaking achievement, but what Evans did was place these devices
in succession so as to allow continuous production, and the
elimination of many slow human jobs that degraded the quality of the
product by tracking dirt and contaminants around inside the
mill.\cite[p. 203]{evansMillguide} 

It took some time for the high level of automation found in the Evans
mill to spread across other industries, and the mill may find its
closest cousins in the ``automatic'' factories of the 1950s and 1960s
and the roboticized factories of today, but Evans's contemporaries
were not uninterested in increasing efficiency and output. Paul
Revere, one of America's early industrialists, applied shifts in
manufacturing techniques to transition himself from an
artisan worker to manager and overseer of others over his long
metalworking career.\cite[p. 187]{martello} Like a small number of his postrevolutionary
contemporaries, he improved his circumstances by becoming a manager
and owner rather than a laborer, but manufacturing itself was a site of public
debate, pitted against the ``inherent virtue'' of agricultural pursuits.
Tench Coxe, a political economist, wrote in 1810 that ``new machines
and power sources allowed even greater productivity with less labor,
further underscoring the connection between technology and republican
virtue.''\cite[p. 217]{martello} To Coxe's romantic view, these machines  worked ``as if they
were animated beings, endowed with all the talents of their inventors,
laboring with organs that never tire, and subject to no expense of
food, or bed, or raiment, or dwelling.''\cite[p. xxv]{coxe}

But these romantic words did not represent the whole reality of
industrial machine labor. Human labor of
maintenance and supervision is implicit in these manufacturing
machines, but it is rendered invisible by the rhetoric that the
machines themselves require no bed or board. At the same time, Coxe's
use of the word ``endowed'' should focus our attention on exactly which
of the ``talents'' of the inventors have been automatized, and the human
labor necessarily involved in that conferring of capabilities. 

Arsenal practice was the site of multiple revolutions in U.S.
manufacturing technology through the 1800s and early 1900s, notably
the development of the so-called “American system” of interchangeable
parts, through which a gradual increase in mechanization would seem to
continue, driven by the tight tolerances necessary for this production
method. And yet armorers and managers at Harper's Ferry resisted the
mechanization of their craft,\footnote{Blanchard's many automatic
  machines for making gunstocks were of particular
  importance,\cite[p. 56]{roesmithHarpers} but a wide variety of machinery
  was implemented in the gun-making process at Harper's Ferry. Most of
 these machines still required significant human labor. Nash's
 barrel-turning machine in part mechanized the production of barrels
 of standardized dimensions: it consisted of a lathe on a wooden
 frame, with human-operated props to hold the barrel in
 place.\cite[p. 119]{roesmithHarpers} The worker also had to continually
 measure the barrel with a caliper, and adjust the device's chisel
 appropriately.\cite[p. 121]{roesmithHarpers} This gradual implementation of
 mechanized labor was continued in further machines produced by Hall.
 His straight-cutting machine, an early version of a milling machine,
 had as its distinctive feature the ability to be tended by ``common
 hands'' without a loss of
accuracy.\cite[p. 239]{roesmithHarpers}}, and while certain competencies were
transferred from the skilled worker
to the technical apparatus, human oversight and operation was still
integral to the production of weaponry using the new technology. It was
not clear until after the fact that more mechanization was necessarily
better: Harper's Ferry remained ``competitive'' with Springfield's
costs through the mid 1830s.\cite[p. 324]{roesmithHarpers}

The same exchange of competencies characterized Ford's assembly line
production as well, which is cited by David Hounshell as the rise of
true mass production in America.\cite[p. 217]{hounshell} Ford's factory developed fixtures
and gauges, designed to allow for use by unskilled machine tenders. As Donald
Norman writes in Things that Make us Smart---itself, reminiscent of Ed
Hutchins's cognitive anthropology and work on distributed cognition
systems---``the world remembers things for us, just by being
there.''\cite[p. 147]{normanThings} But while the gauge simplifies the
assurance  of quality, it does not
automate it: it simply changes the effort from a more complex judgment
of quality and measurement to a simpler one. While he instituted the
five-dollar day to attempt to solve labor problems at the factory, and
compensate laborers for becoming part of the ``production machine,''
Ford also attracted a wide variety of well-educated skilled mechanics
to his automobile plants.\cite[p. 223]{hounshell} Like Evans, Blanchard, Hall, and others
before them, these mechanics applied their skills to design machines,
and simplify and standardize work processes. The individual judgment
of the assembly line laborer was displaced into standardized tools and
fixtures, built into these technologies by the labor of skilled
machinists and designers.

Meanwhile, Taylorism in factories created ``new managerial
functions'' performed by ``new classes of people with new titles and
more clearly specified responsibilities.''\cite[p. 120]{aitken} A focus on the
people---who are they? where are they? what are they doing?---shows that
one of the fundamental and enduring characteristics of Taylor's
system, the expansion of management roles and the further division of
labor, is not about mechanical automation but about new and altered
types of human work. Industrial processes in the early 1900s continued
the removal of management and strategic decision making from the
workers most physically engaged in product production, installing it
instead within formal organizational structures and the employees that
constituted them. 

This pattern of delayed recognition and contingent change repeats for
numerical control in assembly line production. Numerical control (NC),
developed in the 1940s and 1950s as an outgrowth of World War II
research into feedback systems, slowly began to produce industrial
robots that could perform factory tasks without direct human
intervention. Robots slowly began to replace assembly line jobs such
as spray painting and welding, but adoption was gradual, with only
about 6,000 robots in use in American factories by the mid
1970s.\cite[p. 159]{nyeAmericas} Industrial robots, while automating tasks,
had a way of generating large contingents of skilled human laborers
who still needed to be paid for their services. Industrial robots were
complicated, and needed a variety of skilled workers to tend them, and
to repair them when the broke down. These early experiments did not
increase profits because of the volume of highly skilled labor needed
to keep the machines operating.\cite[p. 162]{nyeAmericas} The development of NC machines
proceeded with a specific interest in eliminating skilled workers, but
the jobs that disappeared were largely unskilled or semi-skilled
laborers.\cite[p. 164]{nyeAmericas} And while Norbert Weiner, in his 1950 book \emph{The Human Use of
Human Beings}, prophesied the end of ``deadly uninteresting'' jobs, which
would be mechanized within 20 years, such changes have still not
totally come to pass.\cite[p. 161]{nyeAmericas} To compound the problem, new industries of
skilled workers---record-and-playback machine designers, and NC machine
programmers---sprang up to furnish factories with their tools.

The process of standardization,
mechanization, and automation has been a process of attempting to
wrest control of the production of components from those closest to
handling them and concentrating it in the hands of management. The Air Force's Integrated Computer-Aided Manufacturing program (ICAM)
brings to light further complexities in the story of the automated
factory: ICAM attempted to aid shop floor automation by automating certain
management functions, ``to try to reduce the enormous indirect costs
that have resulted from the effort to reduce labor costs and remove
power and judgment from the shop floor,'' costs that have continued to
dog new rationalization strategies.\cite[p. 330]{nobleForces} ICAM, like the mythical
ouroboros, sought to offer automation as the ``solution to the problems
generated by automation,'' providing automated scheduling functions,
inventory control, and design tools to ``provide better management
control'' and ``free management from excessive routine duties to do
creative work''---the creative work that the management had attempted to
place in their own hands, in the first place, through earlier
processes of rationalization.\cite[p. 330]{nobleForces}  Automation, Noble's ICAM example shows,
can be used both to routinize work---for the manual laborer---and to
eliminate the routine in favor of the creative---for managers and newly
generated classes of creative workers. 

%% Noble cites 330, 330
Automation may look very different depending on where in the hierarchy
a person happens to fall, but the historical lesson is that human
involvement remains, though altered in space, time, and kind. As John
Diebold pointed out in 1953, there will be ``no worker-less
factories as a result of automation.''\cite[p. 63-64]{dieboldNew}

So we come to our first of many contradictions at the heart of
automated vehicle research. Their promise:
to provide to us a greater measure of creativity in the act of
driving, to remove some of the ``menial'' and routine tasks of manual
control in favor of strategic decision-making. In this analogy, the
driver goes from being the manual laborer to the creative manager. But
despite a focus on relieving tedium, this is not the way these systems
have primarily been envisioned. Instead, in the process of following
the dream of fully automated operation--where the human labor has been
entirely removed from the shoulders of the person in the vehicle, and
displaced to the invisible labors of mapping, programming, and
monitoring---engineers are designing systems where the ``driver''
seems present largely to ensure the operation of capital-intensive
machinery, burdened with new but perhaps even more menial tasks of
machine tending.\footnote{See for example Tom Simonite, “Lazy Humans
Shaped Google's New Autonomous Car,”\cite{simonite}  which discusses
the human role within Google's test vehicle,
and the company's response. This is purely speculation, but due to
the way the system operated previously, it is possible at least one Google
employee fell asleep at the wheel, which was the catalyst for their
concern and change in focus.} 

Why might this be? To understand it, we must understand something
about AI history, and the ideologies intertwined with artificial
intelligence research.

%% This approach
%% presents serious risks. It may be some time before the human inside
%% the car can be entirely disengaged with the driving task,2 if that is
%% even something we want as a culture, which means an interim period of
%% operation potentially characterized by “hours of boredom punctuated by
%% moments of terror.”3 The danger of human inattention,4 which has shown
%% up in aircraft automation with sometimes disastrous results, is
%% actually pushing aircraft manufacturers to more fully involve the
%% human in the process of flying, even as cockpits become more and more
%% computerized.5 And by focusing on ever more automation, rather than
%% appropriate automation, we may also be removing some of the parts of
%% driving that are most enjoyable: by replacing the skilled craftsman
%% with the automaton and the machine tender, we risk making driving
%% sterile and dull. 

\subsection{A Brief History of Artificial Intelligence}
%% 1.2) place to introduce the brief overview of AI history, of what goes
%% into the different approaches
%% --starting from historical visions of automata (1 p)
%% --classical symbolic approaches / physical symbol system (3 p)
%% ----which in a way we have returned to today w/ explicit mapping
%% --Rodney Brooks subsumption architecture and ``world is its own best
%% model'' (3 p)
%% --Leaning heavily on H. R. Ekiba and his approach to illuminating the
%% unstated assumptions of AI research areas (2 p)
Intelligent machines are not a new idea. The myths about Hephaestus and
his creations, notably Talos, a golden female automaton, come to us
from antiquity,\cite{mccorduck}  but continue to be cited as historical antecedents in
literature on autonomous robots and their ethical issues.\cite[p. 3]{patricklin} Automata,
or rather semblances of automata, appear in Hellenic Egypt, with
priests as puppeteers pulling their strings.\cite[Ch. 1]{mccorduck} The history of
artificial life is intertwined with that of autonomous machines: the
creation of Pygmalion's Galatea echoes the same practices and
concerns, as does the story of Mary Shelley's “modern Prometheus” (of
particular note for the purposes of this argument, Shelley's vision of
artificial life is inspired and physically mobilized by static
electricity, the infusing of a ``spark of being'' into the creature; and
electrostatics were a highly public research topic and indeed public
spectacle, complete with live demonstrations, in the 1700s and early
1800s).\cite[p. 44]{shelley} Judah Loew ben Bezalel, a Talmudic scholar, is in legend the
creator of the golem, a being animated from clay who functioned as a
spy against the Gentiles.\cite[Ch. 1]{mccorduck} Though Loew's was not the only golem
recorded in myth, the rabbi occupies a special position among the most
prominent AI researchers of the 20th century: Pamela McCorduck records
that Marvin Minsky and Joel Moses grew up with a ``family tradition
that they are descendants of Rabbi Loew,'' and Moses claims a number of
other American scientists, including John von Neumann and Norbert
Wiener, also consider themselves among the descendants.\cite[Ch. 1]{mccorduck} This is all
to say that while the technological drivers of conceptual visions are
more contemporary, ancient myth and legend continue to subtly underpin
research in autonomy and artificial intelligence.

%% Drosz and Vaucanson
As Jessica Riskin chronicles in her studies of eighteenth and
nineteenth century automata, clever inventors, interested in going
beyond mere representation, created a variety of impressive
simulations of life---that is simulation in its modern sense, meaning
experimental models that can elucidate the natural, rather than its
contemporary sense which would have meant artifice.\cite[p. ??]{riskinDuck}
Makers of automata strove to imitate the very materials of life,
hoping to ``make the parts of their machine work as much as possible
like the parts of living things and thereby to test the limits of
resemblance between synthetic and natural life.''\cite[p. ??]{riskinDuck}
Automata of this era ``bled,'' ``defecated,'' and ``breathed,'' though some
of these functions were themselves faked, such as in the case of
Vaucanson's Duck. Nevertheless, imitation was central to the project,
and in this way these early automata prefigured at least some of the
developments in AI and Alife.

%% Steam governors
But many of these automata, despite being surprisingly accurate
mimeses of life, did little in terms of interaction with the
environment. Meaningful interaction requires closing the loop between
sensing and acting in the manner of a
 ``teleological,'' self-governing mechanism with corrective feedback.
 Indicative of Norbert Wiener's later research into cybernetics, such
 corrective feedback mechanisms had been studied since at least the
 late 18th century, when James Watt incorporated a governor into his
 steam engine. Watt himself had pulled from earlier applications of
 governors in windmills, which had been used since at least the 17th
 century.\cite{richardhills} James Clerk Maxwell, most famous for his equations of
 electricity and magnetism published in the 1865 paper ``A Dynamical
 Theory of the Electromagnetic Field'' (among the most important
 equations in physics), published a paper in 1868 on centrifugal
 governors in steam engines in the Proceedings of Royal Society. This
 paper, ``On governors,'' became one of the central papers in early
 control theory.\cite{ottomayr} Bringing together a number of existing
 areas including control theory,
 cybernetics---from the Greek \emph{cybernetes} meaning
 ``steersman''\cite[p. 6]{wienerMainIdeas}---extended their reach to more complex
 systems: ``control and communication in the animal and machine''. As
 Norbert Wiener puts it, control, or the
 feedback mechanism, is necessary for the extension of information
 theory into communication theory. Cybernetics envisions the world in
 terms of feedback mechanisms, which can be used to explain a variety
 of phenomena in living organisms: homeostasis, balance, and motion
 disorders like locomotor ataxia and Parkinsons all fall within the
 cybernetic sphere\cite[p. 10-15]{wienerMainIdeas}. And all are envisioned
 as outcomes of systems that pass messages internally. In this way
 cybernetics is a forerunner of the discipline of
 artificial intelligence, which is interested in re-creating many of the same
 self-regulating systems within computer systems.

%% 1950s Dartmouth Conf
%% and subsequent winter (search Winston, Six Ages)
Integral to the history of AI as a field is that it was fundamentally
interdisciplinary from the start. Like its forebear cybernetics, it
brought together researchers from physics, mathematics, biology, and
early cognitive science. The field began in earnest with the Dartmouth
Conference in 1956, which brought together many of the enduring big
names in the field. Hosted by John McCarthy (who originated the name
``Artificial Intelligence''), Marvin Minsky, Nathaniel Rochester, and
Claude Shannon, attendees included Trenchard More, Oliver Selfridge,
Ray Solomonoff, Allen Newell, and Herbert Simon: all were united by
``the idea that there was a rigorous and objective way of explaining
the human intellect.''\cite[Ch. 5]{mccorduck} The research areas of
the ``Dartmouth Summer Research Project'' included language learning
and use, ``neuron'' networks,\footnote{Neural networks are one of the
  intriguing long-term stories of AI research, subject of much
  controversy over the years regarding whether or not they would
  actually work. A couple of theoretical developments altered them
  from a curiosity to one of the main techniques in modern AI. This
  half-century journey presaged by the one sentence: ``Partial
  results have been obtained by the problem needs more theoretical
  work''.\cite{dartmouthconf} CAN I FIND THE PERSON WHO SAID SOMETHING
SIMILAR?} self-improving machines, and computational 
creativity.\cite{dartmouthconf} Early successes spurred romantic
predictions, and by 1960, human-level intelligence was predicted by
some to be only a decade away.\cite[p. 3]{winston} Overexpectation, however,
lead to a first AI ``winter'' from about 1965 to 1970, in which the
grand promises of AI were shown to be much further off: the current
techniques simply did not yield advances at the required rate. As Pat
Winston put it: ``Everyone searched for a kind of philosopher's stone,
a mechanism that when placed in a computer would require only data to
become truly intelligent.''\cite[p. 4]{winston} But by the 1970s
research was improving, and excitement building again.


%% DARPA SCI
The early to mid 1980s were also a time of great developments in
Artificial Intelligence, an era of ``celebrity science,'' high hopes,
big investments, and subsequent great public disappointment with the
coming of another ``AI Winter'' beginning in 1987 and
1988.\footnote{These are roughly the dates Russell and Norvig give in
  \emph{Artificial Intelligence} \cite{russellnorvig}} But despite the
warnings of Roger Schank and Marvin Minsky, that overoptimistic
expectations for AI would result in another winter like the previous
one in the 1970s, overall expectations were high, especially within
the business community, which funded companies and assimilated AI
techniques into real applications.\cite[afterword]{mccorduck} Though the 1980s continued divides
within the field about approaches to artificial intelligence, it
actually resulted in a wide variety of successful projects based on
improvements to expert systems, machine learning, natural language
processing, and computer vision.\cite[afterword]{mccorduck} The 1983 US Strategic Computing
initiative, led by Robert Kahn at DARPA, had AI as its third focus
area, with “image understanding” and interpretation—made possible by
the digitized image—as long range project goals. In its revised
10-year plan, the initiative even included an autonomous land vehicle
alongside a pilot's associate and computerized battle management
software. But the project suffered serious management problems, and
was eventually canceled, precipitating the general crash in AI
funding—through which research quietly continued, waiting for another
up-tick in public interest. But the Strategic Computing project,
whatever its lofty goals, was no failure. McCorduck cites Roland and
Shiman as saying that ``AI now performs miracles unimagined when SC
began, though it can't do what SC promised,'' which speaks to the
important developments that were made in the service of DARPA's vision.\cite[afterword]{mccorduck}


%% Classical symbol approaches (3)
Though there are a number of approaches to AI research, different
ideas of how machine intelligence can be achieved, it is instructive
to look specifically at three how AI paradigms envision their project. 

%%specifically w/ reference to ideologies

%% Rodney Brooks (3)

%% Deep learning, ``self-aware'' mario, image recognition and other
%% claptrap (1) (incl. John von Neumann & ALife (See CMS790 paper)



%% 2.1) data gathering and mapping
%% --this approach presupposes large amounts of data collection (maps),
%% and opens the way to more information about the passengers (2 p)
%% --see Google's patents in particular which talk about sensing things
%% like the number of passengers (4 p)


%% 2.2) machine vision
%% --pull from the Spectator paper (4 p)


%% 2.2) what standard of safety? 
%% --better than an average person? or better than the best people? (1 p)
%% --human problems with using projected statistics to define policy:
%% does it make people feel better to know a system is statistically
%% safer when they are uncomfortable with it? see cars/planes for example
%% (3 p)
%% -->perspective of caution? or  ``losing lives every day'' that
%% could be saved?
%% ----how much risk is legitimate?? (2 p)
%% --airlines as a place where we see the different dimensions of
%% statistical safety vs. passenger's feelings of safety (3 p)
%% ---drawing from PARC/CAST documents


%% 2.3) functionalism over understanding
%% --for better or worse these devices are not humanlike in their
%% understanding
%% ----AI winters & the rise of commercial ML AI with much smaller goals
%% (see Wired articles, etc) (3 p)
%% --the ideology that drives them is an engineering mindset: ``just make
%% it work'' (2 p)
%% --pulling the quote from my USC talk: also interesting to note
%% regarding 1.2 that it seems to expect a humanoid robotic chauffeur in
%% a normal car! (1 p)
%% -and of course interviews with people (Ryan Chin; Walker Smith etc.
%% where they matter to this type of vehicle)


