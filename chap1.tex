\chapter{Ideologies and Their Stakes}

Ideologies (and their stakes) (visions/histories) (32 p)

\subsection{Automation of Work}
%% 1.1) deep history is the mechanization/automation of work (4 p)
%% --going back to factories/IRev: replacing human competencies with
%% machine competencies and moving the people into new roles
%% --which engages old debates about the role of the human

Human technological progress since antiquity has
involved continual re-negotiations of human labor and the roles of
animals and mechanisms in the labor process. Due to a confluence
of factors---the continuing miniaturization of computing technology, new
advances in machine learning and artificial intelligence algorithms, a
gradual increase in battery capacities, faster wireless networks---the
horizons for everyday automation are broader than ever. 

The popular visions of this technology focus on the future: the
idea that in just two decades the majority of cars on the road will be
fully autonomous. Even respected business information and consulting
bodies have bought into this dream.\footnote{For example, the IHS
  predicts 54 million such vehicles by 2035, which is not as extreme,
  but still a sizable fraction of road vehicles \cite{IHSstudy}} In these vehicles, the users would
step in, select a destination, and would then be free to read, sleep,
watch a movie, answer emails, or otherwise occupy themselves without
needing to pay any attention to the operation of the vehicle. While this
vision has its benefits, it makes many people nervous about
ceding their driving agency to a computer system.\cite{clytton} News articles fret
about what will happen when no one knows how to drive manually any
more,\cite{pross} a classic fear of ``de-skilling'' that is implicated in so many
other implementations of computers. Furthermore, coexistence with
autonomous or automated systems is presented as a fundamentally new
situation, as if human beings had never before had to work and live
with and next to automated systems, presenting new benefits and
dangers, and requiring new roles for their human tenders.

However, automation already has a deep
history in the industrial sector. Current debates and fears about de-skilling, human jobs,
and the role and value of human labor return us to questions that have
plagued factories, and labor's relationship to machinery, since the
early Industrial Revolution.

Robotic cars are sometimes situated as
the next step for robots after the factory, their final emergence into
the real world having conquered the factory floor.\footnote{See for
  example ``Robot Vehicles'' in RobotWorx \cite{robotworx}, which describes automated cars as having the sensors
  industrial robots have had for many years.} But just what the
processes of standardization, mechanization, and automation (or
``automatization'') have done to the factory, and to laborers in it, is
not clearly understood among many who write about autonomous cars. Within this
forgotten history---which is substituted for by an imagined person-less
factory that does not exist in the real world---there are even lessons to be
learned by the research community. This past is relevant, perhaps
more than ever, to the future of transportation.

A search for the beginnings of industrial automation takes us to the middle of
the 18th century: Vaucanson's mechanical loom dates to 1741, and formed the basis of
 later developments in weaving by Joseph Marie Jacquard.\cite[p. 9]{dieboldImpact} 
But the first true example of industrial automation originating in the
United States does not come until Oliver Evans' work in the 1780s on
automated grist mills.\cite[p. 5]{roesmithYankee} Through a series of elevators and descenders,
horizontal screws, spreaders and rakes, his mill moved grain from raw
agricultural commodity to finished product: sifted flour. And ideally,
all parts of the process would occur without human intervention. 

In reality, the process both increased efficiency and decreased the costs
of production, so much so that the same basic machinery is still in
use today in some smaller milling operations.\cite{wyegrist} None of his individual
inventions---which he lists as the elevator, ``conveyer,'' hopper-boy,
drill, and descender in his 1795 miller's guide---was a particularly
groundbreaking achievement, but what Evans did was place these devices
in succession so as to allow continuous production, and the
elimination of many slow human jobs that degraded the quality of the
product by tracking dirt and contaminants around inside the
mill.\cite[p. 203]{evansMillguide} 

It took some time for the high level of automation found in the Evans
mill to spread across other industries, and the mill may find its
closest cousins in the ``automatic'' factories of the 1950s and 1960s
and the roboticized factories of today, but Evans's contemporaries
were not uninterested in increasing efficiency and output. Paul
Revere, one of America's early industrialists, applied shifts in
manufacturing techniques to transition himself from an
artisan worker to manager and overseer of others over his long
metalworking career.\cite[p. 187]{martello} Like a small number of his postrevolutionary
contemporaries, he improved his circumstances by becoming a manager
and owner rather than a laborer, but manufacturing itself was a site of public
debate, pitted against the ``inherent virtue'' of agricultural pursuits.
Tench Coxe, a political economist, wrote in 1810 that ``new machines
and power sources allowed even greater productivity with less labor,
further underscoring the connection between technology and republican
virtue.''\cite[p. 217]{martello} To Coxe's romantic view, these machines  worked ``as if they
were animated beings, endowed with all the talents of their inventors,
laboring with organs that never tire, and subject to no expense of
food, or bed, or raiment, or dwelling.''\cite[p. xxv]{coxe}

But these romantic words did not represent the whole reality of
industrial machine labor. Human labor of
maintenance and supervision is implicit in these manufacturing
machines, but it is rendered invisible by the rhetoric that the
machines themselves require no bed or board. At the same time, Coxe's
use of the word ``endowed'' should focus our attention on exactly which
of the ``talents'' of the inventors have been automatized, and the human
labor necessarily involved in that conferring of capabilities. 

Arsenal practice was the site of multiple revolutions in U.S.
manufacturing technology through the 1800s and early 1900s, notably
the development of the so-called “American system” of interchangeable
parts, through which a gradual increase in mechanization would seem to
continue, driven by the tight tolerances necessary for this production
method. And yet armorers and managers at Harper's Ferry resisted the
mechanization of their craft,\footnote{Blanchard's many automatic
  machines for making gunstocks were of particular
  importance,\cite[p. 56]{roesmithHarpers} but a wide variety of machinery
  was implemented in the gun-making process at Harper's Ferry. Most of
 these machines still required significant human labor. Nash's
 barrel-turning machine in part mechanized the production of barrels
 of standardized dimensions: it consisted of a lathe on a wooden
 frame, with human-operated props to hold the barrel in
 place.\cite[p. 119]{roesmithHarpers} The worker also had to continually
 measure the barrel with a caliper, and adjust the device's chisel
 appropriately.\cite[p. 121]{roesmithHarpers} This gradual implementation of
 mechanized labor was continued in further machines produced by Hall.
 His straight-cutting machine, an early version of a milling machine,
 had as its distinctive feature the ability to be tended by ``common
 hands'' without a loss of
accuracy.\cite[p. 239]{roesmithHarpers}}, and while certain competencies were
transferred from the skilled worker
to the technical apparatus, human oversight and operation was still
integral to the production of weaponry using the new technology. It was
not clear until after the fact that more mechanization was necessarily
better: Harper's Ferry remained ``competitive'' with Springfield's
costs through the mid 1830s.\cite[p. 324]{roesmithHarpers}

The same exchange of competencies characterized Ford's assembly line
production as well, which is cited by David Hounshell as the rise of
true mass production in America.\cite[p. 217]{hounshell} Ford's factory developed fixtures
and gauges, designed to allow for use by unskilled machine tenders. As Donald
Norman writes in Things that Make us Smart---itself, reminiscent of Ed
Hutchins's cognitive anthropology and work on distributed cognition
systems---``the world remembers things for us, just by being
there.''\cite[p. 147]{normanThings} But while the gauge simplifies the
assurance  of quality, it does not
automate it: it simply changes the effort from a more complex judgment
of quality and measurement to a simpler one. While he instituted the
five-dollar day to attempt to solve labor problems at the factory, and
compensate laborers for becoming part of the ``production machine,''
Ford also attracted a wide variety of well-educated skilled mechanics
to his automobile plants.\cite[p. 223]{hounshell} Like Evans, Blanchard, Hall, and others
before them, these mechanics applied their skills to design machines,
and simplify and standardize work processes. The individual judgment
of the assembly line laborer was displaced into standardized tools and
fixtures, built into these technologies by the labor of skilled
machinists and designers.

Meanwhile, Taylorism in factories created ``new managerial
functions'' performed by ``new classes of people with new titles and
more clearly specified responsibilities.''\cite[p. 120]{aitken} A focus on the
people---who are they? where are they? what are they doing?---shows that
one of the fundamental and enduring characteristics of Taylor's
system, the expansion of management roles and the further division of
labor, is not about mechanical automation but about new and altered
types of human work. Industrial processes in the early 1900s continued
the removal of management and strategic decision making from the
workers most physically engaged in product production, installing it
instead within formal organizational structures and the employees that
constituted them. 

This pattern of delayed recognition and contingent change repeats for
numerical control in assembly line production. Numerical control (NC),
developed in the 1940s and 1950s as an outgrowth of World War II
research into feedback systems, slowly began to produce industrial
robots that could perform factory tasks without direct human
intervention. Robots slowly began to replace assembly line jobs such
as spray painting and welding, but adoption was gradual, with only
about 6,000 robots in use in American factories by the mid
1970s.\cite[p. 159]{nyeAmericas} Industrial robots, while automating tasks,
had a way of generating large contingents of skilled human laborers
who still needed to be paid for their services. Industrial robots were
complicated, and needed a variety of skilled workers to tend them, and
to repair them when the broke down. These early experiments did not
increase profits because of the volume of highly skilled labor needed
to keep the machines operating.\cite[p. 162]{nyeAmericas} The development of NC machines
proceeded with a specific interest in eliminating skilled workers, but
the jobs that disappeared were largely unskilled or semi-skilled
laborers.\cite[p. 164]{nyeAmericas} And while Norbert Weiner, in his 1950 book \emph{The Human Use of
Human Beings}, prophesied the end of ``deadly uninteresting'' jobs, which
would be mechanized within 20 years, such changes have still not
totally come to pass.\cite[p. 161]{nyeAmericas} To compound the problem, new industries of
skilled workers---record-and-playback machine designers, and NC machine
programmers---sprang up to furnish factories with their tools.

The process of standardization,
mechanization, and automation has been a process of attempting to
wrest control of the production of components from those closest to
handling them and concentrating it in the hands of management. The Air Force's Integrated Computer-Aided Manufacturing program (ICAM)
brings to light further complexities in the story of the automated
factory: ICAM attempted to aid shop floor automation by automating certain
management functions, ``to try to reduce the enormous indirect costs
that have resulted from the effort to reduce labor costs and remove
power and judgment from the shop floor,'' costs that have continued to
dog new rationalization strategies.\cite[p. 330]{nobleForces} ICAM, like the mythical
ouroboros, sought to offer automation as the ``solution to the problems
generated by automation,'' providing automated scheduling functions,
inventory control, and design tools to ``provide better management
control'' and ``free management from excessive routine duties to do
creative work''---the creative work that the management had attempted to
place in their own hands, in the first place, through earlier
processes of rationalization.\cite[p. 330]{nobleForces}  Automation, Noble's ICAM example shows,
can be used both to routinize work---for the manual laborer---and to
eliminate the routine in favor of the creative---for managers and newly
generated classes of creative workers. 

%% Noble cites 330, 330
Automation may look very different depending on where in the hierarchy
a person happens to fall, but the historical lesson is that human
involvement remains, though altered in space, time, and kind. As John
Diebold pointed out in 1953, there will be ``no worker-less
factories as a result of automation.''\cite[p. 63-64]{dieboldNew}

So we come to our first of many contradictions at the heart of
automated vehicle research. Their promise:
to provide to us a greater measure of creativity in the act of
driving, to remove some of the ``menial'' and routine tasks of manual
control in favor of strategic decision-making. In this analogy, the
driver goes from being the manual laborer to the creative manager. But
despite a focus on relieving tedium, this is not the way these systems
have primarily been envisioned. Instead, in the process of following
the dream of fully automated operation--where the human labor has been
entirely removed from the shoulders of the person in the vehicle, and
displaced to the invisible labors of mapping, programming, and
monitoring---engineers are designing systems where the ``driver''
seems present largely to ensure the operation of capital-intensive
machinery, burdened with new but perhaps even more menial tasks of
machine tending.\footnote{See for example Tom Simonite, “Lazy Humans
Shaped Google's New Autonomous Car,”\cite{simonite}  which discusses
the human role within Google's test vehicle,
and the company's response. This is purely speculation, but due to
the way the system operated previously, it is possible at least one Google
employee fell asleep at the wheel, which was the catalyst for their
concern and change in focus.} 

Why might this be? To understand it, we must understand something
about AI history, and the ideologies intertwined with artificial
intelligence research.

%% This approach
%% presents serious risks. It may be some time before the human inside
%% the car can be entirely disengaged with the driving task,2 if that is
%% even something we want as a culture, which means an interim period of
%% operation potentially characterized by “hours of boredom punctuated by
%% moments of terror.”3 The danger of human inattention,4 which has shown
%% up in aircraft automation with sometimes disastrous results, is
%% actually pushing aircraft manufacturers to more fully involve the
%% human in the process of flying, even as cockpits become more and more
%% computerized.5 And by focusing on ever more automation, rather than
%% appropriate automation, we may also be removing some of the parts of
%% driving that are most enjoyable: by replacing the skilled craftsman
%% with the automaton and the machine tender, we risk making driving
%% sterile and dull. 

\subsection{A Brief History of Artificial Intelligence}
%% 1.2) place to introduce the brief overview of AI history, of what goes
%% into the different approaches
%% --starting from historical visions of automata (1 p)
%% --classical symbolic approaches / physical symbol system (3 p)
%% ----which in a way we have returned to today w/ explicit mapping
%% --Rodney Brooks subsumption architecture and ``world is its own best
%% model'' (3 p)
%% --Leaning heavily on H. R. Ekiba and his approach to illuminating the
%% unstated assumptions of AI research areas (2 p)
Intelligent machines are not a new idea. The myths about Hephaestus and
his creations, notably Talos, a golden female automaton, come to us
from antiquity,\cite{mccorduck}  but continue to be cited as historical antecedents in
literature on autonomous robots and their ethical issues.\cite[p. 3]{patricklin} Automata,
or rather semblances of automata, appear in Hellenic Egypt, with
priests as puppeteers pulling their strings.\cite[Ch. 1]{mccorduck} The history of
artificial life is intertwined with that of autonomous machines: the
creation of Pygmalion's Galatea echoes the same practices and
concerns, as does the story of Mary Shelley's “modern Prometheus” (of
particular note for the purposes of this argument, Shelley's vision of
artificial life is inspired and physically mobilized by static
electricity, the infusing of a ``spark of being'' into the creature; and
electrostatics were a highly public research topic and indeed public
spectacle, complete with live demonstrations, in the 1700s and early
1800s).\cite[p. 44]{shelley} Judah Loew ben Bezalel, a Talmudic scholar, is in legend the
creator of the golem, a being animated from clay who functioned as a
spy against the Gentiles.\cite[Ch. 1]{mccorduck} Though Loew's was not the only golem
recorded in myth, the rabbi occupies a special position among the most
prominent AI researchers of the 20th century: Pamela McCorduck records
that Marvin Minsky and Joel Moses grew up with a ``family tradition
that they are descendants of Rabbi Loew,'' and Moses claims a number of
other American scientists, including John von Neumann and Norbert
Wiener, also consider themselves among the descendants.\cite[Ch. 1]{mccorduck} This is all
to say that while the technological drivers of conceptual visions are
more contemporary, ancient myth and legend continue to subtly underpin
research in autonomy and artificial intelligence.

%% Drosz and Vaucanson
As Jessica Riskin chronicles in her studies of eighteenth and
nineteenth century automata, clever inventors, interested in going
beyond mere representation, created a variety of impressive
simulations of life---that is simulation in its modern sense, meaning
experimental models that can elucidate the natural, rather than its
contemporary sense which would have meant artifice.\cite[p. ??]{riskinDuck}
Makers of automata strove to imitate the very materials of life,
hoping to ``make the parts of their machine work as much as possible
like the parts of living things and thereby to test the limits of
resemblance between synthetic and natural life.''\cite[p. ??]{riskinDuck}
Automata of this era ``bled,'' ``defecated,'' and ``breathed,'' though some
of these functions were themselves faked, such as in the case of
Vaucanson's Duck. Nevertheless, imitation was central to the project,
and in this way these early automata prefigured at least some of the
developments in AI and Alife.

%% Steam governors
But many of these automata, despite being surprisingly accurate
mimeses of life, did little in terms of interaction with the
environment. Meaningful interaction requires closing the loop between
sensing and acting in the manner of a
 ``teleological,'' self-governing mechanism with corrective feedback.
 Indicative of Norbert Wiener's later research into cybernetics, such
 corrective feedback mechanisms had been studied since at least the
 late 18th century, when James Watt incorporated a governor into his
 steam engine. Watt himself had pulled from earlier applications of
 governors in windmills, which had been used since at least the 17th
 century.\cite{richardhills} James Clerk Maxwell, most famous for his equations of
 electricity and magnetism published in the 1865 paper ``A Dynamical
 Theory of the Electromagnetic Field'' (among the most important
 equations in physics), published a paper in 1868 on centrifugal
 governors in steam engines in the Proceedings of Royal Society. This
 paper, ``On governors,'' became one of the central papers in early
 control theory.\cite{ottomayr} Bringing together a number of existing
 areas including control theory,
 cybernetics---from the Greek \emph{cybernetes} meaning
 ``steersman''\cite[p. 6]{wienerMainIdeas}---extended their reach to more complex
 systems: ``control and communication in the animal and machine''. As
 Norbert Wiener puts it, control, or the
 feedback mechanism, is necessary for the extension of information
 theory into communication theory. Cybernetics envisions the world in
 terms of feedback mechanisms, which can be used to explain a variety
 of phenomena in living organisms: homeostasis, balance, and motion
 disorders like locomotor ataxia and Parkinsons all fall within the
 cybernetic sphere\cite[p. 10-15]{wienerMainIdeas}. And all are envisioned
 as outcomes of systems that pass messages internally. In this way
 cybernetics is a forerunner of the discipline of
 artificial intelligence, which is interested in re-creating many of the same
 self-regulating systems within computer systems. This replacement of
 competencies tends toward the obsolescence of the human being.

%% 1950s Dartmouth Conf
%% and subsequent winter (search Winston, Six Ages)
Integral to the history of AI as a field is that it was fundamentally
interdisciplinary from the start. Like its forebear cybernetics, it
brought together researchers from physics, mathematics, biology, and
early cognitive science. The field began in earnest with the Dartmouth
Conference in 1956, which brought together many of the enduring big
names in the field. Hosted by John McCarthy (who originated the name
``Artificial Intelligence''), Marvin Minsky, Nathaniel Rochester, and
Claude Shannon, attendees included Trenchard More, Oliver Selfridge,
Ray Solomonoff, Allen Newell, and Herbert Simon: all were united by
``the idea that there was a rigorous and objective way of explaining
the human intellect.''\cite[Ch. 5]{mccorduck} The research areas of
the ``Dartmouth Summer Research Project'' included language learning
and use, ``neuron'' networks,\footnote{Neural networks are one of the
  intriguing long-term stories of AI research, subject of much
  controversy over the years regarding whether or not they would
  actually work. A couple of theoretical developments altered them
  from a curiosity to one of the main techniques in modern AI. This
  half-century journey presaged by the one sentence: ``Partial
  results have been obtained by the problem needs more theoretical
  work''.\cite{dartmouthconf} CAN I FIND THE PERSON WHO SAID SOMETHING
SIMILAR?} self-improving machines, and computational 
creativity.\cite{dartmouthconf} Early successes spurred romantic
predictions, and by 1960, human-level intelligence was predicted by
some to be only a decade away.\cite[p. 3]{winston} Overexpectation, however,
lead to a first AI ``winter'' from about 1965 to 1970, in which the
grand promises of AI were shown to be much further off: the current
techniques simply did not yield advances at the required rate. As Pat
Winston put it: ``Everyone searched for a kind of philosopher's stone,
a mechanism that when placed in a computer would require only data to
become truly intelligent.''\cite[p. 4]{winston} But by the 1970s
research was improving, and excitement building again.


%% DARPA SCI
The early to mid 1980s were also a time of great developments in
Artificial Intelligence, an era of ``celebrity science,'' high hopes,
big investments, and subsequent great public disappointment with the
coming of another ``AI Winter'' beginning in 1987 and
1988.\footnote{These are roughly the dates Russell and Norvig give in
  \emph{Artificial Intelligence} \cite{russellnorvig}} But despite the
warnings of Roger Schank and Marvin Minsky, that overoptimistic
expectations for AI would result in another winter like the previous
one in the 1970s, overall expectations were high, especially within
the business community, which funded companies and assimilated AI
techniques into real applications.\cite[afterword]{mccorduck} Though the 1980s continued divides
within the field about approaches to artificial intelligence, it
actually resulted in a wide variety of successful projects based on
improvements to expert systems, machine learning, natural language
processing, and computer vision.\cite[afterword]{mccorduck} The 1983 US Strategic Computing
initiative, led by Robert Kahn at DARPA, had AI as its third focus
area, with “image understanding” and interpretation—made possible by
the digitized image—as long range project goals. In its revised
10-year plan, the initiative even included an autonomous land vehicle
alongside a pilot's associate and computerized battle management
software. The project suffered serious management problems, and
was eventually canceled, precipitating the general crash in AI
funding—through which research quietly continued, waiting for another
up-tick in public interest. But the Strategic Computing project,
whatever its lofty goals, was no failure. McCorduck cites Roland and
Shiman as saying that ``AI now performs miracles unimagined when SC
began, though it can't do what SC promised,'' which speaks to the
important developments that were made in the service of DARPA's vision.\cite[afterword]{mccorduck}


Though there are a number of approaches to AI research, different
ideas of how machine intelligence can be achieved, it is instructive
to look specifically at how three AI paradigms envision their project.
Each is an answer to the question ``how can we build systems that
can operate without humans?''

%% Classical symbol approaches (3)
%%specifically w/ reference to ideologies
The first important AI paradigm is the classical symbolic system
approach. Associated with Allen Newell and Herb Simon, the idea of the
physical-symbol system hypothesis is that ``symbols lie at the root of
intelligent action''\cite[p. 109]{newellsimon}. Therefore not only
does intelligence require symbolic manipulation, it may indeed be
coextensive with physical-symbol systems, in other words a
physical-symbol system has ``necessary and sufficient means'' for
intelligence and intelligent action.\cite[p. 111]{newellsimon} Such
objects are symbol systems in that they contain symbols and processes
that act upon symbols. And they are physical in that they obey
physical laws and are realizable, in reality, through engineering.
These symbol systems would arrive at answers through a technique known as
heuristic search: by looking through a tree of possibilities in an
intelligent way, we arrive at the appropriate answer.\footnote{The key
  point of heuristic search is that such answers are approximate, but
  arrived at quickly, rather than exact, but arrived at slowly or,
  perhaps, never at all.} Intelligence is applied in heuristic search
by the pruning of the tree: rather than having to apply brute force to
search the entire space, an intelligent system applying heuristic
search makes decisions at each node as to which branches are most
likely to produce a good result and searches those.\cite[p.
  124]{newellsimon} As Newell and Simon wrote what makes a problem a
problem is ``not that a large amount of search is required for its
solution, but that a large amount \emph{would} be required if a requisite
level of intelligence were not applied'': the task of intelligence is
to ``avert the ever-present threat of the explosion of
search.''\cite[p. 125]{newellsimon} 

The conceit then, of the physical-symbol system hypothesis is twofold.
First, it assumes that
human beings essentially operate in this manner: that we apply
symbolic logic and heuristic search to provide for our intelligent
actions. Second, it assumes that computers can be true physical-symbol
systems. These two assumptions are not necessary clear. John Searle
essentially rejects the computer as a physical-symbol system in his
Chinese room example. Instead, the computer (room) is seen as a cheap
imitation of such a system: a room into which strange symbols are
passed, the appropriate responses looked up in a book,
and then passed out again, all without anything in the room having
access to their meaning.\cite{chineseSearle} Though meaningless
symbols are being processed by such a contraption, his view is that no
electronic computer ``can really manipulate symbols, nor really
designate or interpret anything at all,''\cite{escapingBoden} This is
a philosophical question of whether or not computers are capable of
true intelligence, not a matter of whether or not they can
convincingly imitate intelligence (and other philosophers of AI, such
as Margaret Boden, do not take Searle's view on the subject). 

The former point proves harder to dodge. While it may be true, as is
fundamental to the field of AI and which even Searle holds, that
machines can think because ``we are precisely such machines,''\cite[p.
83]{chineseSearle} and therefore it should be possible to create
intelligent machines, this does not guarantee that symbol systems are
the way to achieve intelligence. It may well be that introspection on
thought is a large part of why the logical theory seemed so
compelling: we like to think we are logical. Researchers like Newell
and Simon used ``think aloud'' experiments to identify problem solving
techniques,\cite[Ch. 10]{mccorduck} which seems naturally to suggest a logical response: when
asked to describe how we came to some decision, basing it in logic
seems the most acceptable alternative. While we certainly may apply logic
and process symbols, there is good reason to think that isn't how we
spend most of our time. Symbolic logic takes a lot of mental capacity,
so generally, we use other sorts of shortcut processes to come to
decisions. Pollock calls these ``quick and inflexible'' or ``Q\&I''
models\cite[p. 120]{pollock} and Dennett refers to them as ``habitual methods'' or
mechanical routines\cite[p. 157]{dennett} We rely on Q\&I models to do
much of our day-to-day reasoning because it is ``very important for
humans to be able to make rough probability judgments''\cite[p.
  120]{pollock}, and accepting the output of such approximate models
is not at all unreasonable in the absence of evidence for their
inappropriateness to a situation. A logical solution may be difficult
to arrive at in some situations. The physical-symbol system hypothesis
is just that, a hypothesis, and, historically, the classical symbolic
approach did not yield results as quickly as expected. This was part
of the reason for the first AI winter, and researchers moved on to
other approaches. While logic is part of the puzzle, it seems that it
is not the entireity of it. But this was the archetypal dream of early
AI: humanlike intelligence via logic systems.

%% Rodney Brooks (3)
One of those researchers pioneering a new way of doing AI was Rodney
Brooks. Part of a different wave of AI researchers, disillusioned with
the failures of logic-based robotic systems (like Shakey, the SRI
robot named for its tendency to shake when in motion)\cite[Ch. 10 (CHECK)]{mccorduck} to
achieve intelligent results, he posited a new way to build intelligent
robots, defined by the subsumption architecture.\cite[p. 353]{mobilebrooks} Above and
beyond the problems that physical-symbol system based AI had in
relatively controlled domains, robots controlled with these
techniques, overwhelmed by the complexity of the real world, responded
slowly and ineptly. To rectify this, Brooks attempted to cut out
cognition altogether, focusing instead only on sensing and
action.\cite[Afterword]{mccorduck} Rather than attempt to engineer a
human-level intelligence at once, when years of research had failed to
produce anything approaching these results, why not start small?
Insects, after all, lack cognition, but respond more adroitly to the
world than Shakey and its contemporaries. Uncertainty everywhere in
the physical world makes modeling the world extremely difficult: the
model has a tendency to get out of sync, and there needs to be
something to return it to accuracy. By building systems that can
accommodate uncertainty, Brooks believed, more progress could be
made.\cite[p. 347]{mobilebrooks} Instead of modeling the world, treat
the world as ``its own best model'' and focus on embodiment and action
in the world.\cite[p. 256]{ekbia}

Rather than being built into a single integrated system, the
subsumption approach is by definition modular. More complex behaviors
are built out of simper ones: level zero competence might avoid objects,
while the next level would wander aimlessly, and the next would
attempt to wander to places it had not been before.\cite[p.
  351--352]{mobilebrooks} Each layer is separate (and can actually run
on its own processor), and can read data from
and write data into the layer below it (hence the term subsumption,
the higher levels subsume the lower). When a higher level fails or
cannot run, lower levels continue to operate, and basic behavior is
maintained.\cite[p. 355]{mobilebrooks} This approach has tempting biological connections as well:
accurately or not, one can imagine the human being as a robot with a subsumption
architecture, where breathing and heartbeat are lower than balance,
which is lower than voluntary motion, which is lower than logical
thought. And temptingly, it is an approach that can handle
uncertainties, as each layer is built to be robust in the inevitable
event of inaccuracies and lost messages. Theoretically, one could
imagine building systems of great complexity using these approaches,
but this architecture, the radical relativist response to the
structural realism of the physical-symbol system, did not alone make
it past robots of insect intelligence.

Brooks himself abandoned the project of building intelligence from
these humble, subsumptive blocks. Instead, his research on building
the Cog robot shifts focus from ``emergence'' to ``integration,'' and
reversed some of his initial fervor to avoid representation.\cite[p.
  258]{ekbia} He skipped the middle of the evolutionary tree, straight
to humanoid forms, because the evolutionary approach was too slow: 
Brooks reported, it was ``starting to look like, if I was really
lucky, I might be remembered as the guy who built the best artificial
cat,'' a distinction he apparently did not desire.\cite[p.
  65]{brooksflesh}\footnote{As cited in \cite[p. 258]{ekbia}.}
But to do this, bootstrapped knowledge from other sources was
necessary, bringing back some of the old approaches.

%% Deep learning
A number of conceptually different approaches to AI have been tried
over the past 50 years, often with the same sorts of ambivalences as
Brooks's later work.\footnote{Ekbia's book does an excellent job
  exploring and cataloging many of these, more than I have space for.
  But logical and neorobotic approaches appear most relevant as
  broad-strokes historical precursors for inclusion here.} Ideas fade
and resurface as fashions change, and greater computational power
allows ``failed'' techniques to be tried anew. Ours is the age of
statistical AI. What the physical-symbol system and embodied cognition
have in common is their attempt to start from first-principles or zero
knowledge, and to build inexorably toward intelligence. The difficulty
of this road led Brooks to recombine some of the old manner of
knowledge representation into his robotic techniques. The new source
of bootstrapping information is data, ``Big'' and ``small.'' Current
techniques are primarily statistical in nature, leveraging these data
sets by training machine-learning systems; but as we will see
machine-learning systems have properties that make them ill-suited for
safety-critical systems, and autonomous vehicles are designed with a
mix of approaches that allows for more introspection into their
workings. The buzzword of the day seems to be ``deep learning,'' which
appears to yield radical new possibilities everywhere it is
applied---it really just means a return of neural
networks,\footnote{These systems still have little or nothing to do
  with actual neurons; they are brain-like in only the barest
  toy-model sort of way, despite how they are often represented in the
  press} which, armed with some improvements in weight generation, more layers of
nodes, and more data to train with, have been able to eclipse many of
the previous techniques\cite{???}.

FIND AN NEURAL NETWORK PRIMER I CAN REFER TO\footnote{
(perhaps Jordan, Michael I.; Bishop, Christopher M. (2004). ``Neural
Networks''. In Allen B. Tucker. Computer Science Handbook, Second
Edition (Section VII: Intelligent Systems). Boca Raton, FL: Chapman &
Hall/CRC Press LLC)}

Neural networks are essentially just a web of nodes with
interconnections between them. They may consist of multiple layers
(hence ``deep'' in deep learning) or may be shallow. Also known as
``multilayer perceptrons,'' such networks have an input layer and
output layer, with one or more intervening hidden layers. Each node
corresponds to a particular feature or combination of features in the
input, and therefore works to classify inputs into different
categories. Each node is also connected to all the nodes in the
following layer with some tunable weight. The process of training the
neural network involves adjusting those connection weights so as to
be more sensitive in distinguishing different types of
inputs.\footnote{THIS ALL NEEDS SOME CLARITY} This tuning,
traditionally, has been done using supervised method and
backpropagation of errors: the output result is compared to an
expected classification result, and the error used to tune the weights
more appropriately. Deep learning extends this technique, decreasing
the amount of ``feature-engineering'' (a human task, identifying the
features the network should use to distinguish inputs) required to
train the system. The ``ideal'' training situation is entirely
unsupervised: the network independently ``learns'' the features of
unlabeled and unclassified input, without any human input. This ideal
prospers both due to convenience in terms of human effort (less
programmer effort required to build labeled sets of training data) and
due to the deep-seated ideology that machine independence is
paramount. Unsupervised learning is often seen as more impressive and
therefore more valuable research.\cite{???}



%%   $$$$$$$$      NOTE      $$$$$$$$$
%%
%%
%%
%%      More HISTORY, Less CRITIQUE
%%                (here)
%%
%%




%% ``self-aware'' mario, image recognition and other
%% claptrap (1) (incl. John von Neumann & ALife (See CMS790 paper)??)
But as we have seen before, the AI hype-machine is again doing its
work. And popular claims about the utopic promise of deep learning, to
learn about the world ``on its own,''
abound. Google's and Stanford's recent
improvements in image recognition\cite{markoffImage} triggered a wave
of popular speculation about computer vision meeting or surpassing
that of humans. And yet as we will see, the sort of ``understanding''
involved is limited, dealing only with static images and objects
captioned by humans in the training data. The systems would not be
able to answer other questions about the scenes that have to do with
the material properties of the objects, or likely results of various
actions.\cite{gomesJordan} It is important to understand why this technique is not a
panacea in order to understand the real shape and implications
of the driverless car vision. Another recent slew of articles focuses on
the ``self-aware'' Mario created by researchers at the University of
T\"{u}bingen. Any pretense to worry about a ``self-aware AI . . . with
an insatiable desire for material wealth'' that knows ``how to kill''\cite{vincentMario}
is simply journalistic excess\footnote{As the
  researchers well know. This case is just a convenient example of how
such stories spiral out from the lab and acquire new meanings.}, and suggest
significantly more care must be taken in the use of such terms. Mario
is programmed with emotional states like a word-processing program is
programmed with different modes or display parameters. These states
are simply caricatures of emotions, and that plus the program's
natural-language interface makes it appear more eerily sci-fi than it
actually is. But this is the environment that automous cars, as a
research area of artificial intelligence, find themselves in today:
part of a new surge (bubble?) of interest in the field, driven by new
or newly extended techniques. But the ideologies related to artificial
intelligence are not the only ones implicated in self-driving car
research, as these vehicles are not merely programs but mobile robots
involved in gathering, mapping, seeing, ``knowing,'' and keeping-safe.


%% 2.1) data gathering and mapping
%% --this approach presupposes large amounts of data collection (maps),
%% and opens the way to more information about the passengers (2 p)
%% --see Google's patents in particular which talk about sensing things
%% like the number of passengers (4 p)
\subsection{Data Gathering and Monitoring} 

%%   $$$$$$$$      NOTE      $$$$$$$$$
%%
%%
%%
%%      More HISTORY, Less CRITIQUE
%%                (here)
%%
%%
%% ``this assumes xyz'' rather than ``but really abc''


Autonomous vehicles are and will continue to be networked
technologies. This connectedness brings with it great possibilities
for coordinating traffic and improving city planning, as well as great
risks to privacy and security, whether devices are networked with each
other or simply connected to central servers. 

With what networks, and for what reasons, will autonomous vehicles be
connected? Current driverless car concepts depend on networked
information for vehicle guidance. While, historically, certain
guidance systems have been insulated from communications---inertial
guidance systems for intercontinental ballistic
missiles are a particular example of this,\cite{mackenzie}---and
Google's vehicles use inertial navigation devices\cite{knightfurther}
alongside other sources of position data, current navigation systems
depend on global positioning satellites. Google's approach to
driverless car development currently necessitates accurate
and expensive\footnote{An autonomous vehicle researcher at MIT
  quoted prices in the range of $70,000 to $100,000 per device for the
GPS alone, while noting that those would of course come down with
greater production volume.} differential GPS receivers.

But a number of other developments already suggest that autonomous
vehicles will be connected to more than just global positioning
networks. GM's OnStar service already connects equipped vehicles to central servers for
purposes of safety, security, and convenience. The system has the
ability to automatically alert the authorities in case of an accident
or theft. It also provides vehicle diagnostics to the owner's tablet
or smartphone, and the OnStar app also allows the owner to configure settings, lock and
unlock doors, and remotely operate the lights and horn.\cite{onstar}
While these vehicles are not (yet) autonomous, OnStar's present
capabilities are representative of features that will become more common in highly
connected and computerized vehicles, including autonomous ones.

Additionally, vehicles that can receive information from each other and from the
roadway are a top priority for the NHTSA, and have been on the
research agenda for decades.\cite[p. 11]{wetmore} These concepts would
collect and use data to streamline traffic flow and provide
information to reduce delays and accidents. But there is potential to
do far more with the available information,
and data collected by the vehicle to make possible its own functioning
could be made to serve other purposes. Information about the vehicle
and its surroundings, including the locations of cars and pedestrians,
precise GPS coordinates of the vehicle itself, and the vehicle's speed
and acceleration, not only represent important knowledge for
path-finding by the vehicle itself, but new sources of potential
revenue for the groups in position to collect them. Uber, which
through its GPS-enabled ride-hiring applications still collects only a
fraction of the data that would be available through a self-driving
vehicle, has agreed to share its ride data with municipalities for purposes of
city planning.\cite{uberJardin} Though this data is ostesibly being
shared for the public good, it also serves private ends.

Google has envisioned vehicles that can determine their number of
occupants, and use facial-recognition or other biometric systems to
identify them. According to one patent,\cite{predictPatent} these vehicles could prevent
unauthorized persons from putting a child in a car, prevent convicted
sex offenders from operating their vehicles within the
legally-required distances of schools and playgrounds, or prevent a
car's doors from being opened (even from the inside) by a child unless
an authorized adult is present. These are only visions, and patents
are notorious for trying to cover as many possible angles of a
technology even if they are not intended to be applied in practice.
But these suggestions represent a perspective on safety and societal
order that posits technological surveillance and enforcement as an
appropriate preventative measure against criminal behavior. Whether or not protecting
against these threats is an appropriate use of this information is a
matter for societal judgment, but such proposals, if enacted, would
require these vehicles to have unprecedented levels of very sensitive
knowledge about people and their lives: biometrics, criminal
histories, family and trust networks. 

Data ideology tells us that we can understand ourselves better through
these data, that we can use them to determine patterns we never knew were
there. Out of this ideology come publications like Uber's blog, which
describes customer ``insights'' gained through their ride data, which
are ostensibly interesting to the public. ``Look here,'' they seem to
say, ``we can tell you about \emph{you}.'' But it important to
recognize that the ideologies of data collection---for corporate
profit and public use---are deeply intertwined, and strongly
influenced by the possibility of using machine learning techniques and
statistical analyses to find and exploit patterns.


\subsection{Maps and Mapping}

Other types of data collection are also implicated in the current
vision of the driverless car project. In order to drive with us,
autonomous systems will have to understand,
for at least a practical sense of “understanding,” traffic rules and
their accompanying signs, signals, lanes, and customs. This is, at its
core, a highly complex problem of interpretation and representation
for machines. Human understanding is built
through years of experience: it is through existing as a human being
in a particular cultural context that we know to drive on roads but
not on sidewalks, and how to tell the difference. But we do not have
this luxury in training machines.

Here, the connections of autonomous systems to information networks
again become important. The vehicles in the
DARPA Grand Challenge did not navigate “on their own”: they used GPS
to follow a path laid out for them in advance, using their autonomy
only to avoid obstacles like rocks and ruts.\footnote{John Leonard,
  discussion with the author, December 3, 2014} And though successful
road tests have been accomplished without navigational assistance,
using only visual stimuli (such as the EUREKA PROMETHEUS project
in the 1990s)\cite{???}, modern systems are tending to use more external
stimuli, rather than less, in an attempt to increase safety. Even as
advanced as it is, Google's autonomous vehicle technology requires
hyper-detailed 3D maps in order to operate properly on public
roadways.\cite{???} These maps are generated by vehicles outfitted with
special sensor arrays, like the LIDAR Google uses for their autonomous
vehicles, which drive a route and collect data which can be used to
reconstruct the model.\cite{???}

Pre-made maps are used so the vehicle knows where stoplights, signs,
and curbs are, reducing the computational load on the machine in the
crowded visual landscape of driving, and allowing it to focus on
elements of the environment that are changing rather than those likely
to be static.\cite{???} Mapping claims a unique capability to represent the real, objectively
and diagrammatically, but also requires that world to remain largely
static, at least on the order of how long it takes to update the map
for a particular region. The necessary level of continual mapping is a massive task if the
vehicles must be usable everywhere. The United States alone contains almost
8.5 million road miles\footnote{Data as of 2008,
  http://blog.cubitplanning.com/2010/02/road-miles-by-state/\cite{???}},
and it took years for Google Streetview to acquire the level of
coverage it currently has. The utopian discourse of driverless cars
implicitly suggests that such vehicles will be available everywhere,
and are the solution to nationwide transit problems. But widespread
egalitarian access to maps-based devices depends upon a rapid,
widespread mapping initiative.

The seemingly universalizing forces of maps and computer programming
have a tendency to help hide issues of geographical and cultural
specificity, which are rendered invisible in the utopian narrative.
But though engineering practice holds that these issues are
conquerable, they should be anything but invisible. Programmed devices
must must know about speed limits, about traffic
lights, about rules of the road that were never designed for
autonomous systems. These devices must respond to human caprices and
be adapted to longstanding, ingrained laws and habits. They must
include historical knowledge, rooted in the legal and social histories
of roadways, which may differ between cities and states, and certainly
between countries across the world. Local customs and behaviors differ, and even if maps are
available, the same vehicle programming may not work for Los Angeles,
Boston, and the rural Midwest, let alone Singapore, Mumbai or Cairo.
The map, for all of its objective standardization, still represents
real places subject to cultural histories and vulnerable to
socio-economic dynamics. These social and regional issues are often
ignored in the driverless vehicle narrative, but nevertheless stand to
be critical to the manner in which these technologies could come to
enter everyday life.



\subsection{Machine Vision}
%% 2.2) machine vision
%% --pull from the Spectator paper (4 p)
%% INCL. machine learning
Interpretation of the world around us is a task that seems
particularly easy for human beings, but particularly difficult for
machines. The invention of the photocell, early a tool for workplace
monitoring and surveillance, provided a simple channel through which
electrical systems could respond to the amount of light reaching
them.\cite{???} Though the photocell can easily provide a computer system
with access to brightness information over time, perceiving detail and
depth, identifying shapes, and interpreting expression and motion are
all capabilities of human vision that require more sophisticated
technologies to reproduce. DARPA's 1983 Strategic Computing Initiative
included image interpretation as one of its main
focus areas.\cite{???} But it is only relatively recently that real-time video
processing, needed for camera-based navigation, became feasible for computer
systems small enough to fit in a standard automobile.\cite{???} And machine
vision problems, including object recognition and scene
interpretation, continue to be difficult, even with increased
processing power and new algorithms. 

As an engineering discipline, computer vision takes a decidedly
practical and reductionist view of what it means to see. The goal is
generally not to achieve creative interpretation or aesthetic
valuation, but to differentiate free space from things a robot should
not run into.\cite{???} But this so-called objective focus still encodes
certain subjective judgments about objects (including people),
behaviors, and intent. And while computer vision is having success
with object detection, there is a wide variety of human knowledge
about objects and scenes that is missing in current computer models.\cite{???}

Though vision has not always been the sensory mode that dominated
autonomous vehicle research, vision is a particularly attractive sense to
use, as it is integral to how humans drive. In an attempt to build
autonomous vehicles that can operate without infrastructural changes,
research has moved away from tracks and cables toward vision-guided
systems. New approaches were pioneered by Ernst Dickmanns at
University Bundswerhr in Munich, with the vision-guided VaMoRs van,
and continued via the EUREKA PROMETHEUS project in 1987, in which
Dickmanns and Daimler-Benz built cars guided by analog video
cameras.\cite{???} Like the earlier VITA project by Daimler that used an
analog video-camera signal processed through a framegrabber, these
cars digitized analog video at relatively low resolutions. The
features the systems searched for, including lane markings and other
cars, are geometrically distinct and visible even in small images.\cite{???}

Vision-guided systems, now using digital video cameras and
off-the-shelf consumer hardware, have the benefit of being inexpensive
and insensitive to interference from other nearly devices (unlike
sonar, for example, which becomes problematic in crowded
situations\footnote{John Leonard, discussion with the author, December
3, 2014.}). Some commercial systems, such as that developed for Mercedes-Benz's
self-driving S-class, which is slowly finding its way into consumer
vehicles, are primarily guided based on such visual sensors.\cite{???} To
these sensors, recent research has added roof-mounted LIDAR arrays.
LIDAR, short for Light Detection and Ranging, is a distance
sensor, which is applied in vehicles to scan the environment with a
rotating array of laser beams to create a detailed 360-degree
representation of objects and their distances. This technology
solves some of the difficulties of image interpretation by default, as
it can provide highly-sensitive information about free space and
obstacles. Shape-detection algorithms can then be used (in addition to
vision-based data) to classify obstacles as different types of
objects: pedestrians, bicyclists, cars, and trucks.\cite{???}

Pedestrian detection algorithms search for person-like shapes, where
“person-like” is determined by, for example, training a classifier
using thousands of images previously labeled (by people) as being images of humans, so
the system can learn the features that correlate with a person being
in a particular region of an image. These detected categories allow the system to make
statistical predictions about likely types of behavior: according to
one of Google's patent applications, bicyclists are likely to be more
erratic than trucks, and should be treated
accordingly.\cite{predictPatent} These sorts of predictions are
something that human drivers do consistently, and are therefore also
likely important to how autonomous vehicles may drive.\footnote{The
  DARPA Urban Challenge crash, the first crash between two autonomous
  cars, provides an important lesson on the vagaries of object
  detection: the classification threshold between moving and
  stationary, set too high, allowed one vehicle to interpret the other
  as stationary, leaving no room for unexpected behavior.\cite{???}} 

And because autonomous cars see---putatively “as we see”---their sight can
be leveraged as visual evidence of their operation. Computer vision systems that identify
pedestrians can be shown to do so, via the detection boxes that act as
diagnostic tools for researchers and direct representations of
internal system information. The new technologies of vehicle automation thereby produce through
their operation new forms of evidence, which can be presented through
electronic information media.


%% 2.2) functionalism over understanding
%% --for better or worse these devices are not humanlike in their
%% understanding
%% ----AI winters & the rise of commercial ML AI with much smaller goals
%% (see Wired articles, etc) (3 p)
%% --the ideology that drives them is an engineering mindset: ``just make
%% it work'' (2 p)
%% --pulling the quote from my USC talk: also interesting to note
%% regarding 1.2 that it seems to expect a humanoid robotic chauffeur in
%% a normal car! (1 p)
%% -and of course interviews with people (Ryan Chin; Walker Smith etc.
%% where they matter to this type of vehicle)
\subsection{Functionalism and Utilitarianism} 
But for better or worse, self-driving cars will not be human-like in
understanding, even while they can detect and identify pedestrians.
A wide range of software companies and startups have entered the AI
industry, but these companies are not primarily focused on
general-purpose AI. Though certain ventures still hold out the dream
of doing so, several AI winters have shown that the creation of
general intelligence is very difficult, and is by no means around the
corner. Those working in the field are well aware of
this.\cite{???-articlerebuttingmusk} So much current work is
fundamentally utilitarian, building systems with clear goals, metrics
for success, and market segments.

The utilitarian model of AI makes good sense for a number of reasons.
First, much can be achieved with current technologies. Rather than
focusing on a long-term project, which would carry much greater risks
and rewards which are further off, achieving short term goals is an
attractive prospect. It can not only attract money but can be
profitable sooner. Though short term commercial viability is not
necessarily applicable for this vision of the self-driving vehicle, it
is possible to have working prototypes on the road, generating
interest and publicity even in controlled conditions. Taking
journalists for test drives drums up interest, even if the technology
is not ready for full-scale deployment. Make no mistake: the current
vehicles are capable of recognizing pedestrians, but any claim that
they ``know'' or ``understand'' is a tenuous misuse of words which
could no longer credibly mean what they generally do. 

Second, humanlike characteristics may not be helpful for building
specific applications. One would likely not want one's self-driving car
to be preoccupied or emotional.\cite{???-wiredFutureofAI} It may be that, for utilitarian
purposes like driving, many of the characteristics of the human are
detrimental, their elimination helpful and intentional. Much of the
discussion around why autonomous vehicles are necessary centers on
just such qualities: distractability, sleepiness, lapses in
concentration.\cite{???} We would not wish to emulate such
characteristics in robotic systems. However, these are human
capabilities, but also only caricatures of the human. People possess a
variety of other capabilities which might be helpful to certain AI
systems. As AI researcher Doug Lenat wrote in 1997:\blockquote{Before
  we let robotic chauffeurs drive around our streets, I'd want the
  automated driver to have a general common sense about the value of a
cat versus a child versus a car bumper, about children chasing balls
into the streets, about young dogs being more likely to dart in front
of cars than old dogs (which, in turn, are more likely to bolt than
elm trees are), about death being a very undesirable thing.\cite[p.
  122]{ekbia}} 

Current approaches, however, assume this kind of deep understanding is
unnecessary, both for the technical creation of such vehicles as well
as their public acceptance. It is an interesting read on the changing
times to notice that Lenat's statement seems to suggest humanoid
robotic drivers operating regular cars. As well as moving toward
functionalist systems, the industry has moved toward embedded systems
within devices, systems that make no pretenses to be humanoid, but
instead revel in appliance-hood. Whether this shift in the form of AI
systems makes customers more or less nervous about computer-driven
vehicles is an empirical question\footnote{CAN I FIND RESEARCH ON
  THIS}. But it certainly suggests a desire to make the systems more
invisible---and actually has important implications for human-computer
interaction, as we will see in the next section. Though it may seem
obvious, it is vital to remember that the devices we discuss here may
have the properties Lenat mentions only in the way that Mario is
``self-aware,'' possessing programmed constraints that cause it to
avoid people, and perhaps expect children to behave erratically. 


%% 2.3) what standard of safety? 
%% --better than an average person? or better than the best people? (1 p)
%% --human problems with using projected statistics to define policy:
%% does it make people feel better to know a system is statistically
%% safer when they are uncomfortable with it? see cars/planes for example
%% (3 p)
%% -->perspective of caution? or  ``losing lives every day'' that
%% could be saved?
%% ----how much risk is legitimate?? (2 p)
%% --airlines as a place where we see the different dimensions of
%% statistical safety vs. passenger's feelings of safety (3 p)
%% ---drawing from PARC/CAST documents
\subsection{Safety and Statistical Risk}

But regardless of understanding, what drives the self-driving car
narrative is safety; specifically, the poor safety record of human
drivers and the potential for machines to do much better, free from
human frailties of distraction and fatigue. How bad are human
drivers, really? Conventional stories of human drivers paint us as
plainly terrible, prone to road rage and drowsyness and generally
doing anything except paying attention\footnote{Numerous popular
  articles take this position\cite{???}}. However, the statistics tell
a slightly different story. Official numbers show that the number of
motor vehicle accidents has remained in the range of 10--11 million
per year for most of the late 2000s. The death rate seems to have
decreased overall in this time, settling somewhere below 1.5 deaths
per 100 million vehicle miles, presumably due to a combination of
better safety features (especially since 1990) and other
factors.\cite{???} The total number of vehicle-related deaths is much
lower, at a still significant 35,000 deaths per year. But this alone
does not tell the story. At around 1.5 deaths per 100 million vehicle
miles, or about 1 death per 67 million miles, humans seem relatively
competent in a statistical sense. The average American, who might
drive 1 million miles\cite{???} is unlikely to be
involved in a fatal crash in his or her lifetime. Looking at non-fatal
accidents as well, humans get involved in about one accident per
286,000 vehicle miles. Part of why the autonomous vehicle problem is
such a difficult one is that these numbers are relatively high.

Theoretically, computers can do better. But especially careful human
drivers can also clearly beat the human average. But how safe do
autonomous vehicles need to be in order to be allowed on our roads?
Safer than the average human? or safer than the very best drivers?
Such questions have real impact when it comes to how devices are
designed and when they become commercially viable. The autonomous
vehicle enterprise seems to call for using such projected statistics
to define policy. One common narrative is that undue caution in the
rollout of autonomous vehicles will directly ``cost'' lives, since
people are killed by human drivers every day.\cite{???-blog} However,
people are also accustomed to the current automobile death rate, and
any autonomous vehicle crashes are likely to attract deep
scrutiny\footnote{As Jim Womack has pointed out, there is no good side
to change as a regulator. Regulators are not congratulated when things
go right, only criticized when things go wrong. So some measure of
tentativeness is almost certainly justified to the regulatory mind.
(Discussion with the author, December 3, 2014)} as
to whether a human could have prevented the accident.\cite{???}

These are questions of policy, but also questions of human acceptance.
This ideology posits lowering accident rates above all else, leaving
no space for human squeamishness about technology and responsibility. 
The statistical argument suggests that the death rate is all that
matters, but humans are notoriously bad at understanding and
responding to statistics. Does it make us feel better---more
comfortable, more likely to get into autonomous taxis and spend our
money on autonomous cars---to know that they are statistically safer
than the average driver? 

Airlines seem to be a place where these different dimensions of
safety---statistical safety and feelings of safety---already have
become visible. Despite the comparative statistical safety of flying,
people tend to be more afraid of flying than getting in their
cars.\cite{???} Well this may have to do with a number of factors,
including that aircraft do not remain on the ground during operation,
it also represents a situation in which passengers give over their
agency to pilots performing a job they do not understand and could not
take over in an emergency.\cite{???-canIFindAnythingOnThis} SIMILAR
ISSUES WITH TRAINS?\cite{??} The
PARC/CAST working group had to account for these issues in their recent
recommendations about airplane safety. They judged that if air traffic
were to continue to increase with XXXX rates of crashes, by XXXX it
would reach one fatal crash per day, which would be intolerable to
airline customers.\cite{???-PARCCAST} So actual safety rates are only
one component of interest when considering how consumers react to
modes of transportation. Perceived safety may be very different.


%% 3) (or worked in to each of the previous) current research
%%--not historical, but actual last-20-years papers about this
%%--examples of where ideologies bleed through!!!!
