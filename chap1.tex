\chapter{Narratives and Counternarratives}
\label{chap:1}

%% was sources \& ideologies
%% 2. Where is that coming from?
%% -Sci-fi and historical dreama
%% -a cultural vision of automation of work
%% -AI history
%% -NHTSA/SAE policy/standards (instantiated in)
%% -researchers in the field!

%Reconsider numbers in subsection headings


%%1) science fiction; feels disconnected from the meat of the chapter
%%could be introductory to whole thesis
%%or may need more depth

%%2) designed dreams needs more work; 
%%signal connection: designers thinking about science fiction
%%possibly remove designed dreams from title

%%3) stronger punch/framing MY argument in the historical overview
%% around p30 ``the narrative of the automation of work''. . .

%%4) more to define what is at stake in engineering standards/policy
%%docs; why I quibble with their definitions
%% I may be able to do this drawing on the other subsections
%%as a CONCLUSION, wrapping up the other three things
%%and what is LOST/OVERLOOKED; the ability to deal with nuance and
%%REAL ACTIVITY
%%other part of an analysis of this kind of stuff: did it for a
%%reason; what is the work that it does (reactionary thing in some
%%cases; pushes for a particular image of what this can be)
%%--visions of human; SAE history of thinking as engineers; people as
%%predictable black-boxed inputs and outputs
%%--NHTSA as reactionary, as trying to cope with what Industry says;
%%and as history of dealing with people and responsibility in
%%particularly binary ways; REAL PRESSURE to get it OUT THERE to guide
%%development
%%maybe split into different sub-headers with context like NHTSA:
%%Agency Trying to Catch Up or w/e

%%5) not a good conclusion to the chapter so far; should have a sense
%%we have gotten somewhere and learned something

%%=====================================


The popular narratives about self-driving vehicles have not appeared
out of nowhere. But what are their roots? What are the sources from which
these stories come, the founts of technological ideology from which
they spring? Journalistic accounts of technological change are of
course impacted by a wide variety of practices and perspectives, from
the market and readership needs of news organizations to the pervasive
culture of commodity scientism \cite{smithSelling}. The
reasons for why these stories take a particular form may be innumerable
and difficult to pin down in general. But specifically regarding the ways that
driverless cars are figured in the press as autonomous
machines---that the only image of the technology that bears
investigation is that of fully self-driving vehicles, an imagined
technological peak---: science fiction, factory automation history,
artificial intelligence, and current engineering and policy documents. Some of
these sources themselves have more direct impacts, such as
policy documents that stand to shape the way state legislation proceeds. Each,
in its turn, impacts the popular narrative in specific and important
ways, which must be understood if we are to come to grips with the
question of why we see a particular dominant narrative:  why vehicle
automation appears, at least on the surface, in the way that it does.


But some has to do, instead, with other historical images
that are part and parcel of
popular autonomous vehicle narratives, ones that are not connected
with vehicles at all. And among these is the assimilated cultural
understanding of the history of factory labor and the automation of
work.


\section{The Automation of Work and Its Untold History}
%% 1.1) deep history is the mechanization/automation of work (4 p)
%% --going back to factories/IRev: replacing human competencies with
%% machine competencies and moving the people into new roles
%% --which engages old debates about the role of the human

Human technological progress since antiquity has
involved continual re-negotiations of human labor and the roles of
animals and mechanisms in the labor process. Due to a confluence
of factors---the miniaturization of computing technology, new
advances in machine learning and artificial intelligence algorithms, a
gradual increase in battery capacities, faster wireless networks---the
horizons for everyday automation are broader now than ever before. 

But though the focus is on the future, our past is deeply involved
in its presentation. News articles fret
about what will happen when no one knows how to drive manually any
more \cite{pross}, a classic fear of ``de-skilling'' that is implicated in so many
other implementations of computers. Furthermore, coexistence with
autonomous or automated systems is sometimes presented as a fundamentally new
situation, as if human beings had never before had to work and live
with and next to automated systems, presenting new benefits and
dangers, and requiring new roles for their human tenders. However,
automation already has a deep 
history in the industrial sector. Current debates and fears about de-skilling, human jobs,
and the role and value of human labor return us to questions that have
plagued factories, and labor's relationship to machinery, since the
early Industrial Revolution. But as we have already seen with the common
elision of the actual research history of self-driving vehicles, the
stories that get told about driverless cars and factory automation are
primarily those rooted in the cultural consciousness, rather than in
nuanced, factual histories.

One of the commonly referenced histories of automation is that of the
textile mills of the early 1800s, in which a large
proportion of hand-labor was replaced by steam- or
water-powered industrial machinery, and which has entered our collective
consciousness. The
story here is a familiar one: skilled artisans made obsolete by the
lower cost and higher productive capacity of mechanical labor \cite{pewPositive}. By
analogy, new types of skilled labor (taxi and limousine drivers, or
even bus drivers) are now under threat, and their fate should be no
different than that of the workers who came before.

The automation of factory work is an
important touchstone for narratives about the automation of driving
and the creation of autonomous vehicles. But just what the
processes of standardization, mechanization, and automation (or
``automatization'' as the cyberneticists referred to it) have done to
the factory, and to laborers in it, is 
not clearly understood among many who write about autonomous cars. Within this
forgotten history---which is substituted for by an imagined person-less
factory that does not exist in the real world---there are even lessons to be
learned by the research community. This past is relevant, perhaps
more than ever, to the future of transportation.

A search for the beginnings of industrial automation takes us to the middle of
the 18th century: Vaucanson's mechanical loom dates to 1741, and formed the basis of
 later developments in weaving by Joseph Marie Jacquard \cite[p. 9]{dieboldImpact}. 
But the first example of ``complete'' industrial automation originating in the
United States does not come until Oliver Evans's work in the 1780s on
automated grist mills \cite[p. 5]{roesmithYankee}. Through a series of elevators and descenders,
horizontal screws, spreaders and rakes, his mill moved grain from raw
agricultural commodity to finished product: sifted flour. And ideally,
all parts of the process would occur without human intervention. 

In reality, the process both increased efficiency and decreased the costs
of production, so much so that the same basic machinery is still in
use today in some smaller milling operations \cite{wyegrist}. None of his individual
inventions---which he lists as the elevator, ``conveyer,'' hopper-boy,
drill, and descender in his 1795 miller's guide---was a particularly
groundbreaking achievement, but what Evans did was place these devices
in succession so as to allow continuous production, and the
elimination of many slow human jobs that degraded the quality of the
product by tracking dirt and contaminants around inside the
mill \cite[p. 203]{evansMillguide}. 

It took some time for the high level of automation found in the Evans
mill to spread across other industries, and the mill may find its
closest cousins in the ``automatic'' factories of the 1950s and 1960s
and the roboticized factories of today, but Evans's contemporaries
were not uninterested in increasing efficiency and output. Paul
Revere, one of America's early industrialists, applied shifts in
manufacturing techniques to transition himself from an
artisan worker to manager and overseer of others over his long
metalworking career \cite[p. 187]{martello}. Like a small number of
his postrevolutionary 
contemporaries, he improved his circumstances by becoming a manager
and owner rather than a laborer; but manufacturing itself was a site of public
debate, pitted against the ``inherent virtue'' of agricultural pursuits.
Tench Coxe, a political economist, wrote in 1810 that ``new machines
and power sources allowed even greater productivity with less labor,
further underscoring the connection between technology and republican
virtue'' \cite[p. 217]{martello}. To Coxe's romantic view, these machines  worked ``as if they
were animated beings, endowed with all the talents of their inventors,
laboring with organs that never tire, and subject to no expense of
food, or bed, or raiment, or dwelling'' \cite[p. xxv]{coxe}. Though we
may have lost this romanticism, we haven't lost this perceived
animism. Automated machines, like self-driving cars, continue to
excite, impress, and cause fear due to this transformative, if
mechanistic, aliveness.

But these romantic words did not represent the whole reality of
industrial machine labor. Human labor of
maintenance and supervision is implicit in these manufacturing
machines---even the Evans Mill\footnote{Evans cited a reduction in
  labor and expense of ``fully one-half,'' which is not the same as
  its complete elimination. Behind his concession to manual switching
  of machines lies a great volume of labor that is excluded
  from the traditional narrative: switching machines on and off;
  tending and configuring the machines during their operation; examining machines for
  wear, degradation or failure; fixing the machines when they break
  down \cite{evansMillguide}.}---but it is rendered invisible by the
rhetoric that the 
machines themselves require no bed or board. At the same time, Coxe's
use of the word ``endowed'' should focus our attention on exactly which
``talents'' of the inventors have been automatized, and the human
labor necessarily involved in that conferring of capabilities. 

Robotic cars are sometimes situated as
the next step for robots after the factory, their final emergence into
the real world having conquered the factory floor.\footnote{See for
  example ``Robot Vehicles'' in \emph{RobotWorx} \cite{robotworx}, which describes automated cars as having the sensors
  industrial robots have had for many years.} The history of
automation that gets mobilized is one of a teleological progression
toward complete automation of all sectors of work, which does not do
justice to the historical details involved. The tendency to gloss
automation in this manner seems
to be deeply aligned with the struggle of certain groups of
middle-class laborers to keep their jobs in the face of changing
factory conditions. Among the most public and most topical of these
are the Detroit autoworkers, once a bastion of middle-class
respectability, many of whom faced unemployment with the rise of
Japanese (popularly read as ``highly automated'') automotive
might.\footnote{It is worth mentioning here that the ``lean
  production'' techniques largely responsible for industrial
  efficiency of Japanese automotive companies are not primarily about
  automation. Though automation is involved, the essential core of
  \emph{kaizen} manufacturing is increased trust between management
  and labor, assisted by continual communications and high job
  security \cite[p. 198--199]{nyeAmericas}. These na\"{\i}vely seem
  directly opposed to automation.} There is some truth to this, though
the actual story 
is far more complicated and has less to do with automation and more to
do with product priorities and other labor
relations \cite[p. 188--200]{nyeAmericas}. The actual historical
circumstances are, in this case, less important than the general
perception that automated 
systems, such as the robotic arms used for painting and assembly, are
reducing the overall pool of available jobs.\footnote{As an example of
  this contrary history, a report by KPMG produced for the Society of Motor
Manufacturers and Traders suggests that automated vehicles could be
expected to create 320,000 new jobs in the United Kingdom by 2030, only 25,000 of
which would be in automotive manufacturing directly
\cite{toveyCreate}.
That automated vehicles can be seen to create jobs, rather than
eliminate them, has everything to do with the labor involved in their
development, production, testing, and use. This is not only
factory labor.}

%%TODO (DONE): Article about expected impacts in UK, includes lots of jobs and
%%predictions by 2030:
%%
%footnote it

Modern automotive manufacturing is not alone in presenting conflicting ideas
of what automation can do. Arsenal practice was the site of multiple
revolutions in U.S. 
manufacturing technology through the 1800s and early 1900s, notably
the development of the so-called ``American system'' of interchangeable
parts, through which a gradual increase in mechanization would seem to
continue, driven by the tight tolerances necessary for this production
method. And yet armorers and managers at Harper's Ferry resisted the
mechanization of their craft,\footnote{Blanchard's many automatic
  machines for making gunstocks were of particular
  importance \cite[p. 56]{roesmithHarpers} but a wide variety of machinery
  was implemented in the gun-making process at Harper's Ferry. Most of
 these machines still required significant human labor. Nash's
 barrel-turning machine in part mechanized the production of barrels
 of standardized dimensions: it consisted of a lathe on a wooden
 frame, with human-operated props to hold the barrel in
 place \cite[p. 119]{roesmithHarpers}. The worker also had to continually
 measure the barrel with a caliper, and adjust the device's chisel
 appropriately \cite[p. 121]{roesmithHarpers}. This gradual implementation of
 mechanized labor was continued in further machines produced by Hall.
 His straight-cutting machine, an early version of a milling machine,
 had as its distinctive feature the ability to be tended by ``common
 hands'' without a loss of
accuracy \cite[p. 239]{roesmithHarpers}.}, and while certain competencies were
transferred from the skilled worker
to the technical apparatus, human oversight and operation was still
integral to the production of weaponry using the new technology. It was
not clear until after the fact that more mechanization was necessarily
better: Harper's Ferry remained ``competitive'' with
costs at the more highly automated Springfield Armory through the mid
1830s \cite[p. 324]{roesmithHarpers}. 

The same exchange of competencies characterized Ford's assembly line
production as well, which is cited by David Hounshell as the rise of
true mass production in America \cite[p. 217]{hounshell}. Ford's factory developed fixtures
and gauges, designed to allow for use by unskilled machine tenders. As Donald
Norman writes in \emph{Things That Make Us Smart}---and as Ed
Hutchins describes in his cognitive anthropology practice,
particularly his work
on distributed cognition
systems \cite{hutchinsCockpit}---``the world remembers things for us, just by being
there'' \cite[p. 147]{normanThings}. But while the gauge simplifies the
assurance  of quality, it does not
automate it:  it simply changes the effort from a more complex judgment
of quality and measurement to a simpler one. While he instituted the
five-dollar day to attempt to solve labor problems at the factory, and to
compensate laborers for becoming part of the ``production machine,''
Henry Ford also attracted a wide variety of well-educated skilled mechanics
to his automobile plants \cite[p. 223]{hounshell}. Like Evans, Blanchard, Hall, and others
before them, these mechanics applied their skills to design machines,
and simplify and standardize work processes. The individual judgment
of the assembly line laborer was displaced into standardized tools and
fixtures, built into these technologies by the labor of skilled
machinists and designers. This displacement of competences is still in
evidence in automation today, but is an often-neglected part of the
discussion of self-driving cars. The prevailing taxononies of
automation\footnote{By the NHTSA and SAE, discussed in more detail at
  the end of this chapter.} discount the displaced labors of route
mapping, programming, and system supervision. The driver, by these
standards, is only the person sitting in the seat. Those orchestrating
the course of a vehicle from outside---days or months before, or
observing in real-time from a server room---do not count where levels of
automation are concerned. The new labor generated by automation has
been made structurally invisible, as it has been in the cultural
narrative of factory automation.

Also around the turn of the century, Taylorism in factories created ``new managerial
functions'' performed by ``new classes of people with new titles and
more clearly specified responsibilities'' \cite[p. 120]{aitken}. A focus on the
people---who are they? where are they? what are they doing?---shows that
one of the fundamental and enduring characteristics of Taylor's
system, the expansion of management roles and the further division of
labor, is not about mechanical automation but about new and altered
types of human work. Industrial processes in the early 1900s continued
the removal of strategic decision making from the
workers most physically engaged in product production, installing it
instead within formal organizational structures and the managers that
constituted them. 

This pattern of delayed recognition and contingent change repeats for
numerical control in assembly line production. Numerical control (NC),
developed in the 1940s and 1950s as an outgrowth of World War II
research into feedback systems, slowly began to produce industrial
robots that could perform factory tasks without direct human
intervention. Robots began to replace assembly line jobs such
as spray painting and welding, but adoption was gradual, with only
about 6,000 robots in use in American factories by the mid
1970s \cite[p. 159]{nyeAmericas}. Industrial robots, while automating tasks,
had a way of generating large contingents of skilled human laborers
who still needed to be paid for their services. Industrial robots were
complicated, and needed a variety of skilled workers to tend them, and
to repair them when the broke down: these early experiments did not
increase profits because of the volume of highly skilled labor needed
to keep the machines operating \cite[p. 162]{nyeAmericas}. The development of NC machines
proceeded with a specific interest in eliminating skilled workers, but
the jobs that disappeared were largely unskilled or semi-skilled
laborers \cite[p. 164]{nyeAmericas}. And while Norbert Weiner, in his 1950 book \emph{The Human Use of
Human Beings}, prophesied the end of ``deadly uninteresting'' jobs, which
would be mechanized within 20 years, such changes have still not
totally come to pass \cite[p. 161]{nyeAmericas}. To compound the problem, new industries of
skilled workers---record-and-playback machine designers, and NC machine
programmers---sprang up to furnish factories with their tools. This
historical thread should focus our attention on what is added, rather
than removed, by
autonomous vehicles: more complex computer systems may make the driving
task simpler for a given level of safety, but make the system
engineering task, and the tasks of maintenance and repair, more and more complicated.

Norbert Wiener, the famous cyberneticist, was asked to contribute to an
anniversary supplement on automatic machinery for the \emph{St. Louis
Post-Dispatch} in 1953. The piece that he wrote, ``The Machine as Threat
and Promise'' \cite{wienerMachineThreat} hailed the coming of the
robotic factory with his usual 
caution and good sense, but confirmed that ``automatic machinery of a
new sort is assuredly here to stay,'' machines that are ``coupled to the
outer world through the mechanical equivalents of sense organs.'' The
article's photographs by Edward J. Burkhardt, displayed above the
sub-heading, ``The Only Automation Plant in the World,'' show the
machinery of the Rockford Ordinance Plant in Rockford, Illinois, and
the men who tend them. The tenders, largely stoic and inactive, stand
and observe, sit quietly at machines, or push buttons on control
panels, surrounded by the motion of the ``largely automatic process of
turning steel bars into 155 millimetre
shells'' \cite{wienerMachineThreat}. Around the same time, in Ford
Motor Company's Brooks Park engine plants, near Cleveland, forty-two
automatic machines ``linked together by transfer devices that
automatically move the blocks through the complete process, perform
530 precision cutting and drilling operations'' \cite[p.
  9]{dieboldImpact}. Through the 1,545 feet of assembly line, no human
touched the parts. Such is the vision of the automated factory. In
truth an operator stood by each machine, ensuring its continued
operation. But this human labor, menial as it may be, is not often
recognized. One worker described his experience: ``I don't do nothing but
press those two buttons . . . Sometimes I use my thumbs, sometimes I
use my wrists and sometimes I lay my whole arm across'' \cite[p.
  10]{dieboldImpact}. And yet,
despite the meniality of his labor, it is still integral to the
process of production: without it, the line would grind to a halt. The worker
no longer makes choices about how to bore a part, or which tool to
wield. But these choices have been built into the industrial equipment
he oversees. Instead, he monitors the state of the line and chooses to
turn the machine on or off.

But as control is further constituted within management, the roles of
management are rendered more and more menial themselves. The Air
Force's Integrated Computer-Aided Manufacturing program (ICAM)
brings to light further complexities in the story of the automated
factory: ICAM attempted to aid shop floor automation by automating certain
management functions, ``to try to reduce the enormous indirect costs
that have resulted from the effort to reduce labor costs and remove
power and judgment from the shop floor,'' costs that have continued to
dog new rationalization strategies \cite[p. 330]{nobleForces}. ICAM, like the mythical
ouroboros, sought to offer automation as the ``solution to the problems
generated by automation,'' providing automated scheduling functions,
inventory control, and design tools to ``provide better management
control'' and ``free management from excessive routine duties to do
creative work''---the creative work that the management had attempted to
place in their own hands, in the first place, through earlier
processes of rationalization \cite[p. 330]{nobleForces}. Automation, Noble's ICAM example shows,
can be used both to routinize work---for the manual laborer---and to
eliminate the routine in favor of the creative---for managers and newly
generated classes of creative workers. This is analogous to the
current process of self-driving vehicle design: automated systems
substitute the mechanical process of controlling the vehicle inputs
with the new task of the intellectual supervision of the driving
system. This new task is not necessarily simpler or easier, though it
may be intended to be, but can be seen as representing a management,
rather than labor, role in vehicle operations. The key issue this
should bring to light is
precisely how menial the human's role in self-driving car operation
will be, which is currently an open engineering question.

%%LOSING Thread of argument

The narrative of automation progress was once the province of
blue-collar labor only, but is now moving into white-collar work as
well, which partly mobilizes the popular concern about next-generation
automation technologies that perform more difficult and complex
informational tasks. Self-driving vehicles make sense today because of
a general climate that believes in the possibility of automating
knowledge work, and their development feeds back into the perception
that other complex tasks will soon yield themselves to automation. Bill
Gates, speaking at the American Enterprise Institute,
suggested that a large portion of the workforce will find itself
displaced by robots in the next 20 years, including accountants and
other white-collar
jobs \cite{gatesRobots}.
Gate's suggested remedies (low-to-no taxes and decreasing minimum
wages) are painfully biased toward corporate profits, but his comments play directly into
contemporary fears about automation, and support only the
narrative that job-loss due to automation is inevitable, and workers
(and by extension, professional drivers)
should just get out of the way. To this view, the last two hundred
years of innovation in automation is unidirectional and largely
undifferentiated:  from steam power to the assembly line, from
Taylorism to roboticization, each was yet another nail in the coffin
of the human worker---to be continued in the near future with the
elimination of drivers and the labor of navigating a vehicle. But
while it is undeniable that automation has changed the
character of human labor, this perceived uniformity in automation
processes is a figment of the collective imagination. As we saw,
Taylorism, while it attempted to more deeply control and rationalize
the work processes of the individual---in a way mechanizing her---also
created new classes of worker and expanded the role of human managers
in the labor process. Articles that posit factory work as a precursor to the automated
car, without going deeper into that history, implicitly accept a
vision of teleological progress that comes to color the conclusions
and possible futures presented. In truth, automation may look very
different depending on where in the hierarchy 
a person happens to fall, but the historical lesson is that human
involvement remains, though altered in space, time, and kind. As John
Diebold pointed out in 1953, there will be ``no worker-less
factories as a result of automation'' \cite[p. 63-64]{dieboldNew}
precisely because human beings will be needed to construct, to repair,
to manage, and to oversee.

So we come to one of many contradictions at the heart of
automated vehicle research. Their promise:
to provide to us a greater measure of creativity\footnote{They claim
  to do so largely by allowing people to do something other than sit
  in traffic: to read, to sleep, to eat, to work. That this idea of
  automated vehicles as a time-saving convenience feature persists
  despite the potential for these vehicles to also eliminate
  enjoyable, skilled, and rewarding parts of driving is an intriguing
  contradiction. And, at another level, this idea disregards the fact
  that the automated vehicle has shifted since the 1950s from a
  potential family space to an extension of the working environment,
  which threatens its own sort of potentially uncreative labor.} in the act of
driving, to remove some of the ``menial'' and routine tasks of manual
control in favor of strategic decision-making. In this analogy, the
driver goes from being the manual laborer to the creative manager. But
despite a focus on relieving tedium, this is not the way these systems
have primarily been envisioned. Instead, in the process of following
the dream of fully automated operation---where the human labor has been
entirely removed from the shoulders of the person in the vehicle, and
displaced to the invisible labors of mapping, programming, and
monitoring---engineers are designing systems where the ``driver''
seems present largely to ensure the operation of capital-intensive
machinery, burdened with new but perhaps even more menial tasks of
machine tending.\footnote{See for example Tom Simonite, ``Lazy Humans
Shaped Google's New Autonomous Car'' \cite{simonite}, which discusses
the human role within Google's test vehicle,
and the company's response. This is purely speculation, but due to
the way the system operated previously, it is possible at least one Google
employee fell asleep at the wheel, which was the catalyst for their
concern and change in focus.} 

Why might this be? To understand it, we must understand something
about AI history, and the ideologies intertwined with artificial
intelligence research.

%% This approach
%% presents serious risks. It may be some time before the human inside
%% the car can be entirely disengaged with the driving task,2 if that is
%% even something we want as a culture, which means an interim period of
%% operation potentially characterized by “hours of boredom punctuated by
%% moments of terror.”3 The danger of human inattention,4 which has shown
%% up in aircraft automation with sometimes disastrous results, is
%% actually pushing aircraft manufacturers to more fully involve the
%% human in the process of flying, even as cockpits become more and more
%% computerized.5 And by focusing on ever more automation, rather than
%% appropriate automation, we may also be removing some of the parts of
%% driving that are most enjoyable: by replacing the skilled craftsman
%% with the automaton and the machine tender, we risk making driving
%% sterile and dull. 

\section{Resonance of Artificial ``Intelligence''}
%% 1.2) place to introduce the brief overview of AI history, of what goes
%% into the different approaches
%% --starting from historical visions of automata (1 p)
%% --classical symbolic approaches / physical symbol system (3 p)
%% ----which in a way we have returned to today w/ explicit mapping
%% --Rodney Brooks subsumption architecture and ``world is its own best
%% model'' (3 p)
%% --Leaning heavily on H. R. Ekiba and his approach to illuminating the
%% unstated assumptions of AI research areas (2 p)
Artificial intelligence has its own history, distinct from that of the
automation of work, that feeds into driverless car narratives. Many popular
portrayals of AI care little about the actual history of the field,
and generally pull from major moments that garnered enough popular
attention to enter the cultural knowledge-base: ELIZA, Deep Blue,
Watson, and Siri. But coverage of advances in other artificial
intelligence tasks often cross-pollinates with self-driving car
coverage, either through direct reference or indirect association.
Some appreciation for the history of the field
of AI is important for understanding these moments as they are being
used:  as models of progress and evidence of the technological inevitability of
self-driving vehicles. And the technologies of the moment, employed in
cars as well as other AI applications, are critical here. The history
of AI as conveyed through news coverage about self-driving vehicles is
not a complete one, but it is one side of the truth. And even
engineers ignorant of this history may make similar mistakes of understanding.

%%TODO (DONE} Cite the automaton in the western imagination! Kang

Intelligent machines are not a new idea. Just as automation has long
been a part of human history, dreams of artificial life suffuse our
legends---though as Minsoo Kang
rightly notes, these early automata are of diverse types and kinds,
and some are readable as automata only in hindsight, due to their
similarities with contemporary robots \cite[p. 15]{kang}.
Nonetheless, that we continue 
to retell these stories in this way should tell us something about the
lure of the automated, its power as a ``hybrid entity'' that can
mediate between living and nonliving worlds \cite[p. 19]{kang}.
The myths about Hephaestus and 
his creations, notably Talos, a golden female automaton, come to us
from antiquity \cite[Ch. 1]{mccorduck}, but continue to be cited as
historical antecedents in 
literature on autonomous robots and their ethical issues \cite[p.
  3]{patrickLin}. Automata, 
or rather semblances of automata, appear in Hellenic Egypt, with
priests as puppeteers pulling their strings \cite[Ch. 1]{mccorduck}. The history of
artificial life is intertwined with that of autonomous machines: the
creation of Pygmalion's Galatea echoes the same practices and
concerns, as does the story of Mary Shelley's ``modern Prometheus.'' (Of
particular note for the purposes of this argument, Shelley's vision of
artificial life is inspired and physically mobilized by static
electricity, the infusing of a ``spark of being'' into the creature; and
electrostatics were a highly public research topic and indeed public
spectacle, complete with live demonstrations, in the 1700s and early
1800s) \cite[p. 44]{shelley}. Judah Loew ben Bezalel, a Talmudic
scholar, is in legend the 
creator of the golem, a being animated from clay who functioned as a
spy against the Gentiles \cite[Ch. 1]{mccorduck}. Though Loew's was not the only golem
recorded in myth, the rabbi occupies a special position among the most
prominent AI researchers of the 20th century: Pamela McCorduck records
that Marvin Minsky and Joel Moses grew up with a ``family tradition
that they are descendants of Rabbi Loew,'' and Joel Moses claims a number of
other American scientists, including John von Neumann and Norbert
Wiener, also consider themselves among the descendants \cite[Ch.
  1]{mccorduck}. This is all 
to say that while the technological drivers of conceptual visions are
more contemporary, ancient myth and legend continue to subtly underpin
research in autonomy and artificial intelligence.

%% Drosz and Vaucanson
As Jessica Riskin chronicles in her studies of eighteenth and
nineteenth century automata, clever inventors, interested in going
beyond mere representation, created a variety of impressive
simulations of life---that is \emph{simulation} in its modern sense, meaning
``experimental models that can elucidate the natural,'' rather than its
contemporary sense which would have connoted artifice \cite[p. 605--606]{riskinDuck}.
Makers of automata strove to imitate the very materials of life,
hoping to ``make the parts of their machine work as much as possible
like the parts of living things and thereby to test the limits of
resemblance between synthetic and natural life'' \cite[p. 606]{riskinDuck}.
Automata of this era ``bled,'' ``defecated,'' and ``breathed,'' though some
of these functions were themselves faked, such as in the case of
Vaucanson's Duck. Nevertheless, imitation was central to the project,
and in this way these early automata prefigured at least some of the
developments in AI and Alife. These early simulacra, though little known and
rarely referenced today, provide critical background for the attempts
that followed. 

%% Steam governors  κυβερνήτης
Many of these automata, despite being surprisingly accurate
mimeses of life, did little in terms of interaction with the
environment. Meaningful interaction requires closing the loop between
sensing and acting in the manner of a
 ``teleological,'' self-governing mechanism with corrective feedback.
 Indicative of Norbert Wiener's later research into cybernetics, such
 corrective feedback mechanisms had been studied since at least the
 late 18th century, when James Watt incorporated a governor into his
 steam engine. Watt himself had pulled from earlier applications of
 governors in windmills, which had been used since at least the 17th
 century \cite{richardhills}. James Clerk Maxwell, most famous for his equations of
 electricity and magnetism published in the 1865 paper ``A Dynamical
 Theory of the Electromagnetic Field'' (among the most important
 equations in physics), published a paper in 1868 on centrifugal
 governors in steam engines in the \emph{Proceedings of the Royal Society}. This
 paper, ``On Governors,'' became one of the central papers in early
 control theory \cite{ottomayr}. Bringing together a number of existing
 areas including control theory,
 cybernetics---from the Greek \emph{cybernetes}\footnote{This is as
   written in Wiener's contemporary papers. It could more properly be written
   as \emph{kybernetes},
   $\kappa\upsilon\beta\varepsilon\rho\nu\acute{\eta}\tau\eta\varsigma$
   \cite[p. 11]{cybernetics}.} meaning 
 ``steersman'' \cite[p. 6]{wienerMainIdeas}---extended their reach to more complex
 systems: ``control and communication in the animal and machine.'' As
 Norbert Wiener puts it, control, or the
 feedback mechanism, is necessary for the extension of information
 theory into communication theory. Cybernetics envisions the world in
 terms of feedback mechanisms, which can be used to explain a variety
 of phenomena in living organisms: homeostasis, balance, and motion
 disorders like locomotor ataxia and Parkinsons all fall within the
 cybernetic sphere \cite[p. 10-15]{wienerMainIdeas}. And all are envisioned
 as outcomes of systems that pass messages internally. In this way
 cybernetics is a forerunner of the discipline of
 artificial intelligence, which is interested in re-creating many of the same
 self-regulating systems within computers---and which claims a
 similarly broad explanatory role, whether the goal is playing chess,
 investing in the market, or driving a vehicle. This replacement of
 competencies tends toward the obsolescence of the human being, and
 again is interpreted in many stories as a teleological process, a
 perpetual advancement of technological competencies.

Integral to the history of AI as a field is that it was fundamentally
interdisciplinary from the start. Like its forebear cybernetics, it
brought together researchers from physics, mathematics, biology, and
early cognitive science. The field began in earnest with the Dartmouth
Conference in 1956, which assembled many who would continue to
be preeminent researchers over the next decades. Hosted by John
McCarthy (who originated the name
``Artificial Intelligence''), Marvin Minsky, Nathaniel Rochester, and
Claude Shannon, attendees included Trenchard More, Oliver Selfridge,
Ray Solomonoff, Allen Newell, and Herbert Simon: all were united by
``the idea that there was a rigorous and objective way of explaining
the human intellect'' \cite[Ch. 5]{mccorduck}. The research areas of
the ``Dartmouth Summer Research Project'' included language learning
and use, ``neuron'' networks,\footnote{Neural networks are one of the
  intriguing long-term stories of AI research, subject of much
  controversy over the years regarding whether or not they would
  actually work. A couple of theoretical developments altered them
  from a curiosity to one of the main techniques in modern AI. This
  half-century journey was presaged by the one sentence: ``Partial
  results have been obtained but the problem needs more theoretical
  work'' \cite{dartmouthconf}.} self-improving machines, and computational 
creativity \cite{dartmouthconf}. Early successes spurred romantic
predictions, and by 1960, human-level intelligence was foreseen by
some to be only a decade away \cite[p. 3]{winston}. Overexpectation, however,
led to a first AI ``winter'' from about 1965 to 1970, in which the
grand promises of AI were shown to be much further off: the current
techniques simply did not yield advances at the required rate. As Pat
Winston put it: ``Everyone searched for a kind of philosopher's stone,
a mechanism that when placed in a computer would require only data to
become truly intelligent'' \cite[p. 4]{winston}. But by the 1970s
research was improving, and excitement building again.


%% DARPA SCI
The early to mid 1980s were also a time of great developments in
Artificial Intelligence, an era of ``celebrity science,'' high hopes,
big investments, and subsequent great public disappointment with the
coming of another ``AI Winter'' beginning in 1987 and
1988.\footnote{These are roughly the dates Russell and Norvig give in
  \emph{Artificial Intelligence} \cite{russellnorvig}.} But despite the
earlier warnings of Roger Schank and Marvin Minsky that overoptimistic
expectations for AI would result in another winter like the previous
one in the 1970s, overall expectations were high, especially within
the business community, which funded companies and assimilated AI
techniques into real applications \cite[Afterword]{mccorduck}. Though
the 1980s continued with divides in 
the field about approaches to artificial intelligence, the decade
actually resulted in a wide variety of successful projects based on
improvements to expert systems, machine learning, natural language
processing, and computer vision \cite[Afterword]{mccorduck}. The 1983 US Strategic Computing
initiative, led by Robert Kahn at DARPA, had AI as its third focus
area, with ``image understanding'' and interpretation---made possible by
the digitized image---as long range project goals. In its revised
10-year plan, the initiative even included an autonomous land vehicle,
alongside a pilot's associate, and computerized battle management
software \cite{rolandShiman}. The project suffered serious management problems, and
was eventually canceled, precipitating the general crash in AI
funding---through which research quietly continued, waiting for another
up-tick in public interest. But the Strategic Computing project,
whatever its lofty goals, was no failure. McCorduck cites Roland and
Shiman as saying that ``AI now performs miracles unimagined when SC
began, though it can't do what SC promised,'' which speaks to the
important developments that were made in the service of DARPA's vision
\cite[Afterword]{mccorduck}.


Far from the generally accepted story of continued, natural
progression (that while AI may not have been possible before, it must
certainly be possible now due to continued development and greater
computing power), artificial intelligence proceeded in fits and
starts, and there are a number of approaches to AI research,
representative of different
ideas of how machine intelligence can be achieved. It is instructive
to look specifically at how three AI paradigms have envisioned their project.
Each is an answer to the question ``how can we build systems that
can operate without humans?''

%% Classical symbol approaches (3)
%%specifically w/ reference to ideologies
The first important AI paradigm is the classical symbolic system
approach. Associated with Allen Newell and Herb Simon, the main idea of the
physical-symbol system hypothesis is that ``symbols lie at the root of
intelligent action'' \cite[p. 109]{newellsimon}. Therefore not only
does intelligence require symbolic manipulation, it may indeed be
coextensive with physical-symbol systems; in other words, a
physical-symbol system has ``necessary and sufficient means'' for
intelligence and intelligent action \cite[p. 111]{newellsimon}. Such
constructs are \emph{symbol systems} in that they contain symbols and processes
that act upon symbols. And they are \emph{physical} in that they obey
physical laws and are realizable through engineering.
These symbol systems would arrive at answers through a technique known as
heuristic search: by looking through a tree of possibilities in an
intelligent way, they arrive at the appropriate answer.\footnote{The key
  point of heuristic search is that such answers are approximate, but
  arrived at quickly, rather than exact, but arrived at slowly or,
  perhaps, never at all.} Intelligence is applied in heuristic search
by the pruning of the tree: rather than having to apply brute force to
search the entire space, an intelligent system applying heuristic
search makes decisions at each node as to which branches are most
likely to produce a good result and searches those \cite[p.
  124]{newellsimon}. As Newell and Simon wrote, what makes a problem a
problem is ``not that a large amount of search is required for its
solution, but that a large amount \emph{would} be required if a requisite
level of intelligence were not applied'': the task of intelligence is
to ``avert the ever-present threat of the explosion of
search'' \cite[p. 125]{newellsimon}.\footnote{This is somewhat ironic in that
  modern approaches to AI focus particularly on search, which has
  often produced results that surpass logic-based approaches.} 

The conceit, then, of the physical-symbol system hypothesis is twofold.
First, it assumes that
human beings essentially operate in this manner: that we apply
symbolic logic and heuristic search to provide for our intelligent
actions. Second, it assumes that computers can be true physical-symbol
systems. The validity of these two assumptions is not necessary clear. John Searle
essentially rejects the computer as a physical-symbol system in his
Chinese room example. Instead, the computer (room) is seen as a cheap
imitation of such a system: a room into which strange symbols are
passed, the appropriate responses looked up in a book,
and then passed out again, all without anything in the room having
access to their meaning \cite{chineseSearle}. Though meaningless
symbols are being processed by such a contraption, his view is that no
electronic computer ``can really manipulate symbols, nor really
designate or interpret anything at all'' \cite{escapingBoden}. This is
a philosophical question of whether or not computers are capable of
true intelligence, not a matter of whether or not they can
convincingly imitate intelligence (and other philosophers of AI, such
as Margaret Boden, do not take Searle's view on the
subject).\footnote{Indeed, I find the Chinese room example to be
  lacking in several respects, particularly that it rests on some
  na\"{\i}ve assumptions about what can or cannot be intelligent that seem to beg
  the question it tries to pose. What is important about the
  argument is that, right or wrong, it challenges the basis of
  computers as intelligent systems.}

The former point proves harder for symbol-system researchers to dodge.
While it may be true, as is 
fundamental to the field of AI and which even Searle holds, that
machines can think because ``we are precisely such machines,'' \cite[p.
83]{chineseSearle} and therefore it should be possible to create
intelligent machines, this does not guarantee that symbol systems are
the way to achieve intelligence. It may well be that introspection on
thought is a large part of why the logical theory seemed so
compelling:  we like to think that we behave logically. Researchers like Newell
and Simon used ``think aloud'' experiments to identify problem-solving
techniques, \cite[Ch. 10]{mccorduck} which seems naturally to suggest a
logical response:  when
asked to describe how we came to some decision, basing it in logic
seems the most acceptable alternative. While we certainly may apply logic
and process symbols, there is good reason to think that is not how we
spend most of our time. Symbolic logic takes a lot of mental capacity,
so we generally use other sorts of shortcut processes to come to
decisions. Pollock calls these ``quick and inflexible'' or ``Q\&I''
models \cite[p. 120]{pollock} and Dennett refers to them as ``habitual methods'' or
mechanical routines \cite[p. 157]{dennett}. We rely on Q\&I models to do
much of our day-to-day reasoning because it is ``very important for
humans to be able to make rough probability judgments'' \cite[p.
  120]{pollock}, and accepting the output of such approximate models
is not at all unreasonable in the absence of evidence for their
inappropriateness to a situation:  a logical solution may be difficult
to arrive at in some situations. The physical-symbol system hypothesis
is just that, a hypothesis; and historically, the classical symbolic
approach did not yield results as quickly as expected. This was part
of the reason for the first AI winter, and researchers moved on to
other approaches. Achieving humanlike intelligence via logic
systems was the archetypal dream of early AI, but while logic is
part of the puzzle it is not the entireity of it.

%% Rodney Brooks (3)
One of those researchers pioneering a new way of doing AI was Rodney
Brooks. Part of a different wave of AI researchers, disillusioned with
the failures of logic-based robotic systems (like Shakey, the SRI
robot named for its tendency to shake when in motion) \cite[Ch. 10]{mccorduck} to
achieve intelligent results, he posited a new way to build intelligent
robots, defined by the subsumption architecture \cite[p. 353]{mobilebrooks}. Above and
beyond the problems that physical-symbol-system-based AI had in
relatively controlled domains, robots controlled with these
techniques, overwhelmed by the complexity of the real world, responded
slowly and ineptly. To rectify this, Brooks attempted to cut out
cognition altogether, focusing instead only on sensing and
action \cite[Afterword]{mccorduck}. Rather than attempt to engineer a
human-level intelligence at once, when years of research had failed to
produce anything approaching these results, why not start small?
Insects, after all, lack cognition, but respond more adroitly to the
world than Shakey and its contemporaries. Uncertainty everywhere in
the physical world makes world-modeling extremely difficult: the
model has a tendency to get out of sync, and there needs to be
some reliable, calibrating stimulus to return it to accuracy. By building systems that can
accommodate uncertainty, Brooks believed, more progress could be
made \cite[p. 347]{mobilebrooks}: instead of modeling the world, treat
the world as ``its own best model'' and focus on embodiment and action
within the environment \cite[p. 256]{ekbia}.

Rather than being built into a single integrated system, the
subsumption approach is by definition modular. More complex behaviors
are built out of simper ones: level zero competence might avoid objects,
while the next level would wander aimlessly, and the next would
attempt to wander to places it had not been before \cite[p.
  351--352]{mobilebrooks}. Each layer is separate (and can actually run 
on its own processor), and can read data from
and write data into the layer below it (hence the term \emph{subsumption}: 
the higher levels subsume the lower). When a higher level fails or
cannot run, lower levels continue to operate, and basic behavior is
maintained \cite[p. 355]{mobilebrooks}. This approach has tempting
biological connections as well:
accurately or not, one can imagine the human being as a robot with a subsumption
architecture, where breathing and heartbeat are lower than balance,
which is lower than voluntary motion, which is lower than logical
thought. And temptingly, it is an approach that can handle
uncertainties, as each layer is built to be robust in the inevitable
event of inaccuracies and lost messages. Theoretically, one could
imagine building systems of great complexity (e.g. cars) using these approaches,
but this architecture, the radical relativist response to the
structural realism of the physical-symbol system, did not alone manage
to allow the creation of robots that exceeded insect intelligence.

Brooks himself abandoned the project of building intelligence from
these humble, subsumptive blocks. Instead, his later research, for example
the Cog robot, shifts focus from ``emergence'' to ``integration,'' and
reversed some of his initial fervor to avoid representation \cite[p.
  258]{ekbia}. He skipped the middle of the evolutionary tree, straight
to humanoid forms, because the evolutionary approach was too slow: 
Brooks reported, it was ``starting to look like, if I was really
lucky, I might be remembered as the guy who built the best artificial
cat,'' a distinction he apparently did not desire \cite[p.
  65]{brooksflesh}.\footnote{As cited in \cite[p. 258]{ekbia}.}
But to do this, bootstrapped knowledge from other sources was
necessary, bringing back some of the world modeling inherent in older,
symbolic approaches. 

%% Deep learning
A number of conceptually different approaches to AI have been tried
over the past 50 years, often with similarly deep-seated
ambivalences.\footnote{Ekbia's book does an excellent job
  exploring and cataloging many of these, more than I have space for.
  The logical and neorobotic approaches appear most relevant as
  broad-strokes historical precursors for inclusion here. But the rest
of his book is worth reading for more detail on other schools of AI
thought.} Ideas fade 
and resurface as fashions change, and greater computational power
allows ``failed'' techniques to be tried anew. But much of this
history is presently unappreciated outside of the discipline. What the
physical-symbol system and embodied cognition 
have in common is their attempt to start from first-principles or zero
knowledge, and to build inexorably toward intelligence. The difficulty
of this road led Brooks to recombine some of the old manner of
knowledge representation into his robotic techniques. Today, multiple
strains of AI research play out the same tensions. While Tomaso Poggio
and others at the Center for Brains, Minds \& Machines attempt to
emulate human learning,\footnote{Speaking at the CAST Symposium at
  MIT, September 26, 2014, Poggio noted that their approach is
  actually stepping away from ``Big Data'' and AI approaches applied
  by Google, among others.} other researchers bootstrap their machines
with more fully-formed models. Ours is the age of
statistical, probabilistic AI, lead in the robotics space by Sebastian
Thrun, Wolfram Burgard, and Dieter Fox with their book
\emph{Probabilistic Robotics}.\footnote{The book begins with a nod to
  the self-driving car project: ``Wouldn't it be great if all our cars
were able to safely steer themselves, making car accidents a notion of
the past?'' Utopian vision of technological possibility are not
limited to press accounts \cite[p. 3]{thrunProb}.} This research,
organized around a variety of algorithms based on Bayes rule,
distinguishes itself from prior model-based (logical) or ``reactive
behavior-based'' (subsumption) approaches by dealing gracefully with
uncertainties in both the world model and sensor inputs \cite[p.
  9]{thrunProb}. But despite that probabilistic robotics fundamentally
underlies the major automated vehicle innovations reported by the
popular press, it is rarely, if ever, mentioned. What receives attention in the
self-driving car narrative is a different gloss on the probabilistic
system: today's
AI buzzword seems to be ``deep learning,'' which appears to yield
radical new possibilities everywhere it is 
applied---though it really just means a return of neural
networks,\footnote{These systems still have little or nothing to do
  with actual neurons; they are brain-like in only the barest
  toy-model sort of way, despite how they are often represented in the
  press.} which, armed with some improvements in weight generation,
more layers of
nodes, and more data to train with, have been able to eclipse many of
the previous techniques. The new source 
of bootstrapping information is data, ``Big'' and ``small.''

%%EDIT: this could all use some clarity

Neural networks are essentially just a web of nodes with
interconnections between them.\footnote{For a primer on neural
  networks, see \cite{neuralJordan}.} They may consist of multiple layers
(hence ``deep'' in deep learning) or may be shallow. Also known as
``multilayer perceptrons,'' such networks have an input layer and
output layer, with one or more intervening hidden layers. Each node
corresponds to a particular feature or combination of features in the
input, and therefore works to classify inputs into different
categories. Each node is also connected to all the nodes in the
following layer with some tunable weight. The process of training the
neural network involves adjusting those connection weights so as to
be more sensitive in distinguishing different types of
inputs. This tuning,
traditionally, has been done using supervised method and
backpropagation of errors: the output result is compared to an
expected classification result, and the error used to tune the weights
more appropriately. These techniques are actually used in the
probabilistic robotics field, to train learning algorithms to develop
maps out of noisy sensor data \cite[p. 284-297]{thrunProb}. Deep
learning extends this technique, decreasing 
the amount of ``feature-engineering'' (a human task, identifying the
features the network should use to distinguish inputs) required to
train the system. The ``ideal'' training situation is entirely
unsupervised: the network independently ``learns'' the features of
unlabeled and unclassified input, without any human input. This ideal
prospers both due to convenience in terms of human effort (less
programmer effort required to build labeled sets of training
data\footnote{This labeling is considered ``inhumane'' work, and contributes to
a reluctance to use machine learning in automotive applications,
according to G\"{o}de Both \cite{bothpt2}.}) and
due to the deep-seated ideology, which reappears in the imagined
operations of self-driving cars, that machine independence is
paramount. Unsupervised learning is often seen as more impressive and
therefore more valuable research.

The sort of ``understanding''
involved in even the most impressive current systems is limited. Image
recognition systems, for example, are not
able to answer questions about the scenes that have to do with
the material properties of the objects, or likely results of various
actions \cite{gomesJordan}. It is important to understand why this technique is not a
panacea in order to understand the possibility for alternatives to the current
driverless car vision. These statistical techniques leverage existing data
sets by training machine-learning systems; but, as we will see,
machine-learning systems have properties that make them ill-suited for
safety-critical systems, and autonomous vehicles are designed with a
mix of approaches that allows for more introspection into their
workings. Another recent slew of articles focuses on
the ``self-aware'' Mario created by researchers at the University of
T\"{u}bingen. Any pretense to worry about a ``self-aware AI . . . with
an insatiable desire for material wealth'' that knows ``how to kill'' \cite{vincentMario}
is simply journalistic excess,\footnote{As the
  researchers well know. This case is just a convenient example of how
such stories spiral out from the lab and acquire new meanings.} and suggests
significantly more care must be taken in the use of such terms. Mario
is programmed with emotional states similar to the way a word-processing program is
programmed with different modes or display parameters. These states
are simply caricatures of emotions, and that, plus the program's
``natural-language'' interface, makes it appear more eerily
science-fictional than it actually is.

%% ``self-aware'' mario, image recognition and other
%% claptrap (1) (incl. John von Neumann & ALife (See CMS790 paper)??)
As has happened before, such AI hype is proliferating, and these
stories may impact the way all other AI work is
perceived by the public (especially viewed in the light of
popular concern about autonomous vehicle ethics). Popular claims
about the utopic promise of deep learning, to
learn about the world ``on its own,''
abound. Google's and Stanford's recent
improvements in image recognition, driven by deep learning,
\cite{markoffImage} triggered a wave 
of popular speculation about computer vision meeting or surpassing
that of humans. Since vision is the primary sense involved in human driving
and one of the main research areas for automated vehicles, such
advances would seem to translate automatically into the self-driving
car space. Though some articles note that current
state-of-the-art image recognition is still considerably less capable
than people are, many articles still present the uncritical idea that we have solved
the image-recognition problem---or rather, that increases in computing
power mean it will necessarily soon be solved. By this narrative, seemingly limitless
possibilities are open before us: a new universal problem solver, like
feedback control before it, seems to have broadened the frontiers
of the future. As one should expect, the real
history of artificial intelligence and robotics is far more nuanced.
We would do well to remember that 
progress has been slow, contingent, and heterogeneous, a product of a
wide variety of concepts and techniques. Logical, precise models,
reactive-subsumptive control, and probabilistic re-framings of the
navigation problem all had their parts to play in this evolution. And
it is difficult to argue with techniques that ``work,'' which
statistical AI certainly does in the sense that it has been able to
spawn numerous real-world systems, from personal assistants to
self-driving test vehicles, that exceeded that which was possible with
prior approaches. However, the limits of each new paradigm are rarely
obvious \emph{a priori}, and it remains to be seen how far current
algorithms can carry the field, and whether subsequent algorithmic
advances will allow progress to continue unabated. This is the
environment that autonomous cars, as a 
research area of artificial intelligence, find themselves in today:
part of a new surge (or bubble) of interest in the field, driven by new
or newly extended techniques. The search continues for Winston's
philosopher's stone.

\section{TEMP: AI and what it means to know}


Though computer science and philosophies of AI have been using
``intelligence,'' ``knowledge,'' and ``understanding,'' among other
words, to talk about computers since the beginning of the field, these
uses should not be taken at face value. ``Intelligence'' is slippery,
and its definition is not constant over time. It is difficult to
define intelligence in ourselves \cite{huntIntelligence} and yet another thing to define it
in relation to other entities. Weaving was once considered to be a
peculiarly human capability, a sign of an advanced, intelligent
mind \cite[p. 627]{riskinDuck}. But after Vaucanson's loom allowed mechanical devices to weave
seemingly on their own, this capacity was no longer seen as uniquely
human, and was no longer a marker of intelligence. The same process
occurred with chess in the 1990s. When IBM's Deep Blue beat Gary
Kasparov, chess ceased to be the standard by which intelligence could
be judged, precisely because it had been achieved. Real intelligence
had to lie elsewhere: for example, in the game Go, mastery of which
has continued to elude machines.\footnote{``When IBM's Deep Blue beat
  Gary Kasparov in 1997, most Artificial Intelligence researchers and
  commentators decided that chess playing did not require intelligence
  after all and declared a new standard, the ability to play Go'' \cite[p. 623]{riskinDuck}.}

Nevertheless, machines manage to do things that \emph{seem} intelligent. So
though these terms are heuristics for understanding the observed
behaviors of machines, they slip slowly over time from self-conscious
scare-quoted use into casually accepted statements. While automatic
translation may seem ``intelligent,'' or a system that can define \emph{\'{e}toile}
as ``star'' may seem to possess ``knowledge,'' this intelligence or
knowledge is perhaps very different than our own. A deep
epistemological question presents itself: how do we know, and how do
machines ``know''? Many AI systems operate via statistical pattern
recognition, so we may ask whether we believe human intelligence is
also merely pattern recognition: does a system that can associate star
with its definition really know what a star is? Is linguistic
association sufficient for knowledge?

H. R. Ekbia and others remind us that we should be skeptical of the
applications of these terms to computational processes \cite{ekbia}. As we have
seen, Searle's thought experiment of the ``Chinese room''
argues that symbol processing and pattern recognition alone is not
intelligence, though from the outside the results may appear to be
intelligent. Despite the faults of his argument, his caution
about ascribing overly ambitious human ideas to computational
processes is warranted. Even Pamela McCorduck, a colleague of several
notable AI researchers and a believer in the field in general, hedges
on how intelligent some of the programs she discusses in \emph{Machines Who
Think} actually are. Ornstein, Smith, and Suchman, in their 1984
article ``Strategic Computing,'' warn of the difference between domain
capabilities and ``common sense,'' and suggest that ``unwarranted
optimism'' and a particular funding climate (issues also present today)
push researchers to mask the shortcomings of AI with ``semantic
shifts'' \cite[p. 14]{ornstein}. We alter the definitions of ``knowledge'' and ``understanding''
to fit what our machines can do, and these claims, taken literally,
``give rise to unrealistic confidence in the power of the
technology'' \cite[p. 15]{ornstein}.
The two-way process of linguistic and technological change---that
intelligence gets applied to describe whatever researchers manage to
achieve, while real ``intelligence'' retreats away from each
computational advance---leaves these terms poorly defined.


\section{Lessons from Autonomy Research}

Spaceflight, both manned and unmanned, provides myriad reasons to
invest in automated systems: time delays prevent or at least greatly
inhibit remote control from Earth; conditions where humans are not
uniquely equipped to survive suggest the use of mechanical explorers
instead; and precise control functions with redundant backup systems
suggest the use of the unique capacities of computerized systems to
monitor constantly and act immediately in the event of an emergency.
And these are all senses with which the narrative of automation in
spaceflight is inflected today. Certainly unmanned spaceflight had a
strong interest in automation since at least the 1950s, with serious
research being done on simple automated probes to do a fly-by of
Mars \cite[p. 1]{battin}. But the story of automation in spaceflight is
not nearly so simple as it appears. Rather than being the ultimate and
obvious endpoint of a progression of human engineering in space, the
roles and implementations of automation remained fraught, highly
contested by astronauts themselves due to professional pride, national
politics, evidence for the capability of the human being to be in
control, and the dogma among certain groups that such control is
necessarily more reliable than computerized automation---which has not
a little to do with the fragility of pre-integrated-circuit computer
technologies.

\subsection{Manned Spaceflight}

From 1952 to 1954, \emph{Collier's} magazine published a series of articles
titled \emph{Man Will Conquer Space Soon} which described Wernher von
Braun's vision for manned spaceflight \cite{scribdColliers}
\cite{dreamsofspace}.\footnote{The illustrations from this magazine
  are reputed to be some of the ``most influential images of the early
space age'' \cite[p. 9]{marketingMoon}.} Numerous 
articles focused, not surprisingly, on the people who would do the
voyaging, how they would be selected, tested, and kept alive. But
lurking behind this majestic picture of manned spaceflight was a dark
realization for prospective astronauts: to von Braun himself, the
astronaut would be a mere passenger, ferried into space by automated
rockets. In a speech to the Society of Experimental Test Pilots in
August of 1959, a hostile audience for this sort of rhetoric, von
Braun emphasized that human control in rocketry is ``actually
undesirable'' because human beings are ``outrageously slow and
cumbersome'' in missile terms \cite[p. 66-67]{DM}. While von Braun
may have been the most polemical, he was not alone in his cautions to
test pilots about the limits of their capacities.
In an earlier speech to the SETP (October 4, 1957) on automation and
the ``survival'' of the test pilot as a profession, Richard Horner
took a balanced and moderate view on human involvement, but warned
that technology would progress faster than human beings change:
the ``link'' that improves the least ``is the man himself'' \cite[p. 19]{DM}.

Pilots---prospective astronauts---were not about to find themselves
cut out of the control loop, and for good reasons. Not only was their
personal pride at stake---as military test pilots, they possessed a
particular standing in part by virtue of being in control of dangerous
and cutting-edge vehicles like the X-15, which though it necessarily
incorporated significant automation to deal with hypersonic flight and
the transition from atmospheric to extra-atmospheric operation, kept
the pilot firmly in command\footnote{Kelly Johnson, the leading aircraft
designer in the US, working at Lockheed, objected to the manned
component of the X-15
project, but his objections were overruled by the NACA \cite[p.
  46]{DM}.} through fly-by-wire controls \cite[p. 55, 61--62, 77]{hypersonics}---they had 
every reason to doubt the capabilities of computerized guidance and
control systems.\footnote{Even the fly-by-wire control was somewhat
  controversial. Milt Thompson, a test pilot in the X-15 program, said
  of it: ``you would like him [the engineer responsible] to be in the
airplane with you to be exposed to any adverse results'' \cite[p.
  55]{DM}.} Al Blackburn's response to von Braun's speech to the
SETP was particularly empassioned, and recounted his many experiences with
``brain-dead autopilots, broken fire control systems, and failed cockpit
computers'' \cite[p. 68]{DM}. From
his admittedly partial perspective, the resourcefulness of human
pilots is the answer to automated technology's lack of reliability.

%%TODO2 can you find this exact TERM used?
%%the shibboleth ``third world man'' was waiting on the outcome of the
%%space race to decide whether to be communist or capitalist 

Such debates over control did not go away, but lasted through each
of the Mercury, Gemini, and Apollo programs. Mercury, which began the
manned space program in the United States, had an ambivalent
relationship to its astronauts. They were pilots, rather than von
Braun's ``missile riders,'' but only just. While Mercury preserved an
aeronautical joystick, which controlled the reaction control thrusters
(RCS) and thereby allowed the astronauts attitude control over their
space ``capsule''---nomenclature that would soon find itself under
siege \cite[p. 85]{kauffman}---they could not fly the Mercury craft in any
aeronautical sense. Like the warheads of the ballistic missiles from
which the program spawned---and which were in fact used to convey the
Mercury astronauts into space---the astronauts were launched by
automated rockets, and returned to the ground on a purely ballistic
trajectory. But the presence of the human astronaut was greatly
important to the Mercury project, even if he (they were all men)
largely flew as ``Spam in a tin can'' \cite[p. 60]{wolfe}. Putting a man in
space was a political venture more than a scientific one or a matter
of national defense.\footnote{Scientists held and continue to hold
  that unmanned missions are more effective in generating scientific
  discoveries than manned ones.} In the eyes of the world---or at
least as interpreted by Washington---the
reputation of the United States depended on it. Project Apollo was
conceived of by Kennedy as a program of diplomacy,
which would help to ``win the battle that is now going on around the
world between freedom and tyranny'' by impressing the ``minds of men
everywhere'' \cite{KennedyMay25}: ``other countries of the free
world---troubled and restless'' were waiting outcome of the space race to
decide whether to be communist or capitalist \cite{KennedySep7}. Apollo was, among many
other things, a nationalist (and even perhaps imperial) PR campaign.
Spurred on by Russian successes (Sputnik, Gargarin), which were
originally of little political interest until the media reporting on
the events presented them in a frantic light as a possible sign of
Soviet technological dominance,\footnote{There was little coverage in
  Russian papers about the Sputnik launch (and even Gargarin was not
  greatly covered). However, the media frenzy that ensued in the United
States and other countries worldwide made clear the political
importance of being first in space. See \cite{bessonov}.} Kennedy pushed for a greater
investment in space research. But he was ``not that interested in
space''\footnote{He continued: ``I think it's good, I think we ought
  to know about it,'' but balked at the level of funding necessary
  for the manned space program unless it led to a US political and PR
  victory.}---primarily, he wanted to beat the Russians to some big,
visible space project \cite{KennedyNov21}. This project had to
be manned, precisely because human spaceflight captured people's
imaginations, and because the Soviets were certain to continue their
piloted space flight ventures. But Soviet spacecraft continued to be
more automated than their Western counterparts---Gargarin could not
operate manual backup functionality without unlocking a combination
lock, the
code to which was only reluctantly placed in the cockpit in a sealed
envelope rather than being radioed from the ground \cite[p. 89]{DM}---and
this automation was seen as a feminization of space:  Soviet
spacecraft required no skill to operate, and were therefore,
rhetorically at least, less impressive \cite[p. 90]{DM}. This reading was
supported some years later in 1963 by the flight of Valentina
Tereshkova, the 
first woman in space (a milestone not followed by the United States
until the 1980s). This automated approach perhaps fit a ``communist ethos,'' sending
the representatives of every-man and every-woman into
space,\footnote{Neither Gargarin nor Tereshkova had aristocratic
  family backgrounds.} but did
not fit with the ideally meritocratic culture and the political aims
of the US program, not to mention the flying culture of the astronauts
ultimately selected for the program.\footnote{All those selected for
  the Mercury program were military test pilots, despite NASA's
  existence as a civilian agency. Issues of secrecy alone
should not have disqualified other military personnel, and a greater
emphasis on science in the program would have suggested actually
including scientists on flight teams, as Mindell notes. Though over the recruitment
rounds the emphasis on test-pilot status decreased in favor of greater
engineering knowledge (and higher degrees), and though one geologist
set foot on the moon during the final Apollo mission, the astronaut
culture was dominated by influences from the test-pilot community.}

%% Mercury is a tin can with attitude control
%% -but, somewhat conversely, the position of the person in spaceflight
%% was very important politically
%% -Kennedy ``not that interested in space''; wanted to beat the Russians
%% with something BIG
%% -Russian space automation was a sort of feminiziation of space; their
%% astronauts required no skill
%% ---perhaps fit a communist ethos (sending the everyman (or woman)
%% into space), but not a meritocratic, capitalist one

%%%%WARNING CITATION STYLE CHANGE%%%%%%%%
%%cites removed below
%  with manual control\cite[p. 84]{DM}
% a backup for automated systems\cite[p. 83]{DM}

The development of the Gemini spacecraft---actually started after the
Apollo project, and intended to demonstrate capabilities necessary for
a mission to the moon to take place---set out to extend the
capabilities of Mercury, and incorporate lessons learned from the
precursor program. One way this occurred was specifically technical, a
re-engineering of the space capsule to allow systems to be installed
and serviced from the outside of the craft \cite{NASAGeminiConcept}. But another
reorchestration of space flight in the Gemini program involved the
human's interaction with the spacecraft through control systems.
Pilots wanted a role in launch vehicle guidance, and a number of
simulator studies were commissioned to identify the capacity of humans to
actually guide boosters into orbit. Pilots, spun in a
centrifuge in Johnsville, PA to simulate the immense acceleration of
takeoff, flew
simulated rockets: while some tests failed, with astronauts losing
control during stage changes, some were able to launch the vehicle
into orbit, and these tests served as evidence for the importance of
pilots to the ``reliability and flexibility'' of launch vehicles,
the same phrasing used to justify the pilot's role in the X-15
testing \cite[p. 72]{DM}. A few skeptics even argued that the lack of pilots
explained the high failure rate of automated rockets \cite[p. 73]{DM}.
Though no American would ever fly the launch stage of
the rocket, pilots continued to push for control. Robert Voas,
responsible for astronaut selection and training, was a firm believer
in the human role both for monitoring and managing automated systems,
but also 
navigation, communications, and performing research \cite[p. 77]{DM}.
Monitored automation, which prevailed in Mercury---with
the human ``more than secondary, if still less than primary'' \cite[p.
  77]{DM}---continued into the Gemini program. The Gemini pilot
interfaced with an inertial navigation system, a digital computer, and
an optical star tracker, responsible for tasks that would be too heavy
or too difficult to automate, and remaining as a backup for automated
systems. But Mindell holds that advancing
technology was seen to be served by the inclusion of more human
control, which Mercury (in association with simulator tests) had proved was
feasible \cite[p. 84]{DM}. The importance of orbital maneuvers,
 such as rendezvous and docking, for the Gemini missions suggested returning
the role of piloting to the spacecraft pilot, and rendezvous was
indeed performed with manual control. But, as
pilots (particularly, Jim McDivitt) soon discovered, orbital
rendezvous could not be achieved by the
traditional manner of flying: in orbit, flying toward an object is not
an appropriate way to achieve rendezvous, which requires not only
passing close to the target but remaining there for an extended
period. Matching velocity means matching orbits, and so ``catching
up'' to a target may require going slower to change orbits \cite[p.
  86]{DM}. ``Numbers, equations, and calculations'' would be 
required, and were bootstrapped to the pilot's senses with a new
readout, the IVI or Incremental Velocity Indicator, which could be
programmed with particular velocity changes in different axes and
would show visually when those particular burns had been achieved,
so that the human pilot would know when to stop accelerating \cite[p.
  86-87]{DM}. Pilots did not fight the implementation of such 
technology, realizing that anything that ``helped them manage the
chaos [of space flight] would extend their ability,'' and overall
liked piloting the Gemini craft, which they saw as expressing the
``national character'' through the importance of the individual pilot
to the vehicle's operations \cite[p. 88]{DM}. Gemini is 
a triumph of manual control, aided of course by automated read-outs.

%cites removed
% missions actually flew\cite[p. 93]{DM}
The Apollo program started too soon to learn the lessons of
Gemini---the Apollo system design was largely complete by the time Gemini
missions actually flew---but despite the
ultimately high levels of automation present with the Apollo
spacecraft, arguments about human role shaped development on this
parallel program. One major locus of such controversy was the Apollo
guidance computer: Should it allow fully autonomous (from Earth)
operation? What functions should be fully automated? The idea of an
autonomous navigation system was not fundamentally new. The Apollo
project pulled from previous experience in digital computer design and
development, both from military ICBM programs which often used
self-contained inertial guidance units, and from work by Laning and
Trageser from the Instrumentation Lab at MIT on a small Mars probe,
driven by ``self-dependent'' navigation, which could ideally use
complex logic to respond autonomously to problems \cite[p. 99-100]{DM}.
Though self-contained navigation capacity was a goal
for the engineers at MIT responsible for the guidance system---working
under Charles Stark Draper, who was also involved in developing
guidance systems for ICBMs---much of this functionality was ultimately
removed due to computer memory restrictions \cite{tindallMay12}. The Apollo
spacecraft would have to be in contact with Earth for guidance
corrections and instructions over the long term. Human beings were
again counted on for sextant readings, for initiating appropriate
program modes and monitoring automation systems \cite[p.
  4]{BennettExperience}, and the spacecraft 
could operate in a self-contained fashion for some length of
time, albeit with greater positional errors than when a ground
reference is used \cite[p. 191]{BennettCheatham}. The LM's approach
to the surface also involved a complex dance of manual and automated
capabilities, with shifting roles for different phases of the
landing.\footnote{While the details of this are largely superfluous
  here, see \cite{BennettCheatham} for more information.} While the initial
idea for lunar landing was that the 
computer would guide the spacecraft down, given appropriate landmark
fixes from pilots, this was not how the LM was flown: despite
possessing an autopilot capable of automatic landing, pilots used
their prerogative of ultimate control to take over at various stages
of landing: Armstrong, famously, took manual control on the Apollo 11
landing to seek out a cleaner landing spot and avoid a potentially
dangerous crater \cite[p. 3]{DM}. But even this description is too
simplistic. As Allan Klumpp describes in a
paper on the Lunar Module landing system: ``The essence of the
approach phase guidance system is that the LM commander can manually
steer the LM to the selected landing site, yet the trajectory he flies
is produced by an automatic guidance system'' \cite[p. 129]{Klumpp}.
Klumpp's self-described ``hybrid'' system accounts
for the ``obvious desire of any man to control his craft'' and
provides both flexibility and safety \cite[p. 129-130]{Klumpp}.
While the Apollo engineers perhaps
got away with a lot of automation, in part because the astronauts were
busy with Gemini instead, manual controls still proved critical to the
overall operation of the system. But the mission depended on human and
automated systems both, and the successful integration of their roles
and capabilities. When conceived of as a model for automated vehicle
engineering, the history of manned space programs does not fit the
popular narrative of automation, instead presenting a clear example of
joint human-machine systems in action.

\subsection{Unmanned Spaceflight}

While these manned missions were occurring, Jerome Wienser,
presidential science advisor and later president of MIT, and other
scientists were perennially against manned
spaceflight.\footnote{Wiesner not only disagreed with the importance
  of manned space flight, he took issues with the decision to perform
  Lunar Orbit Rendezvous, LOR, as the Apollo mission mode. He
  continued to oppose LOR after the official NASA announcement, and
  argued with von Braun in front of the
  press at Marshall Space Flight Center, until President Kennedy
  stepped in to end the argument \cite[p. 43]{Seamans}.} Unmanned exploration is
cheaper by orders of magnitude \cite[p. 66]{coxMurray}. To serve
scientific goals, humans are not required, and in his role as chair of
the Presidential Science Advisory Committee, Wiesner felt that his
duty was to argue against manned missions on scientific
grounds \cite[Chapter 2]{Levine}.\footnote{While Killian's list of goals for the
  space program included the human need to explore, national defense,
  national pride, and scientific experiment, and while the
  congressional hearings about the space program said little about
  defense, trading it for discussion of propaganda value and a
  ``vertiginous rhetoric'' of scientific exploration, manned space
  exploration is in general not justifiable by scientific ends \cite[p.
    194-197]{smithSelling}.} Instead of a manned program, scientists
have continued to push for unmanned probes,\footnote{And these probes
  actually get leveraged in the popular narrative as sources for
  automated vehicle design inspiration: ``And so Google's new vehicle
  design 
takes a leaf out of NASA's design book to cope with such
eventualities. `It doesn't have a fallback to human---it has redundant
systems,' said Fairfield. `It has two steering motors, and we have
various ways we can bring it to a stop''' \cite{simonite}. This is a
very reductive view even of the most remote of NASA's unmanned space
systems, let alone manned systems with which the Google vehicles must share
the critical characteristic of containing human occupants.} but even for
unmanned exploration systems in space, autonomy is not the
end-all-be-all goal. More important is the link between human
operators and scientists on the ground, and the remote science
platform responsible for carrying out instructions. 

While people may
think of the Mars rovers \emph{Spirit} and \emph{Opportunity} as
autonomous robots remotely carrying out science experiments---as
Clancey describes them often showing up anthropomorphized as a ``robot
geologist'' or 
``explorer'' in news coverage \cite[p. 7]{clancey}---they are
in reality telerobotic systems, remotely carrying out human commands.
The rovers are not 
sent out to wander or find goals by themselves. They drive mostly
blind, with only their immediate obstacle detection, following manual
waypoints entered by human navigators on Earth: their autonomous
path-finding systems are much slower, and use more power, and are
therefore generally avoided \cite[p. 118]{clancey}. While the rovers
are out of contact for the span of two weeks during solar
conjunctions, the instruments lie dormant and the rovers sit
still \cite[p. 25]{clancey}. Though the rovers are technically capable of
carrying out pre-planned science sequences during this time, including
automatic navigation, this capability is not used. Human observation,
human teleoperation, is too important a part of the process, and
autonomous operation without monitoring presents too many risks. Human
scientists define sites of interest, locations to take samples, and
the paths to reach them most safely and effectively. Scientists
contact the rovers at least once per day to relay plans, and again
to retrieve results, with more intense schedules during the early
parts of the program \cite[p. 58]{clancey}. Small sets of
operations are requested, and the outcomes monitored, with new plans
being made by humans to account for new data at each step.

This in some ways
represents greater engagement between scientists and the remote
machine than during the earlier \emph{Viking} mission, in which weeks
were spent writing programs to run the robot \cite[p. 58]{clancey}.
The recent missions step back
from programmatic operation, and toward closely coupled human
supervised control. The Mars rovers could
certainly have been made more autonomous, but this would have opposed
their function. The quality of the mission depends on ``aspects of the
MER's design that promote the \emph{agency} of the scientists''
themselves, rather than automated operation specifically \cite[p.
  xii]{clancey}. Scientific work in the field is ``opportunistic, 
serendipitous, and incremental'' \cite[p. 32]{clancey}, yielding
not so much to \emph{a priori}
plans of great detail, but Suchman's ``situated
action'' \cite{suchmanSA}. Researchers on the ground actually
experience a sense of ``telepresence'' through these ``synergistic''
machines, created 
by virtue of their closely coupled operations and the MER's
semi-anthropomorphic bodies \cite[p. 55]{clancey}. Scientists see
as if they are the rover, and have to ``retool'' their thinking, to
become the ``mind'' of the rover and plan its work in a ``symbiotic''
way \cite[p. 106, 110, 118]{clancey}. The system's autonomy does not replace the
scientists, but allows them to do more of what they want to do (and are
best at), and less of what they don't: some types of automation could reduce the
autonomy of the scientists, and their ability to act creatively and
spontaneously to capitalize on new findings \cite[p.
  118-119]{clancey}. This is a subtle point about automation design
and human agency, one
worth considering when evaluating designs of automated cars.

\subsection{Unmanned Exploration and the Researcher}

As Clancey suggests, autonomy is not an ``inherent property of
technology'' but ``a relation between people, technology, and a
task-environment'' and should be considered in those terms \cite[p.
  119]{clancey}. While we might say, colloquially, that 
``\emph{Opportunity} encountered'' something \cite[p. 8]{clancey}, this
is really just convenient shorthand for scientists on Earth encountering it
through and in concert with the telerobotic platform. Human knowledge,
perception, and common sense is integral to rover missions, and
organizations operating expensive 
technology in a high-risk environment quite reasonably want humans to
be responsible
for the well-being of the equipment. The same issues play out with
robotic vehicles for underwater exploration. The most famous of these,
Alvin and Jason, deeply involve human scientists as operators in real
time, either directly from within the vehicle or through a tethered
link \cite{NOAA1} \cite{NOAA2}. Most AUVs (or Autonomous Underwater
Vehicles) are not 
truly autonomous, but are operated remotely using low-bandwidth data
links via acoustic communications.\footnote{David Mindell, discussion
  with the author, September 3, 2014} This is largely an issue of risk
aversion. While it has often been assumed, even sometimes by system
designers, that long duration missions will be performed autonomously,
this is not the primary mode in which AUVs are operated: the last
thing you want to happen, as a scientist presiding over an expensive
piece of equipment, is to lose the vehicle irretrievably or have an
instrument fail on the first day and not know about it for weeks. So
instead of long, autonomous missions, most operations involve many
shorter missions with acoustic links coupling the device with
shipboard researchers.\footnote{David Mindell, discussion with the
  author, September 3, 2014} 

While self-driving cars would not
be much like Mars rovers or underwater robots, telerobotic exploration
provides potential lessons
to be learned in terms of human and machine roles, and the level of
autonomy one wants from a machine in a human-machine relationship.
Considering what type of interaction promotes the agency
of human owners and occupants---allowing them to do more of what they
want and less of what they don't---is particularly important,
especially while recognizing that the answer to what humans want to do
is not necessarily ``nothing.'' And while local streets are not as
remote and inaccessible as Mars or the bottom of the ocean, having
one's vehicle go missing on the way to or from a parking garage while
operating autonomously would be a significant issue, and the risk of
this kind of failure must be considered in the human-machine system
design. Cars are still capital-intensive pieces of equipment, and we
generally want to know where they are at all times. Whether vehicles
are monitored
from within or without, it seems unlikely designers will be able to
escape the need to loop the human into the decision-making process.

But there is reason to suggest that even this picture of supervision
and monitoring on a long timescale may be too ambitious for automated
vehicles of the near future. As we have discussed, a number of major
engineering problems require solutions before we can build cars that
are capable of fully automated operation in all reasonable situations,
which implies more than occasional supervision is still required: human engagement in
sensing and reacting, on reasonable time scales, remains necessary. The
specter that I have been dodging so far, throughout this chapter and
indeed through much of the thesis, is the issue of human attention.
Issues with attention have always been implicit within the discussion,
since inattention is precisely the human quality that makes
self-driving vehicles most necessary, at least rhetorically. But human
supervisory control, in its practical applications, has much to teach
us about the limits of human attention, and indeed such human factors
research is the source of the NHTSA's claim that users of
higher-levels of automation will not, and will not be expected to,
monitor its operation. An often-used justification
for entirely eliminating the human from the vehicle control loop is
related to this tendency not to monitor the automation system:
experience shows that human operators who become reliant on the
automation to perform a task are ill-equipped to take that task back
in a crisis situation, and task hand-over is likely to be
catastrophic \cite{AF447} \cite{Chowpaper}. A transportation systems
researcher I spoke to 
even cited this as the key reason he felt that highway-only
self-driving systems would not work, and that companies would have to
design for full automation everywhere.\footnote{Some researchers,
  however, do suggest the opposite: Larsson suggests a less perfect
  system might be better 
  by some metrics, as experience with it is empirically related to
  greater awareness of its weaknesses, and an expectation of having to
take control intermittently \cite{larsson}. Notably, this paper and
others like it are representative of human factors or ergonomics research.}

\subsection{Aircraft Automation}

This issue has so far played out most visibly in airplane cockpit
automation, and especially given the role that autopilots have as
models of ground vehicle automation, it is worth examining human
supervisory control in this practical
context. As we have found previously when examining automation
technologies, however, actual autopilot design and use differs 
significantly from its popular representation. It is not a fully automated
system that controls the entire aircraft, but a set of specific tools
leveraged by pilots to increase their agency, similarly to how
geologists use the automated capabilities of the Mars rovers to
increase theirs. Autopilots are also complex systems, with many modes
that require deep technical knowledge and training to use
appropriately \cite{harrisPsych}---and which, when poorly designed from
a human factors 
angle, as they have often been, can behave in ways that are unexpected
by the pilots and antithetical to their wishes. Stories of crashes
due to autopilot mode confusion abound, but one of the most important
incidents to consider in our context is the Air France Flight 447 crash. AF
447 took off from Rio de Janiero, Brazil on its way to Paris, crashing
over the Atlantic Ocean at 02:14 UTC on June 1, 2009. The official
report, published in 2012, identifies the cause of the accident as a
failed hand-over of control from the autopilot system to the pilots:
when the autopilot system shut down due to a failure of its airspeed
indicators, the pilots were not prepared to so suddenly resume the
task of flying a large airliner in poor conditions \cite{AF447}.
Seemingly confused by the situation they were in, the pilot and
co-pilot provided contradictory control inputs to the aircraft and
stalled the plane, causing it to crash. 

%cite removed
%airflow required electronic solutions\cite[p. 33]{DM}

This crash brings a number of human supervisory control questions to
the fore. What is the proper role of pilots? And how quickly can they
be expected to take back control authority over particular aspects of
aircraft operation that have been previously delegated to an automated
system? Are human pilots so unreliable that we should immediately push
toward automating all aircraft operations? Or are autopilots so
finicky that we should emphasize manual flying skills and reduce or
eliminate the use of autopilots. Most likely, the correct answer is
somewhere in between. But this concern with cockpit automation is not
new, and pilots and aircraft designers have been negotiating human
roles in flying machines since the very early days of aviation. The
tension between stable and unstable aircraft design---will the
aircraft ``fly itself'' in a given orientation or does it require
constant control inputs to maintain course---goes back as far as the
Wright Flyer, as do the competing professional identities that
accompany them: are you simply a chauffeur, or a ``true
airman'' \cite[p. 21]{DM}? The Wright's focus on the operator's
skill created the unique profession of the
``pilot,'' and unstable aircraft were the standard for years, until
human fatigue over long periods of flight changed the importance of
stability to aircraft operation \cite[p. 22-24]{DM}. Stability,
this subsequent virtue of a well-engineered airframe, had to be again
renegotiated in the context of supersonic flight, where stability
problems related to supersonic airflow required electronic
solutions. But there was deep concern among the
flying community about black boxes that are out of the pilot's
control, and some saw these devices as ``band-aids'' for engineers not
``skilled'' enough to make the aircraft stable:  J.
O. Roberts' ``The Case Against Automation in Manned Fighter Aircraft''
argued for more emphasis on displaying information to pilots, rather
than using automation to assume control \cite[p. 35]{DM}.
Nevertheless, cockpit automation has continued to increase. Today,
many airlines 
have guidelines that require automation systems to be used
whenever possible \cite[p. 38]{PARCCAST}. 

This airborne experience with stability and automation parallels some developments
in the automotive space: deep controversies exist among automotive
enthusiasts about the proper roles of all-wheel-drive, traction
control, and automatic transmissions in the driving process. Even ABS,
generally accepted today as a positive technology that enhances safety and
performance, is in some senses controversial; this skepticism is
motivated by the idea that 
a professional driver utilizing fully manual ``threshold braking,''
developed through deep experience and human skill, can outperform the
computerized system.\footnote{There are innumerable forum posts on
  this topic scattered around automotive forums, and arguments over
  physical principles and the capabilities of the technology in
  different road conditions are
  intertwined with issues of masculinity and implicit driving skill.}
Human and automated capabilities take on a special role in
  high-performance racing, which, despite the technology, puts the
  human being at the center. Modern racing cars depend on
much accumulated engineering expertise, but racing is as much about
the drivers as the cars and so inventiveness is often tempered by
rules that disallow certain technologies:  ABS and traction control are
banned from Formula 1 racing, for example. As Carlos Martinez-Vela
describes, regarding NASCAR: ``as important as engineering science has
become . . . given
that every team runs virtually the same technology, what makes a
difference in performance is the 'human element.''' The human is the ``only data acquisition
system'' allowed at the track, and the abilities of human beings to
sense and describe performance characteristics, and to work together
as a team, are paramount in this community \cite[p.
  178]{martinezvela}. New consumer performance vehicles---like the 
highly computerized Nissan GT-R, with a plethora of all-wheel-drive
systems---are criticized by some as ``too easy to drive quickly'' \cite{edmunds}.
An overly computerized vehicle is to some soulless, unexciting, too much like
a video game: these vehicles may derisively be said to be for the
PlayStation generation, even while others argue for the superior
abilities that advanced technologies convey upon human
drivers,\footnote{This community has stumbled upon the point of
  human supervisory control, and a rudimentary sort of cyborg
  identity, perhaps without even realizing it. Rather than holding the
machine or the human as prime, the human gains through their
interaction with the technology.}
turning an average driver into a hero or track-star.

% can cite http://www.ctvnews.ca/autos/nissan-s-new-gt-r-model-for-2014-is-this-a-supercar-for-the-playstation-generation-1.1550159

Computerized aids are sites of contradictory feelings and pressures.
Cruise control, when first introduced commercially as Chrysler's ``Auto-Pilot,''
was described as ``faintly
ominious'' by \emph{Popular Science}, but nevertheless seemed like a
``genuine help'' for reducing
fatigue \cite{rowsomePopsci}.
It is well enshrined within legal principles that drivers using
cruise control and even automated driver aids are legally responsible
for controlling their vehicles at all times, and yet these same aids
may reduce attention and ability to react in an emergency
situation---the other side of reducing fatigue.
Traction and stability control are now required for all cars in the US
market \cite{brookingsLiability}, while drivers' identities still modulate the
extent to which such
advancements are seen as valuable, or how often the features get
turned off in day-to-day use. 

While the AF 447 example shows the
perils of human interaction with automated systems, an issue which will
affect automated vehicles as well, its significance
within aircraft development does not seem to be broadly appreciated
outside of that field. It gets taken---including by one of the
automated vehicle researchers I interviewed---as meaning that human
interaction should \emph{never} be expected, and that until automated
vehicles can be fully self-driving in all circumstances, they will not
be sufficiently safe. The risks of a hand-over of control at high
speed would negate, by this reasoning, the benefits of computer
control on the highway. But aircraft companies seem to be learning a
different lesson from the same incidents. Boeing and Airbus, the two
most prominent manufacturers of commercial airliners, are distancing
themselves from complete automation, while increasing the computerization of
their cockpits through digital displays and tools to assist pilots in
the task of flying \cite{787dream} \cite{brownFuture}. The focus for
future development is on adaptive or adaptible automation
rather than complete computer control. Both types of systems allow for
balancing the operator's load---adaptive systems automatically,
adaptible systems by operator request---taking on tasks during
high-stress, busy situations, but handing back tasks during periods of
limited load in order to keep the operator informed of and engaged in
the operation of the system \cite{dahai}. Such systems have their own
engineering challenges, but represent a very different answer to the
question ``what should the role of pilots be?'' than does the further
complete automation of aircraft operations \cite{kaber}. Sustained human engagement
with highly automated systems is possible, but has to be designed into
the system from the beginning. Asking people to be mere machine
tenders, present only to ensure the continued operation of the
machinery, is indeed untenable, as it produces boredom,
inattention, and the risk of catastrophic failures like the AF 447
crash. However, joint human-machine systems research provides avenues
to engage operators in the operation of the system in ways that
increase overall system safety, without necessitating that the human
be entirely eliminated. There is a deep history of
work in human supervisory control (HSC), which I keep alluding to,
that has important implications 
for the real-world design of automated systems, and which tells a
potentially very different story about what automated vehicle
operation will look like from a ``driver's'' or operator's point of view.

\section{Conclusion}

This historical journey has attempted
to shed light on some of the unspoken assumptions behind autonomous
vehicles, assumptions that shape the popular narrative toward
particular visions of automation and away from others. Now, however,
let us turn to consider the stakes of the 
visions we have investigated. Why do these kinds of narratives and
assumptions matter? And what 
reasons might we have for investigating alternatives?

%%CONCLUDE


%% 3) (or worked in to each of the previous) current research
%%--not historical, but actual last-20-years papers about this
%%--examples of where ideologies bleed through!!!!
