\chapter{The Stakes of our Stories}
\label{chap:3}

%% 3. Stakes of stories/tech (swap 4?)
%% -data collection
%% -mapping
%% -ML/machine vision
%% -functionalism vs. Understanding
%% -statistics and risk
%% -planning, policy, cities



%% >traffic-light detection and problems of infrastructure??


%% 2.5??) Worth bringing in questions of terminology, ``intelligence''
%% etc. and the necessary caution in ascribing them to tech; we can look
%% and see that the ``intelligence'' in action here is really very
%% different than what we usually mean when we use that term
%% (human-centric)
%% --so does knowing this reality change what we want to see? (couple of
%% p)


%% Note: SOMEWHERE you should talk about the double-speak; freeing up
%% the roads for car lovers . . . doesn't make sense (chap3 handles it)


%EDITS
%Starr \& Bowker, standards? resonant??

We find ourselves in an era of seemingly continual technological
change that is strangely most noticeable in the most mundane parts of
our lives---how we shop, how we communicate, how we find
partners---almost as if ``designed by a bored researcher who kept one
thumb permanently on the fast-forward button'' \cite[p.
  7]{Neuromancer1984}. No longer does the future seem to be
defined by flying cars and jetpacks. Instead, it is defined by
information, and its collection and use. The stakes of autonomous
vehicles are thereby deeply intertwined with the
stakes of other networked information technologies. And it is in these
current models that we should start the search for why competing
narratives of self-driving car development matter. To illustrate these
issues, I will
take as assumed the general form of the autonomous vehicle as present
in Google's development work: though numerous other models exist, they
are the subject of another section. Here, we seek to take this design
to its logical conclusion and interrogate the numerous social,
cultural, and informational stakes implicit in it.


\subsection{Data Gathering and Monitoring} 

 In May of 2014, the European
Union's Court of Justice 
ruled in the landmark Costeja decision that since Google is processing
``personal data,'' and acting as a ``data controller,'' it may be
compelled to remove links to pages containing personal information
from its search results \cite{ICO}. The EU ruling, in deference to European
tradition and contrary to that of the United States, places an
individual's rights to privacy above the ability of users to access
information online.\footnote{Specifically, ``whilst it is true that the
data subject’s rights also override, as a general rule, that interest
of internet users, this balance may however depend, in specific cases,
on the nature of the information in question and its sensitivity for
the data subject’s private life and on the interest of the public in
having that information'' \cite{COJCosteja}.} There is growing recognition that publicly
available data can be highly sensitive and that it may be beneficial
to allow individuals certain legal rights to control their own
electronic reputations, at least in particular circumstances. Costeja
opens the floodgates---from previously limited, targeted removals
through court cases and copyright law---to the possibility of free and
open public removal of ``public'' information.\footnote{This removal,
  however, is controlled and curated by Google itself, which has not
  waited for guidance on how to proceed but has forged forward on its
  own, as a way to set the standards by which information removal can
  be justified \cite{powlesChaparro}.} But much of our
information is even harder to control.

Many current data-driven business models\footnote{See for example
  Google, which has as its fundamental revenue stream proceeds from
  advertising, which is sold by virtue of it being more accurate or
  targeted than other channels can provide. Facebook's new ad network
  competes in a similar space. Many of these companies provide
  services for ``free,'' where in reality the services are paid for by
the user in the data they generate and/or the advertisements they
view.} are fundamentally united in that increases in
functionality are predicated on invasions of or encroachments on what
we used to think was private, and represent increasingly invasive data
collection and sharing at a massive scale---often this information is
used internally to improve services, but it may also be aggregated and
sold to third parties, and in either event may be stolen or leaked by
disgruntled employees or thieves. This
is particularly important as a precursor of things to
come in the autonomous vehicle space, as such vehicles will allow
large amounts of data to be collected and shared with other entities. 

Clearly, new technologies will require new information, new types of
sensing, in order to operate; but it may be that not all the
information that is collected should be shared outside the immediate
context of its use. What is sensed, and what can be appropriately
transmitted back to servers for processing and storage, is of
paramount importance: this is a question of privacy in context \cite{nissenbaum}.
Following Nissenbaum, privacy must always be seen in the context of
particular users and a particular use \cite[p. 2]{nissenbaum}. It is not that data about our
commuting routes, for example, should never be collected, but that collected data
should not be sent uncritically to any third party without our
knowledge or consent, a violation of our information norms \cite[p.
  3]{nissenbaum}. 
Automated vehicles are and will continue to be networked
technologies. This connectedness brings with it great possibilities
for coordinating traffic and improving city planning, as well as great
risks to privacy and security, whether devices are networked with each
other or simply connected to central servers. There
may be legitimate uses for certain types of sensitive information, but
while providing it to municipal governments specifically to assist in
city planning may be legitimate, selling it to advertisers to help
them design more effective billboards may not be. Privacy issues
involving motor vehicles are likely to become much more complicated as
vehicles become able to record more, and potentially ``know'' more,
about their passengers.


%% 2.1) data gathering and mapping
%% --this approach presupposes large amounts of data collection (maps),
%% and opens the way to more information about the passengers (2 p)
%% --see Google's patents in particular which talk about sensing things
%% like the number of passengers (4 p)

With what networks, and for what reasons, will autonomous vehicles be
connected? Current driverless car concepts depend on networked
information for vehicle guidance. While, historically, certain
guidance systems have been insulated from communications---inertial
guidance systems for intercontinental ballistic
missiles are a particular example of this \cite{mackenzie}---and
Google's vehicles use inertial navigation devices \cite{knightFurther}
alongside other sources of position data, current navigation systems
depend on global positioning satellites. Google's approach to
driverless car development currently necessitates accurate
and expensive\footnote{An autonomous vehicle researcher at MIT
  quoted prices in the range of $70,000 to $100,000 per device for the
GPS alone, while noting that those would of course come down with
greater production volume.} differential GPS receivers.

But a number of other developments already suggest that autonomous
vehicles will be connected to more than just global positioning
networks. GM's OnStar service already connects equipped vehicles to central servers for
purposes of safety, security, and convenience. The system has the
ability to automatically alert the authorities in case of an accident
or theft. It also provides vehicle diagnostics to the owner's tablet
or smartphone, and the OnStar app also allows the owner to configure settings, lock and
unlock doors, and remotely operate the lights and horn \cite{onstar}.
While these vehicles are not (yet) autonomous, OnStar's present
capabilities are representative of features that will become more common in highly
connected and computerized vehicles, including autonomous ones.

Additionally, vehicles that can receive information from each other and from the
roadway are a top priority for the NHTSA, and have been on the
research agenda for decades \cite[p. 11]{wetmore}. These concepts would
collect and use data to streamline traffic flow and provide
information to reduce delays and accidents. But there is potential to
do far more with the available information,
and data collected by the vehicle to make possible its own functioning
could be made to serve other purposes. Information about the vehicle
and its surroundings, including the locations of cars and pedestrians,
precise GPS coordinates of the vehicle itself, and the vehicle's speed
and acceleration, not only represent important knowledge for
path-finding by the vehicle itself, but new sources of potential
revenue for the groups in position to collect them. Uber, which
through its GPS-enabled ride-hiring applications still collects only a
fraction of the data that would be available through a self-driving
vehicle, has agreed to share its ride data with municipalities for purposes of
city planning \cite{uberJardin}. Though this data is ostesibly being
shared for the public good, it also serves private ends: to curry favor with
authorities that might otherwise attempt to shut the service down. The
ride data can also be used to provide internal predictions to help
Uber increase revenue by directing drivers to the right locations at
the right times.\footnote{One potential reason for municipalities to
  collect such data is dynamic road pricing. This is not fundamentally
a bad idea if done in a way that is as non-discriminatory as possible,
and as one of my informants noted in an interview, may be 
the only reliable way to reduce congestion. However, putting another
corporate entity in between road users and the road is not necessarily
a good idea. It strikes me, since I am writing this shortly after the
FCC's reclassification of Internet service under common carriage laws,
that if we find ourselves in a battle over ``road neutrality'' in a
decade or two, it will have a decided air of irony to it. Pervasive
tracking and self-driving allow for all manner of road pricing
schemes, some of which will be unduly discriminatory.} And it would
not be far-fetched, in the current 
information landscape, to see such information sold to third-party advertisers.

% consider 1-2 sentences on risks of connected devices;
% http://blog.kaspersky.com/internet-of-crappy-things/

These sorts of security and privacy issues are not unique to
autonomous vehicles, nor even to networked vehicles. What Roger Clarke
calls ``dataveillance'' is already possible: electronic tolls already
allow for a measure of tracking \cite[p. 25]{nissenbaum}. Networked traffic cameras are
already being used to amass large databases of information about
(non-networked) cars and their travel patterns by reading passing
license plates \cite[p. 26]{nissenbaum}. With sufficient coverage of cities and regions,
these databases can capture the movements of large segments of the
population, and therefore include potentially sensitive information
about people's movements. Despite this, their collection and use
remains largely unregulated.\footnote{Groups such as the EFF and ACLU
  are attempting to do something about this \cite{kayyaliEFF}.} California police are coming under
criticism from civil rights groups about their mass collection of
vehicle position information through police license plate cameras.iv
Clearly, these types of contested data collection are already
possible. But networked vehicles, with their arrays of sensors,
provide more avenues of data collection, and therefore stand to
increase our present-day problems with mass surveillance and personal
privacy.

Increasing surveillance by government and law enforcement is not the
only threat posed by the potential for ever more fully networked
vehicles. Networked technologies do not have a great security record,
and there is no reason to believe automated cars will be any
different, barring greater scrutiny and
regulation.\footnote{Commercial networked devices routinely lack basic
security measures. And for a large number of other systems, security
is defeated by being out-of-date or by using default passwords that
were never changed. See for example \cite{zetter}.}
Networked capabilities within vehicles are being used to
implement vehicle kill-switches, capable of remotely disabling
vehicles on the whim of the controlling organization---for example, if
vehicle rental or loan payments are not completed on time. The
potential for this is quite troubling: not only could drivers
potentially find their cars impossible to start at an arbitrary time,
with serious implications for safety, the existence of these devices
presents opportunities for hackers to cause nuisance or
harm.\cite{goodman}
Existing vulnerabilities in web services and dealership practice
allowed Ramos-Lopez to hijack 100 cars from a Texas dealership and
either disable them or set
the horns
going \cite{poulsenHacker}. A
high-school student at a security camp in July of 2014 was apparently
able to hack into an automobile with \$15 worth of components, and
gain access to lights, horns, and even the remote start
feature \cite{bigelow14}.
The
potential for this kind of access gets more frightening when one
considers the effects an integrated, network-connected vehicle that is capable of
moving and navigating on its own might have on the reach of a hacker or negligent employee.

Additionally, Google has envisioned vehicles that can determine their number of
occupants, and use facial-recognition or other biometric systems to
identify them. According to one patent \cite{predictPatent}, these vehicles could prevent
unauthorized persons from putting a child in a car, prevent convicted
sex offenders from operating their vehicles within the
legally-required distances of schools and playgrounds, or prevent a
car's doors from being opened (even from the inside) by a child unless
an authorized adult is present. These are only visions, and patents
are notorious for trying to cover as many possible angles of a
technology even if they are not intended to be applied in practice.
But these suggestions represent a perspective on safety and societal
order that posits technological surveillance and enforcement as an
appropriate preventative measure against criminal behavior. Whether or not protecting
against these threats is an appropriate use of this information is a
matter for societal judgment, but such proposals, if enacted, would
require these vehicles to have unprecedented levels of very sensitive
knowledge about people and their lives: biometrics, criminal
histories, family and trust networks. 

Prevailing data ideology tells us that we can understand ourselves better through
these data, that we can use them to determine patterns we never knew were
there. Out of this ideology come publications like Uber's
blog\footnote{See blog.uber.com. One particularly noteworthy post,
  from March of 2012, was about customers' ``Rides of Glory''---in
  other words, one-night-stands. This post was later taken down by the
Uber team, but articles about it remain \cite{gigaom_harris}. Another
post about the connection between Uber rides and areas of
crime---notably prostitution---was also removed \cite{venturebeat_obrien}.}, which
describes customer ``insights'' gained through their ride data, which
are ostensibly interesting to the public. ``Look here,'' they seem to
say, ``we can tell you about \emph{you}.'' But it important to
recognize that the ideologies of data collection---for corporate
profit and public use---are deeply intertwined, and strongly
influenced by the possibility of using machine learning techniques and
statistical analyses to find and exploit patterns. Automated vehicles
are part of this general culture, and without significant effort to
resist erosions of privacy and data protection stand to open our lives
to greater scrutiny in terms of the means of and reasons for our
personal mobility.

%%More on the problems of data ideology? or not the place for it?


\subsection{Maps and Mapping}

But the information that may be collected and processed by automated
vehicles is not only about human inhabitants of the environment: other
types of data collection are also implicated in current
visions of the driverless car project. In order to drive with us,
autonomous systems will have to understand,
for at least a practical sense of ``understanding,'' traffic rules and
their accompanying signs, signals, lanes, and customs. This is, at its
core, a highly complex problem of interpretation and representation
for machines. Human understanding is built
through years of experience: it is through existing as a human being
in a particular cultural context that we know to drive on roads but
not on sidewalks, and how to tell the difference. But we do not have
this luxury in training machines.

Localization has been a fundamental issue for robotics since the
beginning of the field. In order to figure out how to act, a robot
needs to know where it is; and in general, robotic environments can be
expected to change rapidly with motion, and to vary significantly from
place-to-place \cite[p. 4]{SLAMbook}. Motions cannot, for the most part,
be pre-planned:  they must continually be reevaluated in new and
emerging situations, as the robot moves through the environment. As a
key part of this puzzle the robot must be able to determine its own
relationship to some target location it needs to get to: this
``target-robot relation'' is an ``unavoidable''\footnote{It is
  possible to create robots that navigate without any modeling of the
environment, as Fern\'{a}ndez-Madrigal et al. point out. However, this
severely limits the range of behavior that is possible. Without
reference to the environment, errors slowly accrue, and a system that
navigates in a known space without localization will find itself
getting further and further off-track.} component of successful
navigation \cite[p. 5]{SLAMbook}. This relation can be expressed
in different forms---either quantitative representations such as maps,
or logical, prepositional statements---and these representations carry
their own techniques of interpretation. 

But there is more to navigation than pure localization. While
localization is the usage of known elements in the environment to
estimate a robot's position, those elements themselves must be known
in order to perform this process. Mapping is the complementary
process, the estimation of ``\emph{unknown} spatial relations that
exist between environment elements'' in order to allow for subsequent
navigation \cite[p. 5]{SLAMbook}. There is a somewhat tricky
precedence problem we face here:  in order to build maps with
autonomous systems, they must know where they are; in order to know
where they are, they must have maps with which to measure
against \cite[p. 6]{SLAMbook}.

Returning to the history of AI,
localization was part of the cause of the poor performance of the
robot Shakey: world modeling through a symbolic approach was slow,
which was rendered even more problematic by the relative lack of
computing power \cite[Afterword]{mccorduck}.
Though one way to determine robot position in the world is through
simultaneous localization and mapping (SLAM)---a serious research area in
robotics, as it is ideal for areas that are impractical to map
beforehand, such as ones that are always changing---it is easier to localize by comparing
measurements to a known map. However, that implies that the map is
pre-made, and therefore sets limits on the rate at which the
environment can change and still allow the robot to operate. By
contrast, the subsumption architecture involved an
attempt to avoid the localization problem by pure sensory response to
the environment, but building a robot that exhibits complex and
predictable behaviors in complex environments, through this approach,
is exceedingly difficult, as evidenced by Brooks's shift to hybrid
modeling approaches. But new AI techniques do allow researchers to
take advantage of new kinds of data \cite{???}. Today, localization and
mapping are both considered ``satisfactorily solved in practical
situations''---though SLAM techniques
are still somewhat less reliable or developed---given sufficient
computational resources and environmental data \cite[p.
  5-6]{SLAMbook}.

%%Fernández-Madrigal, Juan-Antonio, and José Luis Blanco Claraco.
%%2013. Simultaneous Localization and Mapping for Mobile Robots :
%%Introduction and Methods. Hershey, Pa: IGI Global, 2013. eBook
%%Collection (EBSCOhost), EBSCOhost (accessed February 21, 2015).

Here, the connections of autonomous systems to information networks
again become important. The vehicles in the
DARPA Grand Challenge did not navigate ``on their own'': they used GPS
to follow a path laid out for them in advance, using their autonomy
only to avoid obstacles like rocks and ruts.\footnote{John Leonard,
  discussion with the author, December 3, 2014} And though successful
road tests have been accomplished without navigational assistance,
using only visual stimuli (such as the EUREKA PROMETHEUS project
in the 1990s, which navigated only with cameras and therefore could
only follow the road in an automated fashion, rather than navigate to
given GPS coordinates) \cite{UlmerVITA}, modern systems are tending to use more external
stimuli, rather than less, in an attempt to increase safety and
vehicle capability. Even as
advanced as it is, Google's autonomous vehicle technology requires
hyper-detailed 3D maps in order to operate properly on public
roadways \cite{gomesObstacles}. These maps are generated by human-piloted vehicles outfitted with
special sensor arrays, like the LIDAR Google uses for their autonomous
vehicles, which drive a route and collect data which can be used to
reconstruct the model used for future drives.\footnote{Nhai Cao
  (Global Product Line Manager at TomTom), presentation at The Road
  Ahead Forum on Future Cities 2014, Cambridge, MA, MIT, November 21, 2014.} 

The vehicle does not operate in a SLAM mode, at least not completely.
Pre-made maps are used so the vehicle knows where stoplights, signs,
and curbs are, reducing the computational load on the machine in the
crowded visual landscape of driving, and allowing it to focus on
elements of the environment that are changing rather than those likely
to be static \cite{gomesObstacles}. Prior knowledge of speed limits should make the car's
behavior more reliable and predictable in all conditions, even if
speed limit signs are missing or obscured---consider how often human
drivers, when faced with an absence of signs, base their behaviors on
supposition or prior knowledge. When Google's car was certified for
testing in Nevada, Google was allowed to pre-select the route the car
would take, so that they could build the comprehensive model the
system requires beforehand \cite{harrisNevada}. The system would likely not have been
capable of passing a test in which the examiner could have added
detours on the fly. And though Google claims to have driven more than
700,000 miles with their cars, those are not 700,000 unique miles. A
limited, thoroughly pre-mapped route has been driven many times to
achieve those numbers \cite{gomesCircles}.


%%TODO Karen Kaplan
Mapping claims a unique capability to
represent the real, objectively
and diagrammatically, but also requires that world to remain largely
static, at least on the order of how long it takes to update the map
for a particular region. The necessary level of continual mapping is a massive task if the
vehicles must be usable everywhere. The United States alone contains almost
8.5 million road miles\footnote{Data as of 2008 \cite{carneyMiles}},
and it took years for Google Streetview to acquire the level of
coverage it currently has. The maps required for driverless car
localization are a significantly more difficult project in terms of
amount of data, reliability of data, and therefore frequency of
updates. The utopian discourse of driverless cars
implicitly suggests that such vehicles will be available everywhere,
and are the solution to nationwide transit problems. But widespread
egalitarian access to maps-based devices depends upon a rapid,
widespread mapping initiative. The success of such an initiative is
dependent not only on a large amount of human effort and capital
investment, but a series of decisions about what regions should take priority.

TomTom\cite{tomtommaps} and Nokia\cite{ubergizmo} both claim to be attempting such
a mapping project,
but it will not happen immediately or all-at-once. More likely,
certain areas will be 
mapped and restricted, or separate divided public rapid transit
systems will operate on roads that can be carefully
monitored.\footnote{This point came up in a discussion with a
  scientist with experience with a UK-based firm working on public
  rapid transit, or PRT, systems.} While
Mountain View, California may be mapped early, rural West Virginia or
Northern Maine may not be mapped as soon or as frequently.
Inequalities may be increased if routes frequented by upper-middle
class professional commuters, most likely to own new autonomous cars,
are mapped first, while roads around low-income communities are
ignored. Such decisions are easy to imagine being justified by
commercial exigencies---limited funds, potential markets---but would
cut directly against the utopian narratives of driverless cars.

Furthermore, the seemingly universalizing forces of maps and computer programming
have a tendency to hide issues of geographical and cultural
specificity, which are rendered invisible in these utopian narrative.
But though engineering practice holds that these issues are
conquerable, they should be anything but invisible. More than
abstract problems of localization and mapping, automated vehicles
present the problem of having to exist in an environment that is
highly complicated, varied, and cultured. Maps alone are insufficient
for anything but the most simplistic view of vehicle operations.
Programmed devices
must must know about speed limits, about traffic
lights, about rules of the road that were never designed for
autonomous systems. These devices must respond to human caprices and
be adapted to longstanding, ingrained laws and habits. They must
include historical knowledge, rooted in the legal and social histories
of roadways, which may differ between cities and states, and certainly
between countries across the world. Local customs and behaviors differ, and even if maps are
available, the same vehicle programming may not work for Los Angeles,
Boston, and the rural Midwest, let alone Singapore, Mumbai or Cairo.
The map, for all of its objective standardization, still represents
real places subject to cultural histories and vulnerable to
socio-economic dynamics. These social and regional issues are often
ignored in the driverless vehicle narrative, but nevertheless stand to
be critical to the manner in which these technologies could come to
enter everyday life.



\subsection{Machine Vision}
%% 2.2) machine vision
%% --pull from the Spectator paper (4 p)
%% INCL. machine learning
For all this, however, vehicle localization and sensing depends upon
visual interpretation of the vehicle's surroundings. Interpretation of
the world around us is a task that seems
particularly easy for human beings, but particularly difficult for
machines. The invention of the photocell, early a tool for workplace
monitoring and surveillance, provided a simple channel through which
electrical systems could respond to the amount of light reaching
them\cite[p. 44]{faxed} \cite[p. 361]{nyeElectrifying}. Though the photocell can easily
provide a computer system 
with access to brightness information over time, perceiving detail and
depth, identifying shapes, and interpreting expression and motion are
all capabilities of human vision that require more sophisticated
technologies to reproduce. DARPA's 1983 Strategic Computing Initiative
included image interpretation as one of its main
focus areas \cite[Afterword]{mccorduck}. But it is only relatively recently that real-time video
processing, needed for camera-based navigation, became feasible for computer
systems small enough to fit in a standard automobile \cite{vamors-p}. And machine
vision problems, including object recognition and scene
interpretation, continue to be difficult, even with increased
processing power and new algorithms. 

As an engineering discipline, computer vision takes a decidedly
practical and reductionist view of what it means to see. The goal is
generally not to achieve creative interpretation or aesthetic
valuation, but to identify particular objects in a scene, to recognize
faces, or to differentiate free space from things a robot should
not run into\footnote{SRI's Shakey robot, for example, made navigation
  plans via rudimentary computer vision tools, specifically sonar range-finding.}. But this so-called objective focus still encodes
certain subjective judgments about objects (including people),
behaviors, and intent. Consider, for instance, the issue of object
detection. While it may be a objective question whether or not a
particular object (e.g. a wrench) is physically present and visible in
a scene, it is not
necessarily so self-evident which objects are noteworthy or important
to detect (e.g. does the worm gear on an adjustable wrench count as a
separate object in addition
to the wrench itself)---these choices depend on applications, and the judgments of
designers as to what is worth measuring. And while computer vision is having success
with object detection, there is a wide variety of human knowledge
about objects and scenes that is missing in current computer
models, including propositional understandings (``what would happen if
. . ?''), projections about occlusions (``what is behind that?''), and
connections to other sensory modes (``what would that feel like if I
touched it?'') \cite{gomesJordan}.

Regardless, though vision has not always been the sensory mode that dominated
autonomous vehicle research, it is a particularly attractive sense to
use as it is integral to how humans drive. In an attempt to build
autonomous vehicles that can operate without infrastructural changes,
research has moved away from automated systems controlled and
constrained via tracks and cables toward vision-guided
systems. New approaches were pioneered by Ernst Dickmanns at
University Bundswerhr in Munich, with the vision-guided VaMoRs van,
and continued via the EUREKA PROMETHEUS project in 1987, in which
Dickmanns and Daimler-Benz built cars guided by analog video
cameras \cite{ulmerVITA-II}. Like the earlier VITA project by Daimler that used an
analog video-camera signal processed through a framegrabber, these
cars (VITA-II and VaMoRs-P) digitized analog video at relatively low resolutions. The
features the systems searched for, including lane markings and other
cars, are geometrically distinct and visible even in small
images \cite{ulmerVITA}. That these vehicles were designed primarily for
highway operation under constant human supervision made this strategy
acceptable, since pedestrian interactions were likely to be
rare.\footnote{The documentation of the VITA-II mentions pedestrian
  interaction management as future work necessary for changing the
  working domain from the highway to urban and country environments \cite{ulmerVITA-II}.}

%%Can cite SivakSchoettle p. 3 on weather; and they cite Lavrinc 2014 there

The apparent primacy of computer vision, however, only holds for fair
weather. Rain, sleet, and 
snow interfere with vision-guided systems, and currently prevent them
from operating safely, as numerous more skeptical news articles are
happy to note \cite{knightFurther} \cite{gomesObstacles}.  While the
human eye is generally very good at
pattern recognition and filtering out noise, getting computers to
appropriately recognize all 
the necessary road objects already tests
cutting-edge techniques. Precipitation not only makes roads slippery
and less safe for all drivers, it reduces visibility---which decreases
the distances at which objects can be detected, and therefore also the
time the system has to react---as well as potentially decreasing image
contrast and presenting interference to the image---through drops or
streaks on the glass through which images are taken
\cite{rainADAS}. These issues also 
affect people (though our brains can filter out raindrops on the
windshield rather than interpreting them as obstacles), but human
drivers would, at least ideally, choose to drive more slowly given
adverse conditions. One emerging research area
in automated vehicles is raindrop detection for automated driver
assistance systems, in order to help such systems compensate for
reduced visibility, or alter their behavior to be more conservative in
these situations. However, ``developing algorithms that work perfectly
under all weather conditions appears to be unrealistic,'' so such a
modular and cautious approach may well be necessary in order to build
robust systems \cite[p. 50]{rainADAS}. While commercial use of
vision-guided self-driving vehicles on 
sunny days may indeed be possible in the near future, use in
less-than-ideal conditions is likely to be limited for some time,
while human oversight compensates for algorithmic deficiencies. 

Vision-guided systems, now using digital video cameras and
off-the-shelf consumer hardware, do have the benefit of being inexpensive
and insensitive to interference from other nearly devices (unlike
sonar, for example, which becomes problematic in crowded
situations\footnote{John Leonard, discussion with the author, December
3, 2014.}). Some contemporary commercial systems, such as that developed for Mercedes-Benz's
self-driving S-class, which is slowly finding its way into consumer
vehicles, are primarily guided based on such visual sensors \cite{makingBertha}. To
these sensors, recent research has added roof-mounted LIDAR arrays.
LIDAR, short for Light Detection and Ranging, is a distance
sensor, which is applied in vehicles to scan the environment with a
rotating array of laser beams to create a detailed 360-degree
representation of objects and their distances. This technology
solves some of the difficulties of image interpretation by default, as
it can provide highly-sensitive information about free space and
obstacles. Shape-detection algorithms can then be used (in addition to
vision-based data) to classify obstacles as different types of
objects: pedestrians, bicyclists, cars, and trucks\footnote{This also
  works for other kinds of physical objects. See for example the
  detection work done by \cite{fukuda}}.

%% DONE?
%% --Göde Both and reluctance to use Machine Learning (1 p)
%% ---brittle, unpredictable, difficult to ``tune''
%% --Sensors issues and inability to distinguish crushed newspaper from
%% rock (1 p)
%% ---requires a lot of other information, more than simple physical
%% volume
%% ---object identification work in machine vision is slowly progressing,
%% but then we hit the above problems of VV\&T for such models (2 p)

While much computer vision research uses machine-learning algorithms to detect
objects, engineers in automotive applications are justifiably
reluctant to rely on machine-learning: as Göde Both has noted in his
ethnographic research on developers of driverless cars in
Europe \cite{bothpt1},
machine learning techniques are brittle and unpredictable \cite{bothpt2}: neither
characteristic makes them suited for software that must be highly
reliable and on which people's lives literally depend. So mixes of
manual and machine-learning approaches are used to do object
detection. Though machine-learning can be a highly-effective
technique, it is generally difficult or impossible to know what the
system has actually ``learned'' and therefore how it will react in new
and unknown situations \cite{bothpt2}. Pedestrian detection algorithms search for
person-like shapes, where
``person-like'' is determined by, for example, training a classifier
using thousands of images previously labeled (by people) as being images of humans, so
the system can learn the features that correlate with a person being
in a particular region of an image. In a sense, the computer can be
said to develop
a ``concept'' of a person. So long as the right features appear in each
new situation, this approach works; but what the computer has
``learned'' is essentially black-boxed, and resists introspection.
Even discounting these concerns, shape-detection is not a complete
solution, as the knowledge it provides about objects is only
skin-deep: the sensors cannot differentiate a rock from a crumpled
newspaper, and Google's car will serve to avoid
both \cite{gomesCircles}. Further distinguishing objects requires a lot
of information besides position or simple physical volume,
including fine-grained information about the object's surface
appearance, and interpretation of physical properties from observable
behavior (e.g. bouncing or rolling). None of this is fundamentally
impossible, but it presents necessary areas for research.

Detected categories allow the system to make
statistical predictions about likely types of behavior: according to
one of Google's patent applications, bicyclists are likely to be more
erratic than trucks, and should be treated
accordingly \cite{predictPatent}. These sorts of predictions are
something that human drivers do consistently, and are therefore also
likely important to how autonomous vehicles may drive.\footnote{The
  DARPA Urban Challenge crash, the first crash between two autonomous
  cars, provides an important lesson on the vagaries of object
  detection: the classification threshold between moving and
  stationary, set too high, allowed one vehicle to interpret the other
  as stationary, leaving no room for unexpected behavior \cite{collisionPaper}.}
Categories alone, even if achieved, are also insufficient if detected
objects are treated solely as obstacles to be avoided. One notable
researcher who I spoke to drives around searching 
for, and attempting to photograph, situations he believes will cause
trouble for current vehicles. His collection includes many situations with police
directing traffic, especially when combined with sun glare or
occlusions of the sight lines---such as by a mail relay box---that
would make predictions regarding oncoming 
objects difficult. His take-away from these situations is that there
is a significant body of edge-cases that make full autonomy
impractical for the forseeable future. Instead, humans will be
required to account for these sorts of difficult perceptual situations. 

%%TODO
%%<Include quotes from the interview>

However, because autonomous cars see---putatively ``as we see''---their sight can
be leveraged as visual evidence of their operation. Computer vision
systems that identify 
pedestrians can be shown to do so, via the detection boxes that act as
diagnostic tools for researchers and direct representations of
internal system information.\footnote{See for example Volvo's ad for
  the S60's pedestrian detection capabilities \cite{volvovideo}.} The new technologies of vehicle
automation thereby produce through 
their operation new forms of evidence, which can be presented through
electronic information media. We can point to these images and identify
that the vehicle is operating as it should. The box inscribed around
the pedestrian shows this plainly. The 3D environment scan from the
LIDAR system presents the same opportunities for ``transparent'' visual
proof. Three-dimensional shapes, in standing in stark relief against
the background, bear witness to the sensory operation of the vehicle.
These shapes too are demarcated by boxes, which represent their
computational transformation from information into an object or
artifact of interest. Because the visual detection technologies used
by autonomous vehicles are compatible with the visual technologies of
media representation, new types of seeing are opened to us.

%%TODO Kelly Joyce on fMRI: Magnetic Appeal: MRI and the Myth of Transparency
A coincidence of sensing and representation in the evocative and
powerfully persuasive medium of visual representation stands to shift
the way we perceive driverless systems in their operation, providing
us a different manner of insight and introspection, but also
potentially a different level of obfuscation, than ever before. When
considering how these vehicles are presented to us---and the fervor with
which some researchers demand that we accept them\footnote{For
  example, technical speakers at the The Road Ahead conference at MIT,
hosted by the SENSEable Cities lab (November 21, 2014) suggested
concerned users ``get out of the way'' of coming fully autonomous
technology.}---it is important to 
remember the many black boxes of information processing behind the
naturalized image of the sensor readout. The rhetoric of the
self-evident visual image, as it operates in the autonomous vehicle
space, can be made more visible by considering contemporary medical
imaging. As researchers have noted about the prevalence of brain
images from fMRI studies in the popular press, these depictions have
strong rhetorical impact on discourse about neuroscience. Colorful
pictures of the brain, as the inheritors of Enlightenment notions of
visual evidence par excellence, have a way of convincing the viewer of
the validity of reseachers' claims about physical brain locations and
their effects \cite{lehrerNeuro}, especially since those pictures are presented as the
direct, transparent products of sensing techniques \cite[p.
  76]{kellyMagnetic}. The conflation of 
human and machine vision which makes these images so confusing is as
misleading in vehicle navigation as it is in neuroscience.\footnote{In
her book \emph{Magnetic Appeal}, Joyce argues that ``seeing does not
equal truth or unmediated access to the human body,'' but that
practices equating these are so common that images are often used to
stand for such, despite the doctors involved being well aware of how
institutions and social practices shape these kinds of evidence
\cite[p. 76]{kellyMagnetic}. But she also notes that
popular narratives are particularly bad at heeding the sorts of local
practices that shape technological and image-based knowledge, falling
prey to the ``myth of photographic truth'' \cite[p.
  75]{kellyMagnetic}. These tendencies seem to be of great relevance
when considering other complex, technological projects dependent on
imaging and which use images rhetorically, to stand for the ``truth''
of their ability to perform a task---such as detect a pedestrian in a
crosswalk.} Significant 
statistical processing is necessary to make sense of data in either
realm, processing that is generally beyond the public's gaze \cite{koerthFish} but the
end result leaves the viewer with a false sense that something real
has been sensed, revealed, and affirmed by the image.


%% 2.2) functionalism over understanding
%% --for better or worse these devices are not humanlike in their
%% understanding
%% ----AI winters & the rise of commercial ML AI with much smaller goals
%% (see Wired articles, etc) (3 p)
%% --the ideology that drives them is an engineering mindset: ``just make
%% it work'' (2 p)
%% --pulling the quote from my USC talk: also interesting to note
%% regarding 1.2 that it seems to expect a humanoid robotic chauffeur in
%% a normal car! (1 p)
%% -and of course interviews with people (Ryan Chin; Walker Smith etc.
%% where they matter to this type of vehicle)
\subsection{Functionalism, Utilitarianism, Ethics} 
We should likewise not be deceived by an apparent parallelism of human
and machine knowledge, or an elision of the difference between human
and machine ``cognition.'' Self-driving cars will not be human-like in
understanding, even while they can detect and identify pedestrians as
objects of interest within a particular epistemological frame. Whether
or not their machinic perspective is to be lauded is a deep and
philosophical question, but the robots envisioned by current AI
ventures bear little resemblance to those of Asimov, or the dreams that grew
from the Dartmouth Conference. While a wide range of software companies and
startups have entered the AI 
industry, these companies are not primarily focused on
general-purpose AI.\footnote{The primary divisions responsible for two
representative Google projects, Chauffeur and Deep Mind, are
geographically and organizationally separated. A Google Deep Mind
employee I spoke to at MIT said that they had had only very limited
contact with the Chauffeur team. September 10, 2014.} Though certain
ventures still hold out the dream 
of doing so, several AI winters have shown that the creation of
general intelligence is very difficult, and is by no means around the
corner. Those working in the field are well aware of
this \cite{sofgeAIFears}. So much current work is
fundamentally utilitarian, building systems with clear goals, metrics
for success, and market segments.

Though computer science and philosophies of AI have been using
``intelligence,'' ``knowledge,'' and ``understanding,'' among other
words, to talk about computers since the beginning of the field, these
uses should not be taken at face value. ``Intelligence'' is slippery,
and its definition is not constant over time. It is difficult to
define intelligence in ourselves \cite{huntIntelligence} and yet another thing to define it
in relation to other entities. Weaving was once considered to be a
peculiarly human capability, a sign of an advanced, intelligent
mind \cite[p. 627]{riskinDuck}. But after Vaucanson's loom allowed mechanical devices to weave
seemingly on their own, this capacity was no longer seen as uniquely
human, and was no longer a marker of intelligence. The same process
occurred with chess in the 1990s. When IBM's Deep Blue beat Gary
Kasparov, chess ceased to be the standard by which intelligence could
be judged, precisely because it had been achieved. Real intelligence
had to lie elsewhere: for example, in the game Go, mastery of which
has continued to elude machines.\footnote{``When IBM's Deep Blue beat
  Gary Kasparov in 1997, most Artificial Intelligence researchers and
  commentators decided that chess playing did not require intelligence
  after all and declared a new standard, the ability to play Go'' \cite[p. 623]{riskinDuck}.}

Nevertheless, machines manage to do things that seem ``intelligent.'' So
though these terms are heuristics for understanding the observed
behaviors of machines, they slip slowly over time from self-conscious
scare-quoted use into casually accepted statements. While automatic
translation may seem ``intelligent,'' or a system that can define \emph{\'{e}toile}
as ``star'' may seem to possess ``knowledge,'' this intelligence or
knowledge is perhaps very different than our own. A deep
epistemological question presents itself: how do we know, and how do
machines ``know''? Many AI systems operate via statistical pattern
recognition, so we may ask whether we believe human intelligence is
also merely pattern recognition: does a system that can associate star
with its definition really know what a star is? Is mere linguistic
association sufficient for knowledge?

H. R. Ekbia and others remind us that we should be skeptical of the
applications of these terms to computational processes \cite{ekbia}. As we have
seen, Searle's thought experiment of the ``Chinese room''
argues that symbol processing and pattern recognition alone is not
intelligence, though from the outside the results may appear to be
intelligent. Despite the faults of his argument, his caution
about ascribing overly ambitious human ideas to computational
processes is warranted. Even Pamela McCorduck, a colleague of several
notable AI researchers and a believer in the field in general, hedges
on how intelligent some of the programs she discusses in \emph{Machines Who
Think} actually are. Ornstein, Smith, and Suchman, in their 1984
article ``Strategic Computing,'' warn of the difference between domain
capabilities and ``common sense,'' and suggest that ``unwarranted
optimism'' and a particular funding climate (issues also present today)
push researchers to mask the shortcomings of AI with ``semantic
shifts'' \cite[p. 14]{ornstein}. We alter the definitions of ``knowledge'' and ``understanding''
to fit what our machines can do, and these claims, taken literally,
``give rise to unrealistic confidence in the power of the
technology'' \cite[p. 15]{ornstein}.
The two-way process of linguistic and technological change---that
intelligence gets applied to describe whatever researchers manage to
achieve, while real ``intelligence'' retreats away from each
computational advance---leaves these terms poorly defined.

However, the utilitarian model of AI makes good sense for a number of reasons.
First, much can be achieved with current technologies. Rather than
focusing on what is potentially an exceedingly long-term project,
which would carry much greater risks and more distant rewards,
achieving short term goals is an 
attractive prospect. It can attract investment because it can be
profitable sooner. Though short-term commercial viability is not
necessarily applicable for this vision of the self-driving vehicle, it
is possible, at least, to have working prototypes on the road, generating
interest and publicity, even in relatively controlled conditions. Taking
journalists for test drives drums up interest, even if the technology
is not ready for full-scale deployment. Make no mistake: current
prototypes are highly capable, regardless of their faults, but any claim that
they ``know'' or ``understand'' is a tenuous misuse of words which
could no longer credibly mean what they generally do. Such use opens,
rather than closes, questions.

Second, humanlike characteristics may not even be helpful, in general, for building
specific applications. One would likely not want one's self-driving car
to be preoccupied or emotional. It may be
that, for utilitarian 
purposes like driving, many of the characteristics of the human are
detrimental, their elimination helpful and intentional. Much of the
discussion around why autonomous vehicles are necessary centers on
just such qualities: distractability, sleepiness, lapses in
concentration.\footnote{A vast proportion of articles make a claim
  like this. For example: ``They don’t get sleepy or
  distracted, they don’t have blind spots, and there is nothing on
  their 'minds' except getting safely from point A to point B'' \cite{merrill}.} We
would not wish to emulate such 
characteristics in robotic systems. However, though these are human
capabilities, presented in this way they exist largely as caricatures
of the human. People possess a
variety of other capabilities which might be helpful to many AI
applications. As AI researcher Doug Lenat wrote in 1997:
\begin{quote}
Before we let robotic chauffeurs drive around our streets, I'd want the
  automated driver to have a general common sense about the value of a
cat versus a child versus a car bumper, about children chasing balls
into the streets, about young dogs being more likely to dart in front
of cars than old dogs (which, in turn, are more likely to bolt than
elm trees are), about death being a very undesirable thing \cite[p.
  122]{ekbia}.\end{quote} 

This is a difficult knowledge and perception problem. But even more,
it is an issue of selfhood, embodiment, even sentience. While cats,
children and bumpers can be identified as objects, and children
chasing balls into the streets can be identified as patterns, a
computer programmed to respond to these stimuli may respond correctly
without ``knowing'' anything. While a machine can be programmed to avoid
running into people, can it have any understanding of death? Can it be
programmed to ``feel guilt''? Does it need to?

%CAN I FIND RESEARCH ON reactions to humanoid vs. non-human
%AI drivers

Current approaches, however, assume this kind of deep understanding is
unnecessary, both for the technical creation of such vehicles as well
as their public acceptance. If the machine behaves in an appropriate
way, like an ideal human driver, it will be a ``satisfactory social
prosthesis'' in Collins's terms, and ``will not appear alien'' even if
it cannot truly ``understand'' \cite[p. 31]{Collins}. Driving, from
this perspective, involves only behavior-specific acts, which can be
satisfactorily emulated using the behavioral coordinates of
action \cite[p. 33--37]{Collins}. And yet the ideology of artificial
intelligence, the focus of the field itself, is bound up in the idea
of ``intelligent'' machines that can be said to ``know.'' And the
knowing involved in action, as opposed to mere behavior, may be
important for human acceptance of highly automated technologies. It is an
interesting read on the changing 
times to notice that Lenat's statement seems to suggest humanoid
robotic drivers operating regular cars. As well as moving toward
functionalist systems, the industry has moved toward embedded systems
within devices, systems that make no pretenses to be humanoid, but
instead revel in appliance-hood. While once, a dominant dream might have been
to build a world full of humanoid robots, the conceit of modern
consumer AI---IBM's Watson cloud API, ``internet of things''
approaches---is that we can make everything smart. Whether this shift in the form of AI
systems makes customers more or less nervous about computer-driven
vehicles is an empirical question. But it certainly suggests a desire
to make the systems more 
invisible---and actually has important implications for human-computer
interaction, as we will see in chapter \ref{chap:3}. Though it may seem
obvious, it is critical to remember that the devices we discuss here may
have the properties Lenat mentions only in the way that the University
of T\"{u}bingen's AI Mario may be
``self-aware,'' possessing programmed constraints that cause it to
avoid people, and perhaps expect children to behave erratically.
Nonetheless, despite lacking the facilities for actual ethics, these
devices possess an implicit ethics: the
behaviors of these devices will the instantiate moral and
ethical judgments of their human creators, based on
human-authored heuristics and statistical predictions. They will not
act ethically, but must
behave according to their programmatic ethics. This will be
fundamental to how we understand our relationships to such
complicated technological systems.

%%ETHICS
Robot ethics is an issue of growing importance to society at
large---given the rapidly-expanding uses of robotics for labor, military,
research, entertainment, and healthcare, among other regimes \cite[p.
  5-6]{patrickLin}---but we should be very clear what we mean when we
discuss it. Lin, Abney, and Bekey's collection is wide-ranging,
considering perspectives including safety and errors (how should
robots be introduced in order to minimize adverse effects?), law and
responsibility (when accidents happen, who is liable?), ethical codes
(how, and with what ethical frameworks, can robots be programmed to
operate ``ethically''?), and social impact
(how do we weigh potential for robots to eliminate
jobs?) \cite{robotEthics}. Here, I consider ethical codes specifically,
since they interact with functionalist approaches. The question of
driverless car ethics has spurred quite a volume of news articles,
with a greater or lesser handle on the questions
involved.\footnote{One article discusses a notable
  researcher who has purportedly ``stressed the need for driverless
  cars to be flexible enough in their 
engineering to be able to make ethical judgements that aren't
necessarily written into their programming,'' a statement whose
meaning is difficult to parse \cite{jessicaDavies}. Such statements
presumably 
refer to judgments that are not explicitly defined by programmers, but
instead reliant on some kind of more general ethical calculus. A
program that does things that are not in its programming at all,
however, is incoherent. Even bugs, though unintentional, are part of
the program. This sort of phrasing is unfortunate, as it
risks being read as a robot somehow operating ``outside of its
program,'' making ad-hoc ethical decisions based on criteria of its
own invention (even an ethical system trained via machine learning
techniques is fundamentally tied to its programming, though its
behaviors were not strictly ``written'' by its human programmers, but
are instead shaped by their selection of training situations and
responses; and the choice to use these techniques in the first place
comes with a certain risk, that a system
might learn the ``wrong'' ethical behaviors, for example). Such
behavior is impossible for anything but a sentient
AI, as I discuss here; and focus on it distracts from the real issues
at hand.} Lin himself has been successful in urging a dialogue
within various autonomous vehicle research groups about the ethics of
their products, as well as getting media recognition of this push for
self-conscious development \cite{timeEthics}. However, many articles
elide fundamental distinctions in the situations being discussed,
distinctions which should be foremost in our minds when we consider
the stakes of developing autonomous machines.

A focus on machines behaving ethically is at risk of making similar
errors to the conflation of machine and human vision, knowing, or
understanding. Ethical behavior is not necessarily strictly defined by
any one ethical code:  ethics and ``ethics'' are not identical. When we
speak of robotic cars ``making decisions'' of what to do in a crisis
situation, we implicitly accept the idea that such decisions are
really being made by the program. Discussing whether or not those
behaviors are ethical risks suggesting that our robots have all the
capabilities---of sensing, knowing, and processing---necessary to
carry out, \emph{in toto}, what we consider to be ethics \emph{qua} itself. At this
stage in AI development, however, what we are truly discussing is not
the potential ethical robot cars but an \emph{ethical calculus} for
autonomous vehicles. Such a calculus would be a quantification of
ethics according to some particular formalism, so as to allow a
computer program to select a course of action based on a particular
situation. However, in a very real sense, decisions are not
``being made'' by amoral vehicles.\footnote{Not every device behavior
can be predicted, and it would be foolish to place full responsibility
on the programmers: there is real autonomy in devices, in that they
may do things we do not want. But though all devices have bugs and
will be unpredictable in certain circumstances, the first place to
look for ethics, for an implicit or explicit ethical calculus, is the
human beings 
that do the design.} They are being made by software engineers,
self-consciously or not, even if no
explicit calculus is used and the developers' ethics are only
implicitly present as a consequence of coding decisions. Ultimately,
if we want to care about how ethically systems
operate, we must look at how they are programmed, and what goals that
programming is intended to serve. Only with this focus can we agitate
against systems that are fully black-boxed---closed-source, protected
intellectual property potentially defended by anti-circumvention
laws---and enact an ethics impervious to scrutiny.\footnote{Digital
  rights management and anti-circumvention laws are abused routinely
  to lock out consumers and even muzzle security researchers. For a
  recent example, see \cite{higgins}.
}



%% 2.3) what standard of safety? 
%% --better than an average person? or better than the best people? (1 p)
%% --human problems with using projected statistics to define policy:
%% does it make people feel better to know a system is statistically
%% safer when they are uncomfortable with it? see cars/planes for example
%% (3 p)
%% -->perspective of caution? or  ``losing lives every day'' that
%% could be saved?
%% ----how much risk is legitimate?? (2 p)
%% --airlines as a place where we see the different dimensions of
%% statistical safety vs. passenger's feelings of safety (3 p)
%% ---drawing from PARC/CAST documents
\subsection{Safety and Statistical Risk}

%%ALSO: consider the risk comments in Cox \& Murray, Mar2 notes;
%%trying to decide how many 9s to have; and what counts as evidence?
%%is it just statistics, testing of components? or can we judge based
%%on designs??

%%TODO ``competent'' below, TL writes ``?''
The primary driving force in the current self-driving car
narrative is safety; specifically, the poor safety record of human
drivers and the potential for machines to do much better, free from
human frailties of distraction and fatigue. How bad are human
drivers, really? Conventional stories of human drivers paint us as
plainly terrible, prone to road rage and drowsyness, and generally tending
to do anything except pay attention to the road\footnote{Numerous popular
  articles take this position, some based more in fact, and others
  more in rhetoric..
  But human fallibility is often mistaken for humans being useless in
  all situations, a misconception we will return to in chapter
  \ref{chap:3}.}. However, the statistics tell
a slightly different story. Official numbers show that the number of
motor vehicle accidents remained in the range of 10--11 million
per year for most of the late 2000s. The death rate has
decreased overall in this time, settling somewhere below 1.5 deaths
per 100 million vehicle miles, presumably due to a combination of
better safety features (especially since 1990) and other
factors \cite{censusDeaths}. The total number of vehicle-related deaths is much
lower, at a still significant 35,000 deaths per year. But this alone
does not tell the story. At around 1.5 deaths per 100 million vehicle
miles, or about 1 death per 67 million miles, humans seem relatively
competent in a statistical sense. The average American, who might
drive 1 million miles\footnote{This is a conservative estimate,
  extrapolating from an average yearly mileage of between 13,000 and
  15,000 miles \cite{fhwa}.} is unlikely to be
involved in a fatal crash in his or her lifetime. Looking at non-fatal
accidents as well, humans get involved in about one accident per
286,000 vehicle miles. Part of why the autonomous vehicle problem is
such a difficult one is that these numbers are relatively high.

Theoretically, computers can do better. But especially careful human
drivers can also clearly beat the human average. But how safe do
autonomous vehicles need to be in order to be allowed on our roads?
Safer than the average human? or safer than the very best
drivers?\footnote{Chris Gerdes has admitted, as recently as September
  2014, that the deep, intuitive experience of race-car drivers to
  handle emergency situations is something they are still working to
  match \cite{8truthsandmyths}.}
Such questions have real impact when it comes to how devices are
designed and when they become commercially viable---and are at least
partly ethical questions. The autonomous
vehicle enterprise seems to call for using such projected statistics
to define policy. One narrative is that undue caution in the
rollout of autonomous vehicles will directly ``cost'' lives, since
people are killed by human drivers every day \cite{driverlessfuture}. However,
people are also accustomed to the current automobile death rate, and
any autonomous vehicle crashes are likely to attract deep
scrutiny\footnote{As Jim Womack pointed out to me, there is no good side
to change as a regulator. Regulators are not congratulated when things
go right, only criticized when things go wrong. So some measure of
tentativeness is almost certainly justified to the regulatory mind.
(Discussion with the author, December 3, 2014)} as
to whether a human could have prevented the accident. If,
instead, human oversight to otherwise fully automated systems were
required, the additional risks of supervision would also need to be
accounted for (this is dealt with more thoroughly in chapter
\ref{chap:3})---including the potential for risk homeostasis
\cite{Wilde}, the tendency to
behave less cautiously in situations that appear to be more safe.

These are questions of policy, but also questions of human acceptance.
This ideology posits lowering accident rates above all else, leaving
no space for human squeamishness about technology and responsibility. 
The statistical argument suggests that the death rate is all that
matters, but humans are notoriously bad at understanding and
responding to statistics.\footnote{And any possible framework for
  quantitatively measuring and regulating improvement (e.g. number of deaths,
  monetary cost, etc.) hides all manner of assumptions and comes with
  potentially unforseen consequences: the best decisions by some
  metrics will be non-optimal in others.}

Just how safe these vehicles are expected to be has become a point of
public contention. A 2015 whitepaper by Sivak and Schoettle of the
University of Michigan Transportation Research Institute attempts to
make the case that ``it is not a foregone conclusion that a
self-driving vehicle would ever perform more safely than an
experienced, middle-aged driver,'' due primarily to issues of sensing and
predictive knowledge \cite[p. i]{SivakSchoettle}.\footnote{This is worth
recognizing, even if it is also trivially true that nothing about AI
development is a foregone conclusion \emph{a priori}. I believe that
even if ``computational speed, constant vigilance, and lack of
distractibility'' are not alone sufficient to beat out all human
drivers, \cite[p. 4]{SivakSchoettle} AI techniques will approach human abilities to use predictive
knowledge, given sufficient development time.} Most firmly, they
attempt to impress that no conceivable implementation of self-driving
vehicles will have zero fatalities. One popular response to this type of
argument is the following: 
\begin{quote}Of course, the researchers are trying to correct what they regard as
excessive technological optimism. Still, is it entirely fair of them
to compare robocars only to the best drivers? Most accidents are
caused by the worst ones, and it is beginning to become clear that
those are the people that a robot could outperform with one clanky
arm tied behind its back \cite{rossSafety}.\end{quote} 
Others argue that self-driving vehicles are not ``as bad as a middling
driver,'' but ``as \emph{good} as one,'' and place great faith in the
``pinnacle of human mastery of software'' to outperform human drivers \cite{templetonB}.
But these perspectives are far too simplistic, and miss the point in a
significant way. The question of whether an
automated vehicle's fatality rate exceeds some peoples', matches the
safest drivers, or bests all human drivers stands to determine which
vehicles are legal \cite[p. 6]{SivakSchoettle}. While projecting vehicle
risk functions is tricky enough when comparing against a known human
standard, it becomes even more difficult when one considers that the
``conventional vehicle'' risk function is itself going to change with
future safety technologies, including automated driver assistance
features. The fully self-driving vehicle must be compared to a moving
target. Meanwhile, academic researchers are unable to gain access to
these systems to test them against exploits
\cite{madrigalHack}.\footnote{The
implication here is quite disturbing: car companies are so far able to
set the agenda for their own testing (and Google is apparently
lobbying to count simulated virtual tests as road-test miles
\cite{harrisVirtual}),
and third-party oversight is not yet possible.}

%(Phillip E. Ross, ``Has Robocar Safety Been Hyped?'' 20 Jan 2015: http://spectrum.ieee.org/cars-that-think/transportation/safety/has-robocar-safety-been-hyped)

In both cases, measurements and predictions of reliability and risk
are key to the development of autonomous vehicles. The aviation and
defense spaces are in some ways ahead of commercial ground vehicle
research in terms of engineering automated systems, in part because
they have already been forced to confront these issues in earnest:
airplane autopilots have a more developed history than automotive
autodriving, aided by the lower complexity of the air environment,
with simple mechanical autopilots available as early as
1912 \cite[p. 16]{NRCAutonomy}. The \emph{Autonomy Research for
  Civil Aviation} report by the National Research Council pays
significant attention to the fact that ``the lack of generally
accepted design, implementation, and test practices for
adaptive/nondeterministic systems will impede the deployment of some
advanced IA [increasingly autonomous] vehicles and systems'' and that
``existing V\&V [verification and
  validation] approaches and methods are insufficient for advanced IA
systems'' for many of the same reasons \cite[p. 2]{NRCAutonomy}.
Among the high-priority research projects they identify as being both
most pressing and most difficult, they include both the development of
methodologies to ``characterize and bound the behavior of
adaptive/nondeterministic systems'' and the creation of standards for
the ``verification, validation, and certification of IA
systems'' \cite[p. 4]{NRCAutonomy}. As in the automotive space, the
core reason for increasing autonomy is to increase reliability, but
being assured of this reliability is difficult. As the report notes,
however, software creators ``are generally expected to prove that the
software can deliver the intended capabilities at specified
performance levels and in the relevant environment,'' which involves
extensive examination of the code, and testing every logic path,
according to FAA guidelines \cite[p. 39--40]{NRCAutonomy}. But
these approaches are not scaling to more complex systems, and new
validation approaches are required to account for the impacts of human
operators or supervisors on system behavior \cite[p.
  40]{NRCAutonomy}. The NRC panel is not alone in recognizing these
issues: the Defense Science Board in their task force report \emph{The
  Role of Autonomy in DoD Systems} attempts to address the importance
of the larger environment in which automated systems operate, and
within which they can produce ``unintended operational
consequences'' \cite[p. 2]{DSB}. They warn of \emph{brittle}
platforms, and emphasize the importance of developing ways to predict
and understand the resilience of systems\cite[p. 7, 11]{DSB}

%Include NITRD report, if available? http://scenic.princeton.edu/NITRD-Workshop/index.html 

However, when it comes to public adoption of automated technology,
risk assessment is only part of the story. Does it make us feel better---more
comfortable, more likely to get into autonomous taxis and spend our
money on autonomous cars---just to know that they are statistically safer
than the average driver? Airplanes are one type of vehicle for which
these different dimensions of 
safety---statistical safety compared with perceptions or feelings of
safety---have already become visible. Despite the comparative
statistical safety of flying,\footnote{This information is relatively
  available and publicly known \cite{airlinereporter}, though that does not
  necessarily change people's minds about travel.}
people tend to be more afraid of getting on an airliner than getting in their
cars. While this may have to do with a number of factors,
including that aircraft do not remain on the ground during operation,
it also represents a situation in which passengers give over their
agency to pilots performing a job they do not understand and could not
take over in an emergency.

%SIMILAR ISSUES WITH TRAINS?\cite{??}

The CAST working group has had to account for these issues in their
recommendations about airplane safety since 1998. Part of their charter---to
reduce the accident rate by 80 percent \cite[p. 28]{PARCCAST}---is
motivated not so much by the rate itself but predictions about its
most visible effects:  if air traffic were to continue to increase
with industry predictions but to maintain the 1997 rate of 1.5 major
crashes per 1 million departures, we would see one fatal crash per
week by 2005, and one per day by 2025\footnote{See
  \cite{predictmorecrashes},
  for example. This point was made to me by David Mindell, discussion
  with the author, November 12, 2014.}
This frequency of accidents was judged to be intolerable to
airline customers, even though the overall accident rate would be no
larger than its ``very safe'' \cite[p. 129]{PARCCAST} starting
value.\footnote{By comparison, the CAST website describes: ``The
  enhancements made over the past decade have made history in
  commercial aviation - by making it the safest it has ever been.
  Today, fatal accidents are reduced to only one in 22.8 million
  flights, an absolutely remarkable achievement.''\cite{cast-safety.org}} Though
aircraft would be as safe as they 
had ever been, people, on seeing the sheer number and scale of
accidents, would be unlikely to choose to fly. While the issues facing
aircraft and trust are not precisely the same as those for cars, this
example shows that actual safety rates are only
one component of interest when considering how consumers react to
modes of transportation. Perceived safety---due to accident scale,
publicity, trust, or other factors---may be very different. But
while research into how to get human beings to trust robotic drivers
is being done \cite{rossTrust}, the voices pushing for autonomous cars sooner
rather than later would suggest that the statistics are all that
matters (and indeed, would tend to fabricate those statistics out of
mere predictions). 


\subsection{Planning, Policy, Cities}

Besides being presented as steps toward road safety, driverless cars
also become part of the rhetoric of personal mobility, ``public''
transportation, and urban planning. Potential or imagined
benefits of these vehicles include empowerment for the blind or
elderly (as we discussed in chapter \ref{chap:1}), a reduction in
traffic congestion, elimination of parking problems, and the
persistent access to a dense network of hireable point-to-point
transport vehicles (essentially, driverless taxis). There is
  also an environmental case being made for these vehicles, as they
  will drive predictably and can therefore be tuned to be very
  fuel-efficient. The social, environmental, and urban planning
implications are at least worth considering in passing before moving
on to consider alternate models for how these technologies could be
developed. 

In my discussions with a fellow researcher who has been studying the
development of self-driving vehicles, it became clear that the
developers see themselves as people trying to change
society.\footnote{This self-image is not limited to those working on
  mobility systems. John Naughton writes---and my contact with
  programmers would lead me to agree---that many ``are fired with a zealous
  conviction that they are doing great stuff for the
  world.''\cite{http://www.theguardian.com/commentisfree/2015/feb/22/google-tech-elite-living-in-a-parallel-universe-john-naughton}}
They
tend to buy into utopian visions of big cities freed of cars, with no
parked cars on the streets and only driverless cabs. This desire to
change the way cities operate is admirable: our cities have a
difficult hundred-year history of changing to cope with the
automobile, and many of these changes have not been for the better
when looked at through a broad lens. The prevalence of automobiles has
literally re-shaped city centers,\cite{???-kemp} widened
streets,\cite{???-kemp} and generally made many cities less safe for
pedestrians and bicyclists. Many of the streets in the most pedestrian
friendly zones of older cities would no longer be legal to build under
current traffic codes, and the default lane widths for arteries have increased
from 10 to 12 feet over the past fifty years, with disastrous results.\cite{???-http://www.citylab.com/design/2014/10/why-12-foot-traffic-lanes-are-disastrous-for-safety-and-must-be-replaced-now/381117/} In some of the
United States's most pedestrian-unfriendly cities, major intersections
in the downtown area are more than 9 lanes across, each of which is 12
feet wide. These changes in 
regulations and practices have contributed to more sterile urban
centers with reduced potential for vibrant civic life. Urban planners have
in recent years begun to take the automotive threat seriously, pushing
for reduced lane width requirements and designing urban areas that
limit vehicle traffic on purpose.\cite{???-kemp} If self-driving
vehicles have a high probability of effecting a positive change on the
cityscape, as some proponents claim, they could be an important
addition to these developments.

%%TODO input an X below
Congestion and other environmental factors, such as air pollution, are
other social reasons used to support automated vehicle development.
With carefully controlled, automated driving, the fuel consumption of each
individual car could be reduced, just as so-called ``hypermilers''
today use altered---and very conservative---driving techniques in
order to increase their fuel efficiency.\cite{???} As discussed in
chapter \ref{chap:1}, researchers envision using V2V communications to
share road information between vehicles, reduce gaps between cars, and
thereby increase throughput and efficiency of the road system. But
these proposals involve numerous potential pitfalls. While coordinated
fleets of vehicles could increase the throughput of roads by 2 to X
times, significantly greater increases would be necessary to match the
throughputs of buses or subway trains, which can be 100 times
greater.\cite{???-kemp} Much depends on actual usage---subject to
homeostasis of inconvenience and the Jevons paradox. If a factor of 10
increase in highway throughput were possible, but caused a factor of
10 increase in overall miles driven, congestion would not be
ameliorated, and significantly more environmental impact might result. Any
large-scale impacts of automated vehicles are dependent on the
architecture of the overall transportation system. And therefore it is
the transportation system, not automated vehicles alone, that must be remodeled.

Fleets of fully automated vehicles could reasonably be expected to increase
efficiency and decrease reluctance to take trips---making them
less-inconvenient by allowing the driver to perform other tasks---in
which case vehicle usage would be expected to increase. This result is actually
implicit in the visions of those who would support fleets of
driverless taxis to move people around cities:  these concepts are
explicitly oriented toward providing for needs that are presently
under-served, and so presenting automated vehicles as a solution to
city transit problems implies greater usage of such vehicles to meet
those needs. So as increases in the use of cars to commute have
decreased the quality of urban spaces, what is to stop cities of
automated vehicles from
being even more unfriendly to
people? This question was in fact asked at the MIT SENSEable Cities
conference in 2014.\footnote{It received little in the way of reply from the
panel. Nhai Cao suggested that cities will not be built around new
vehicle infrastructure, and that his focus is on building new
capacities into vehicles, which avoid the question. Paolo Santi was
unconcerned with vehicle-to-pedestrian sensing and interaction, but
instead with the very limited domain of mathematical optimization,
a viewpoint that seeks to ignore the potential consequences of such
optimization.} Positive changes to city infrastructure will not happen
automatically. Complex
changes to complex systems require significant modeling work,
experiments, or even trial and error to get right. Old models and
habits need to be broken---consider for example the rules (about road
widths, for instance) enshrined in law and the AASHTO ``Green Book'', as well as their
surrounding culture, rules which are often taken almost as natural
law\cite{???-kemp}---and new ones built to replace them. 

However, we should be aware that these visions, however contingent,
are not universal. Looking more closely, my informant described to me, the visions
of driverless car developers
often have a strong male bias. They see businessmen
taking driverless cabs to work, and students getting drunk and, unable
to drive themselves home, using automated vehicles to ferry themselves
back to their apartments. These visions may be greatly influenced by
the developers themselves---primarily male engineers with masculinist
preconeptions---either thoughtlessly or as a marketing strategy. But
regardless of the source, they suggest designed solutions may tend to
be predisposed to certain types of uses, and less amenable to others
that fall too far outside this masculine vision. Other uses are still
possible, of course---Uber is already being used by parents to ferry
their kids to school, even though it was almost certainly not designed
for that
purpose\cite{???-http://healthland.time.com/2013/11/14/for-more-parents-its-uber-to-the-rescue/--http://www.nytimes.com/2013/09/26/fashion/moms-van-is-called-uber.html?_r=0}---but
inherent gender bias in design is not a problem that should be ignored
when making large-scale changes to infrastructure, as it can deeply
affect the ways and frequencies with which people choose to use new
technologies. Indeed, a recent UK study suggests men may be more
amenable to autonomous vehicle technologies, and it is worth
considering the role that gender-biased technological visions may have
in that
effect\cite{???-http://www.nytimes.com/2013/09/26/fashion/moms-van-is-called-uber.html?_r=0}.
Thinking about the variety in types, uses, and users of road vehicles---from
subcompacts to vans to pickup trucks; from solo commuting to carrying kids to
hauling construction equipment or moving furniture; from the wealthy
to the poor, the urban to the rural---makes clear that cars are
multipurpose vehicles, with culturally-specific uses. 

%%Perhaps add in a quick aside about a map-only interface that
%%requires a destination for operation: this assumes your driving has
%%a clear and defined destination that you know; cruising around may
%%be possible but is not the intended mode of operation

When the
expected user is the commuting, upper-middle-class, working father or
the privileged college boy (and the opposers of the technology are
branded by some, as we saw, as ``soccer moms''), we risk developing
vehicles that preferentially serve certain dominant uses and not
others. When I asked one informant whether the developers have an idea
of what driverless cars would mean for people who have children, he
replied:  ``they don't think about it.'' While certainly not shared by
all developers in this space, it is not a vision we can afford.

This chapter has investigated six different perspectives on the
social, cultural, and informational contexts of the autonomous
vehicle, including:  the technologies involved in its operation, the
data it collects, the statistics with which it is motivated, the
ethics and epistemologies with which it is designed, and the civic
ends to which it might be mobilized. While in chapter \ref{chap:1} we
illuminated the historical background behind the dominant narrative of
the self-driving vehicle, here we sought to illuminate the
consequences of this vision. If we accept this narrative, these design
fictions, for the automotive future, what comes along for the ride? We
have seen that the automated vehicle, so envisioned, exists at the
nexus of a number of deep, important questions about our relationships
to technology: what level of privacy is appropriate? what counts as machine
knowledge? how do we design machines that can be genuinely ethical,
and do we need to? We have also seen that these vehicles present a
number of thorny problems not often recognized: what level of
representational work is required to make them function? how do we
predict the relative safeties of devices that do not exist? can we
achieve better urban design simply by adding autonomy to vehicles?
All of these questions are united by being largely unanswerable---and
the problems, unsolvable---by a focus only on the device itself,
narrowly defined. Instead, these are system-level problems, question
that must be asked of large networks of human and machine actors with
subtle and shifting interrelationships. Some are likely to be issues
for any approach to vehicle automation; however, alternative models of
automation present the possibility to soften some of the more
troubling questions of machine capabilities. Let us turn to a fresh look at
vehicle automation, drawing from sources outside the dominant
narrative, and explore what effect an alternative model has on the
stakes of automotive automation.

