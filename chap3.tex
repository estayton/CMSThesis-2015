\chapter{Hybrid Vehicles, Hybrid Narratives}

Technological Realities (27 p)

%% 4. What other stories are there?
%% -HSC
%% -real history of automation of work
%% -other autonomy research
%% --researchers in the field
%% -some stakes (data) are still applicable, but others may be
%% ameliorated

%% if this will still be long enough, consider eliminating automation
%% and AI from this section and moving them back to ch 1. then these
%% alternative narratives are really not all the alts, but they are
%% those alts that are pretty much never referenced . . . 


\subsection{The Untold History of the Automation of Work}

As we saw in \ref{chap1}, the automation of factory work is an
important touchstone for narratives about the automation of driving
and the creation of autonomous vehicles. But just what the
processes of standardization, mechanization, and automation (or
``automatization'') have done to the factory, and to laborers in it, is
not clearly understood among many who write about autonomous cars. Within this
forgotten history---which is substituted for by an imagined person-less
factory that does not exist in the real world---there are even lessons to be
learned by the research community. This past is relevant, perhaps
more than ever, to the future of transportation.

A search for the beginnings of industrial automation takes us to the middle of
the 18th century: Vaucanson's mechanical loom dates to 1741, and formed the basis of
 later developments in weaving by Joseph Marie Jacquard.\cite[p. 9]{dieboldImpact} 
But the first true example of industrial automation originating in the
United States does not come until Oliver Evans' work in the 1780s on
automated grist mills.\cite[p. 5]{roesmithYankee} Through a series of elevators and descenders,
horizontal screws, spreaders and rakes, his mill moved grain from raw
agricultural commodity to finished product: sifted flour. And ideally,
all parts of the process would occur without human intervention. 

In reality, the process both increased efficiency and decreased the costs
of production, so much so that the same basic machinery is still in
use today in some smaller milling operations.\cite{wyegrist} None of his individual
inventions---which he lists as the elevator, ``conveyer,'' hopper-boy,
drill, and descender in his 1795 miller's guide---was a particularly
groundbreaking achievement, but what Evans did was place these devices
in succession so as to allow continuous production, and the
elimination of many slow human jobs that degraded the quality of the
product by tracking dirt and contaminants around inside the
mill.\cite[p. 203]{evansMillguide} 

It took some time for the high level of automation found in the Evans
mill to spread across other industries, and the mill may find its
closest cousins in the ``automatic'' factories of the 1950s and 1960s
and the roboticized factories of today, but Evans's contemporaries
were not uninterested in increasing efficiency and output. Paul
Revere, one of America's early industrialists, applied shifts in
manufacturing techniques to transition himself from an
artisan worker to manager and overseer of others over his long
metalworking career.\cite[p. 187]{martello} Like a small number of his postrevolutionary
contemporaries, he improved his circumstances by becoming a manager
and owner rather than a laborer, but manufacturing itself was a site of public
debate, pitted against the ``inherent virtue'' of agricultural pursuits.
Tench Coxe, a political economist, wrote in 1810 that ``new machines
and power sources allowed even greater productivity with less labor,
further underscoring the connection between technology and republican
virtue.''\cite[p. 217]{martello} To Coxe's romantic view, these machines  worked ``as if they
were animated beings, endowed with all the talents of their inventors,
laboring with organs that never tire, and subject to no expense of
food, or bed, or raiment, or dwelling.''\cite[p. xxv]{coxe}

But these romantic words did not represent the whole reality of
industrial machine labor. Human labor of
maintenance and supervision is implicit in these manufacturing
machines, but it is rendered invisible by the rhetoric that the
machines themselves require no bed or board. At the same time, Coxe's
use of the word ``endowed'' should focus our attention on exactly which
of the ``talents'' of the inventors have been automatized, and the human
labor necessarily involved in that conferring of capabilities. 

Arsenal practice was the site of multiple revolutions in U.S.
manufacturing technology through the 1800s and early 1900s, notably
the development of the so-called “American system” of interchangeable
parts, through which a gradual increase in mechanization would seem to
continue, driven by the tight tolerances necessary for this production
method. And yet armorers and managers at Harper's Ferry resisted the
mechanization of their craft,\footnote{Blanchard's many automatic
  machines for making gunstocks were of particular
  importance,\cite[p. 56]{roesmithHarpers} but a wide variety of machinery
  was implemented in the gun-making process at Harper's Ferry. Most of
 these machines still required significant human labor. Nash's
 barrel-turning machine in part mechanized the production of barrels
 of standardized dimensions: it consisted of a lathe on a wooden
 frame, with human-operated props to hold the barrel in
 place.\cite[p. 119]{roesmithHarpers} The worker also had to continually
 measure the barrel with a caliper, and adjust the device's chisel
 appropriately.\cite[p. 121]{roesmithHarpers} This gradual implementation of
 mechanized labor was continued in further machines produced by Hall.
 His straight-cutting machine, an early version of a milling machine,
 had as its distinctive feature the ability to be tended by ``common
 hands'' without a loss of
accuracy.\cite[p. 239]{roesmithHarpers}}, and while certain competencies were
transferred from the skilled worker
to the technical apparatus, human oversight and operation was still
integral to the production of weaponry using the new technology. It was
not clear until after the fact that more mechanization was necessarily
better: Harper's Ferry remained ``competitive'' with Springfield's
costs through the mid 1830s.\cite[p. 324]{roesmithHarpers}

The same exchange of competencies characterized Ford's assembly line
production as well, which is cited by David Hounshell as the rise of
true mass production in America.\cite[p. 217]{hounshell} Ford's factory developed fixtures
and gauges, designed to allow for use by unskilled machine tenders. As Donald
Norman writes in Things that Make us Smart---itself, reminiscent of Ed
Hutchins's cognitive anthropology and work on distributed cognition
systems---``the world remembers things for us, just by being
there.''\cite[p. 147]{normanThings} But while the gauge simplifies the
assurance  of quality, it does not
automate it: it simply changes the effort from a more complex judgment
of quality and measurement to a simpler one. While he instituted the
five-dollar day to attempt to solve labor problems at the factory, and
compensate laborers for becoming part of the ``production machine,''
Ford also attracted a wide variety of well-educated skilled mechanics
to his automobile plants.\cite[p. 223]{hounshell} Like Evans, Blanchard, Hall, and others
before them, these mechanics applied their skills to design machines,
and simplify and standardize work processes. The individual judgment
of the assembly line laborer was displaced into standardized tools and
fixtures, built into these technologies by the labor of skilled
machinists and designers.

Meanwhile, Taylorism in factories created ``new managerial
functions'' performed by ``new classes of people with new titles and
more clearly specified responsibilities.''\cite[p. 120]{aitken} A focus on the
people---who are they? where are they? what are they doing?---shows that
one of the fundamental and enduring characteristics of Taylor's
system, the expansion of management roles and the further division of
labor, is not about mechanical automation but about new and altered
types of human work. Industrial processes in the early 1900s continued
the removal of management and strategic decision making from the
workers most physically engaged in product production, installing it
instead within formal organizational structures and the employees that
constituted them. 

This pattern of delayed recognition and contingent change repeats for
numerical control in assembly line production. Numerical control (NC),
developed in the 1940s and 1950s as an outgrowth of World War II
research into feedback systems, slowly began to produce industrial
robots that could perform factory tasks without direct human
intervention. Robots slowly began to replace assembly line jobs such
as spray painting and welding, but adoption was gradual, with only
about 6,000 robots in use in American factories by the mid
1970s.\cite[p. 159]{nyeAmericas} Industrial robots, while automating tasks,
had a way of generating large contingents of skilled human laborers
who still needed to be paid for their services. Industrial robots were
complicated, and needed a variety of skilled workers to tend them, and
to repair them when the broke down. These early experiments did not
increase profits because of the volume of highly skilled labor needed
to keep the machines operating.\cite[p. 162]{nyeAmericas} The development of NC machines
proceeded with a specific interest in eliminating skilled workers, but
the jobs that disappeared were largely unskilled or semi-skilled
laborers.\cite[p. 164]{nyeAmericas} And while Norbert Weiner, in his 1950 book \emph{The Human Use of
Human Beings}, prophesied the end of ``deadly uninteresting'' jobs, which
would be mechanized within 20 years, such changes have still not
totally come to pass.\cite[p. 161]{nyeAmericas} To compound the problem, new industries of
skilled workers---record-and-playback machine designers, and NC machine
programmers---sprang up to furnish factories with their tools.

The process of standardization,
mechanization, and automation has been a process of attempting to
wrest control of the production of components from those closest to
handling them and concentrating it in the hands of management. The Air Force's Integrated Computer-Aided Manufacturing program (ICAM)
brings to light further complexities in the story of the automated
factory: ICAM attempted to aid shop floor automation by automating certain
management functions, ``to try to reduce the enormous indirect costs
that have resulted from the effort to reduce labor costs and remove
power and judgment from the shop floor,'' costs that have continued to
dog new rationalization strategies.\cite[p. 330]{nobleForces} ICAM, like the mythical
ouroboros, sought to offer automation as the ``solution to the problems
generated by automation,'' providing automated scheduling functions,
inventory control, and design tools to ``provide better management
control'' and ``free management from excessive routine duties to do
creative work''---the creative work that the management had attempted to
place in their own hands, in the first place, through earlier
processes of rationalization.\cite[p. 330]{nobleForces}  Automation, Noble's ICAM example shows,
can be used both to routinize work---for the manual laborer---and to
eliminate the routine in favor of the creative---for managers and newly
generated classes of creative workers. 

<Return to Brooks Park and the Wiener shell plant>


%% Noble cites 330, 330
Automation may look very different depending on where in the hierarchy
a person happens to fall, but the historical lesson is that human
involvement remains, though altered in space, time, and kind. As John
Diebold pointed out in 1953, there will be ``no worker-less
factories as a result of automation.''\cite[p. 63-64]{dieboldNew}

So we come to our first of many contradictions at the heart of
automated vehicle research. Their promise:
to provide to us a greater measure of creativity in the act of
driving, to remove some of the ``menial'' and routine tasks of manual
control in favor of strategic decision-making. In this analogy, the
driver goes from being the manual laborer to the creative manager. But
despite a focus on relieving tedium, this is not the way these systems
have primarily been envisioned. Instead, in the process of following
the dream of fully automated operation--where the human labor has been
entirely removed from the shoulders of the person in the vehicle, and
displaced to the invisible labors of mapping, programming, and
monitoring---engineers are designing systems where the ``driver''
seems present largely to ensure the operation of capital-intensive
machinery, burdened with new but perhaps even more menial tasks of
machine tending.\footnote{See for example Tom Simonite, ``Lazy Humans
Shaped Google's New Autonomous Car,''\cite{simonite}  which discusses
the human role within Google's test vehicle,
and the company's response. This is purely speculation, but due to
the way the system operated previously, it is possible at least one Google
employee fell asleep at the wheel, which was the catalyst for their
concern and change in focus.} 

Why might this be? To understand it, we must understand something
about AI history, and the ideologies intertwined with artificial
intelligence research.

\subsection{``Intelligence'' through AI History}

%% 1950s Dartmouth Conf
%% and subsequent winter (search Winston, Six Ages)
Integral to the history of AI as a field is that it was fundamentally
interdisciplinary from the start. Like its forebear cybernetics, it
brought together researchers from physics, mathematics, biology, and
early cognitive science. The field began in earnest with the Dartmouth
Conference in 1956, which brought together many of the enduring big
names in the field. Hosted by John McCarthy (who originated the name
``Artificial Intelligence''), Marvin Minsky, Nathaniel Rochester, and
Claude Shannon, attendees included Trenchard More, Oliver Selfridge,
Ray Solomonoff, Allen Newell, and Herbert Simon: all were united by
``the idea that there was a rigorous and objective way of explaining
the human intellect.''\cite[Ch. 5]{mccorduck} The research areas of
the ``Dartmouth Summer Research Project'' included language learning
and use, ``neuron'' networks,\footnote{Neural networks are one of the
  intriguing long-term stories of AI research, subject of much
  controversy over the years regarding whether or not they would
  actually work. A couple of theoretical developments altered them
  from a curiosity to one of the main techniques in modern AI. This
  half-century journey presaged by the one sentence: ``Partial
  results have been obtained by the problem needs more theoretical
  work''.\cite{dartmouthconf} CAN I FIND THE PERSON WHO SAID SOMETHING
SIMILAR?} self-improving machines, and computational 
creativity.\cite{dartmouthconf} Early successes spurred romantic
predictions, and by 1960, human-level intelligence was predicted by
some to be only a decade away.\cite[p. 3]{winston} Overexpectation, however,
lead to a first AI ``winter'' from about 1965 to 1970, in which the
grand promises of AI were shown to be much further off: the current
techniques simply did not yield advances at the required rate. As Pat
Winston put it: ``Everyone searched for a kind of philosopher's stone,
a mechanism that when placed in a computer would require only data to
become truly intelligent.''\cite[p. 4]{winston} But by the 1970s
research was improving, and excitement building again.


%% DARPA SCI
The early to mid 1980s were also a time of great developments in
Artificial Intelligence, an era of ``celebrity science,'' high hopes,
big investments, and subsequent great public disappointment with the
coming of another ``AI Winter'' beginning in 1987 and
1988.\footnote{These are roughly the dates Russell and Norvig give in
  \emph{Artificial Intelligence} \cite{russellnorvig}} But despite the
warnings of Roger Schank and Marvin Minsky, that overoptimistic
expectations for AI would result in another winter like the previous
one in the 1970s, overall expectations were high, especially within
the business community, which funded companies and assimilated AI
techniques into real applications.\cite[afterword]{mccorduck} Though the 1980s continued divides
within the field about approaches to artificial intelligence, it
actually resulted in a wide variety of successful projects based on
improvements to expert systems, machine learning, natural language
processing, and computer vision.\cite[afterword]{mccorduck} The 1983 US Strategic Computing
initiative, led by Robert Kahn at DARPA, had AI as its third focus
area, with “image understanding” and interpretation—made possible by
the digitized image—as long range project goals. In its revised
10-year plan, the initiative even included an autonomous land vehicle
alongside a pilot's associate and computerized battle management
software. The project suffered serious management problems, and
was eventually canceled, precipitating the general crash in AI
funding—through which research quietly continued, waiting for another
up-tick in public interest. But the Strategic Computing project,
whatever its lofty goals, was no failure. McCorduck cites Roland and
Shiman as saying that ``AI now performs miracles unimagined when SC
began, though it can't do what SC promised,'' which speaks to the
important developments that were made in the service of DARPA's vision.\cite[afterword]{mccorduck}


Though there are a number of approaches to AI research, different
ideas of how machine intelligence can be achieved, it is instructive
to look specifically at how three AI paradigms envision their project.
Each is an answer to the question ``how can we build systems that
can operate without humans?''

%% Classical symbol approaches (3)
%%specifically w/ reference to ideologies
The first important AI paradigm is the classical symbolic system
approach. Associated with Allen Newell and Herb Simon, the idea of the
physical-symbol system hypothesis is that ``symbols lie at the root of
intelligent action''\cite[p. 109]{newellsimon}. Therefore not only
does intelligence require symbolic manipulation, it may indeed be
coextensive with physical-symbol systems, in other words a
physical-symbol system has ``necessary and sufficient means'' for
intelligence and intelligent action.\cite[p. 111]{newellsimon} Such
objects are symbol systems in that they contain symbols and processes
that act upon symbols. And they are physical in that they obey
physical laws and are realizable, in reality, through engineering.
These symbol systems would arrive at answers through a technique known as
heuristic search: by looking through a tree of possibilities in an
intelligent way, we arrive at the appropriate answer.\footnote{The key
  point of heuristic search is that such answers are approximate, but
  arrived at quickly, rather than exact, but arrived at slowly or,
  perhaps, never at all.} Intelligence is applied in heuristic search
by the pruning of the tree: rather than having to apply brute force to
search the entire space, an intelligent system applying heuristic
search makes decisions at each node as to which branches are most
likely to produce a good result and searches those.\cite[p.
  124]{newellsimon} As Newell and Simon wrote what makes a problem a
problem is ``not that a large amount of search is required for its
solution, but that a large amount \emph{would} be required if a requisite
level of intelligence were not applied'': the task of intelligence is
to ``avert the ever-present threat of the explosion of
search.''\cite[p. 125]{newellsimon} 

The conceit then, of the physical-symbol system hypothesis is twofold.
First, it assumes that
human beings essentially operate in this manner: that we apply
symbolic logic and heuristic search to provide for our intelligent
actions. Second, it assumes that computers can be true physical-symbol
systems. These two assumptions are not necessary clear. John Searle
essentially rejects the computer as a physical-symbol system in his
Chinese room example. Instead, the computer (room) is seen as a cheap
imitation of such a system: a room into which strange symbols are
passed, the appropriate responses looked up in a book,
and then passed out again, all without anything in the room having
access to their meaning.\cite{chineseSearle} Though meaningless
symbols are being processed by such a contraption, his view is that no
electronic computer ``can really manipulate symbols, nor really
designate or interpret anything at all,''\cite{escapingBoden} This is
a philosophical question of whether or not computers are capable of
true intelligence, not a matter of whether or not they can
convincingly imitate intelligence (and other philosophers of AI, such
as Margaret Boden, do not take Searle's view on the subject). 

The former point proves harder to dodge. While it may be true, as is
fundamental to the field of AI and which even Searle holds, that
machines can think because ``we are precisely such machines,''\cite[p.
83]{chineseSearle} and therefore it should be possible to create
intelligent machines, this does not guarantee that symbol systems are
the way to achieve intelligence. It may well be that introspection on
thought is a large part of why the logical theory seemed so
compelling: we like to think we are logical. Researchers like Newell
and Simon used ``think aloud'' experiments to identify problem solving
techniques,\cite[Ch. 10]{mccorduck} which seems naturally to suggest a logical response: when
asked to describe how we came to some decision, basing it in logic
seems the most acceptable alternative. While we certainly may apply logic
and process symbols, there is good reason to think that isn't how we
spend most of our time. Symbolic logic takes a lot of mental capacity,
so generally, we use other sorts of shortcut processes to come to
decisions. Pollock calls these ``quick and inflexible'' or ``Q\&I''
models\cite[p. 120]{pollock} and Dennett refers to them as ``habitual methods'' or
mechanical routines\cite[p. 157]{dennett} We rely on Q\&I models to do
much of our day-to-day reasoning because it is ``very important for
humans to be able to make rough probability judgments''\cite[p.
  120]{pollock}, and accepting the output of such approximate models
is not at all unreasonable in the absence of evidence for their
inappropriateness to a situation. A logical solution may be difficult
to arrive at in some situations. The physical-symbol system hypothesis
is just that, a hypothesis, and, historically, the classical symbolic
approach did not yield results as quickly as expected. This was part
of the reason for the first AI winter, and researchers moved on to
other approaches. While logic is part of the puzzle, it seems that it
is not the entireity of it. But this was the archetypal dream of early
AI: humanlike intelligence via logic systems.

%% Rodney Brooks (3)
One of those researchers pioneering a new way of doing AI was Rodney
Brooks. Part of a different wave of AI researchers, disillusioned with
the failures of logic-based robotic systems (like Shakey, the SRI
robot named for its tendency to shake when in motion)\cite[Ch. 10 (CHECK)]{mccorduck} to
achieve intelligent results, he posited a new way to build intelligent
robots, defined by the subsumption architecture.\cite[p. 353]{mobilebrooks} Above and
beyond the problems that physical-symbol system based AI had in
relatively controlled domains, robots controlled with these
techniques, overwhelmed by the complexity of the real world, responded
slowly and ineptly. To rectify this, Brooks attempted to cut out
cognition altogether, focusing instead only on sensing and
action.\cite[Afterword]{mccorduck} Rather than attempt to engineer a
human-level intelligence at once, when years of research had failed to
produce anything approaching these results, why not start small?
Insects, after all, lack cognition, but respond more adroitly to the
world than Shakey and its contemporaries. Uncertainty everywhere in
the physical world makes modeling the world extremely difficult: the
model has a tendency to get out of sync, and there needs to be
something to return it to accuracy. By building systems that can
accommodate uncertainty, Brooks believed, more progress could be
made.\cite[p. 347]{mobilebrooks} Instead of modeling the world, treat
the world as ``its own best model'' and focus on embodiment and action
in the world.\cite[p. 256]{ekbia}

Rather than being built into a single integrated system, the
subsumption approach is by definition modular. More complex behaviors
are built out of simper ones: level zero competence might avoid objects,
while the next level would wander aimlessly, and the next would
attempt to wander to places it had not been before.\cite[p.
  351--352]{mobilebrooks} Each layer is separate (and can actually run
on its own processor), and can read data from
and write data into the layer below it (hence the term subsumption,
the higher levels subsume the lower). When a higher level fails or
cannot run, lower levels continue to operate, and basic behavior is
maintained.\cite[p. 355]{mobilebrooks} This approach has tempting biological connections as well:
accurately or not, one can imagine the human being as a robot with a subsumption
architecture, where breathing and heartbeat are lower than balance,
which is lower than voluntary motion, which is lower than logical
thought. And temptingly, it is an approach that can handle
uncertainties, as each layer is built to be robust in the inevitable
event of inaccuracies and lost messages. Theoretically, one could
imagine building systems of great complexity using these approaches,
but this architecture, the radical relativist response to the
structural realism of the physical-symbol system, did not alone make
it past robots of insect intelligence.

Brooks himself abandoned the project of building intelligence from
these humble, subsumptive blocks. Instead, his research on building
the Cog robot shifts focus from ``emergence'' to ``integration,'' and
reversed some of his initial fervor to avoid representation.\cite[p.
  258]{ekbia} He skipped the middle of the evolutionary tree, straight
to humanoid forms, because the evolutionary approach was too slow: 
Brooks reported, it was ``starting to look like, if I was really
lucky, I might be remembered as the guy who built the best artificial
cat,'' a distinction he apparently did not desire.\cite[p.
  65]{brooksflesh}\footnote{As cited in \cite[p. 258]{ekbia}.}
But to do this, bootstrapped knowledge from other sources was
necessary, bringing back some of the old approaches.

What the physical-symbol system and embodied cognition
have in common is their attempt to start from first-principles or zero
knowledge, and to build inexorably toward intelligence. The difficulty
of this road led Brooks to recombine some of the old manner of
knowledge representation into his robotic techniques. The new source
of bootstrapping information is data, ``Big'' and ``small.'' Current
techniques are primarily statistical in nature, leveraging these data
sets by training machine-learning systems; but as we have seen
machine-learning systems have properties that make them ill-suited for
safety-critical systems, and autonomous vehicles are designed with a
mix of approaches that allows for more introspection into their
workings.

As we have seen, the sort of ``understanding''
involved is limited, dealing only with static images and objects
captioned by humans in the training data. The systems would not be
able to answer other questions about the scenes that have to do with
the material properties of the objects, or likely results of various
actions.\cite{gomesJordan} It is important to understand why this technique is not a
panacea in order to understand the possibility for alternatives to the current
driverless car vision. Another recent slew of articles focuses on
the ``self-aware'' Mario created by researchers at the University of
T\"{u}bingen. Any pretense to worry about a ``self-aware AI . . . with
an insatiable desire for material wealth'' that knows ``how to kill''\cite{vincentMario}
is simply journalistic excess\footnote{As the
  researchers well know. This case is just a convenient example of how
such stories spiral out from the lab and acquire new meanings.}, and suggest
significantly more care must be taken in the use of such terms. Mario
is programmed with emotional states like a word-processing program is
programmed with different modes or display parameters. These states
are simply caricatures of emotions, and that plus the program's
natural-language interface makes it appear more eerily sci-fi than it
actually is.


\subsection{Human Supervisory Control}

—while Hutchins' cockpit
remembers its speeds through a combination of human activity and
physical cognitive aids,\cite{???} the NHTSA's “vehicle” may sometimes “not
perform a control function.”\cite{???}

ref back to NHTSA/SAE; starting from P. 9 of the DM paper: The documentation of the Google car's Nevada driving test
exposes fractures in the traditional perspectives on vehicle
automation.\cite{???} 

As a way to INTRODUCE HSC, actually

\subsection{Lessons from Autonomy Research}

\subsection{Whither Alternate Narratives?}



1) bring in the telerobotics/supervisory control literature
-why supervisory control? and where is it used? (5 p)
-examine the issue of deskilling vs. the ``irony of automation'' well
known in Human Factors, in which increased automation actually
increases human load (2 p)
-but SC allows for particular combinations of human skills and machine
competencies (1 p)
--used all over industry, aviation, undersea (2 p)


3) situate the moonshot approach (which may actually be easier in some
senses but less socially acceptable) and the mixed approach in a way
that does not support a teleology of autonomous vehicle development
(think Sheridan's graph of telerobotics, not the SAE's 5 stages) (4 p)
--in historical context of previous systems that are wholly or
partially autonomous (4 p)





