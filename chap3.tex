\chapter{Hybrid Controls, Hybrid Possibilities}

Technological Realities (27 p)

%% 4. What is the alternative to the teleological progression implicit
%% in the for- and against- stories?
%% (start with story of planes [the backing off from full autonomy] or Mars rovers or something
%% -HSC
%% -Autonomy research from other areas
%% -envision some alternatives
%% -some stakes (data) are still applicable, but others may be
%% ameliorated

%% if this will still be long enough, consider eliminating automation
%% and AI from this section and moving them back to ch 1. then these
%% alternative narratives are really not all the alts, but they are
%% those alts that are pretty much never referenced . . . 

\subsection{``Intelligence'' through AI History}

%% 1950s Dartmouth Conf
%% and subsequent winter (search Winston, Six Ages)
Integral to the history of AI as a field is that it was fundamentally
interdisciplinary from the start. Like its forebear cybernetics, it
brought together researchers from physics, mathematics, biology, and
early cognitive science. The field began in earnest with the Dartmouth
Conference in 1956, which brought together many of the enduring big
names in the field. Hosted by John McCarthy (who originated the name
``Artificial Intelligence''), Marvin Minsky, Nathaniel Rochester, and
Claude Shannon, attendees included Trenchard More, Oliver Selfridge,
Ray Solomonoff, Allen Newell, and Herbert Simon: all were united by
``the idea that there was a rigorous and objective way of explaining
the human intellect.''\cite[Ch. 5]{mccorduck} The research areas of
the ``Dartmouth Summer Research Project'' included language learning
and use, ``neuron'' networks,\footnote{Neural networks are one of the
  intriguing long-term stories of AI research, subject of much
  controversy over the years regarding whether or not they would
  actually work. A couple of theoretical developments altered them
  from a curiosity to one of the main techniques in modern AI. This
  half-century journey presaged by the one sentence: ``Partial
  results have been obtained by the problem needs more theoretical
  work''.\cite{dartmouthconf} CAN I FIND THE PERSON WHO SAID SOMETHING
SIMILAR?} self-improving machines, and computational 
creativity.\cite{dartmouthconf} Early successes spurred romantic
predictions, and by 1960, human-level intelligence was predicted by
some to be only a decade away.\cite[p. 3]{winston} Overexpectation, however,
lead to a first AI ``winter'' from about 1965 to 1970, in which the
grand promises of AI were shown to be much further off: the current
techniques simply did not yield advances at the required rate. As Pat
Winston put it: ``Everyone searched for a kind of philosopher's stone,
a mechanism that when placed in a computer would require only data to
become truly intelligent.''\cite[p. 4]{winston} But by the 1970s
research was improving, and excitement building again.


%% DARPA SCI
The early to mid 1980s were also a time of great developments in
Artificial Intelligence, an era of ``celebrity science,'' high hopes,
big investments, and subsequent great public disappointment with the
coming of another ``AI Winter'' beginning in 1987 and
1988.\footnote{These are roughly the dates Russell and Norvig give in
  \emph{Artificial Intelligence} \cite{russellnorvig}} But despite the
warnings of Roger Schank and Marvin Minsky, that overoptimistic
expectations for AI would result in another winter like the previous
one in the 1970s, overall expectations were high, especially within
the business community, which funded companies and assimilated AI
techniques into real applications.\cite[afterword]{mccorduck} Though the 1980s continued divides
within the field about approaches to artificial intelligence, it
actually resulted in a wide variety of successful projects based on
improvements to expert systems, machine learning, natural language
processing, and computer vision.\cite[afterword]{mccorduck} The 1983 US Strategic Computing
initiative, led by Robert Kahn at DARPA, had AI as its third focus
area, with “image understanding” and interpretation—made possible by
the digitized image—as long range project goals. In its revised
10-year plan, the initiative even included an autonomous land vehicle
alongside a pilot's associate and computerized battle management
software. The project suffered serious management problems, and
was eventually canceled, precipitating the general crash in AI
funding—through which research quietly continued, waiting for another
up-tick in public interest. But the Strategic Computing project,
whatever its lofty goals, was no failure. McCorduck cites Roland and
Shiman as saying that ``AI now performs miracles unimagined when SC
began, though it can't do what SC promised,'' which speaks to the
important developments that were made in the service of DARPA's vision.\cite[afterword]{mccorduck}


Though there are a number of approaches to AI research, different
ideas of how machine intelligence can be achieved, it is instructive
to look specifically at how three AI paradigms envision their project.
Each is an answer to the question ``how can we build systems that
can operate without humans?''

%% Classical symbol approaches (3)
%%specifically w/ reference to ideologies
The first important AI paradigm is the classical symbolic system
approach. Associated with Allen Newell and Herb Simon, the idea of the
physical-symbol system hypothesis is that ``symbols lie at the root of
intelligent action''\cite[p. 109]{newellsimon}. Therefore not only
does intelligence require symbolic manipulation, it may indeed be
coextensive with physical-symbol systems, in other words a
physical-symbol system has ``necessary and sufficient means'' for
intelligence and intelligent action.\cite[p. 111]{newellsimon} Such
objects are symbol systems in that they contain symbols and processes
that act upon symbols. And they are physical in that they obey
physical laws and are realizable, in reality, through engineering.
These symbol systems would arrive at answers through a technique known as
heuristic search: by looking through a tree of possibilities in an
intelligent way, we arrive at the appropriate answer.\footnote{The key
  point of heuristic search is that such answers are approximate, but
  arrived at quickly, rather than exact, but arrived at slowly or,
  perhaps, never at all.} Intelligence is applied in heuristic search
by the pruning of the tree: rather than having to apply brute force to
search the entire space, an intelligent system applying heuristic
search makes decisions at each node as to which branches are most
likely to produce a good result and searches those.\cite[p.
  124]{newellsimon} As Newell and Simon wrote what makes a problem a
problem is ``not that a large amount of search is required for its
solution, but that a large amount \emph{would} be required if a requisite
level of intelligence were not applied'': the task of intelligence is
to ``avert the ever-present threat of the explosion of
search.''\cite[p. 125]{newellsimon} 

The conceit then, of the physical-symbol system hypothesis is twofold.
First, it assumes that
human beings essentially operate in this manner: that we apply
symbolic logic and heuristic search to provide for our intelligent
actions. Second, it assumes that computers can be true physical-symbol
systems. These two assumptions are not necessary clear. John Searle
essentially rejects the computer as a physical-symbol system in his
Chinese room example. Instead, the computer (room) is seen as a cheap
imitation of such a system: a room into which strange symbols are
passed, the appropriate responses looked up in a book,
and then passed out again, all without anything in the room having
access to their meaning.\cite{chineseSearle} Though meaningless
symbols are being processed by such a contraption, his view is that no
electronic computer ``can really manipulate symbols, nor really
designate or interpret anything at all,''\cite{escapingBoden} This is
a philosophical question of whether or not computers are capable of
true intelligence, not a matter of whether or not they can
convincingly imitate intelligence (and other philosophers of AI, such
as Margaret Boden, do not take Searle's view on the subject). 

The former point proves harder to dodge. While it may be true, as is
fundamental to the field of AI and which even Searle holds, that
machines can think because ``we are precisely such machines,''\cite[p.
83]{chineseSearle} and therefore it should be possible to create
intelligent machines, this does not guarantee that symbol systems are
the way to achieve intelligence. It may well be that introspection on
thought is a large part of why the logical theory seemed so
compelling: we like to think we are logical. Researchers like Newell
and Simon used ``think aloud'' experiments to identify problem solving
techniques,\cite[Ch. 10]{mccorduck} which seems naturally to suggest a logical response: when
asked to describe how we came to some decision, basing it in logic
seems the most acceptable alternative. While we certainly may apply logic
and process symbols, there is good reason to think that isn't how we
spend most of our time. Symbolic logic takes a lot of mental capacity,
so generally, we use other sorts of shortcut processes to come to
decisions. Pollock calls these ``quick and inflexible'' or ``Q\&I''
models\cite[p. 120]{pollock} and Dennett refers to them as ``habitual methods'' or
mechanical routines\cite[p. 157]{dennett} We rely on Q\&I models to do
much of our day-to-day reasoning because it is ``very important for
humans to be able to make rough probability judgments''\cite[p.
  120]{pollock}, and accepting the output of such approximate models
is not at all unreasonable in the absence of evidence for their
inappropriateness to a situation. A logical solution may be difficult
to arrive at in some situations. The physical-symbol system hypothesis
is just that, a hypothesis, and, historically, the classical symbolic
approach did not yield results as quickly as expected. This was part
of the reason for the first AI winter, and researchers moved on to
other approaches. While logic is part of the puzzle, it seems that it
is not the entireity of it. But this was the archetypal dream of early
AI: humanlike intelligence via logic systems.

%% Rodney Brooks (3)
One of those researchers pioneering a new way of doing AI was Rodney
Brooks. Part of a different wave of AI researchers, disillusioned with
the failures of logic-based robotic systems (like Shakey, the SRI
robot named for its tendency to shake when in motion)\cite[Ch. 10 (CHECK)]{mccorduck} to
achieve intelligent results, he posited a new way to build intelligent
robots, defined by the subsumption architecture.\cite[p. 353]{mobilebrooks} Above and
beyond the problems that physical-symbol system based AI had in
relatively controlled domains, robots controlled with these
techniques, overwhelmed by the complexity of the real world, responded
slowly and ineptly. To rectify this, Brooks attempted to cut out
cognition altogether, focusing instead only on sensing and
action.\cite[Afterword]{mccorduck} Rather than attempt to engineer a
human-level intelligence at once, when years of research had failed to
produce anything approaching these results, why not start small?
Insects, after all, lack cognition, but respond more adroitly to the
world than Shakey and its contemporaries. Uncertainty everywhere in
the physical world makes modeling the world extremely difficult: the
model has a tendency to get out of sync, and there needs to be
something to return it to accuracy. By building systems that can
accommodate uncertainty, Brooks believed, more progress could be
made.\cite[p. 347]{mobilebrooks} Instead of modeling the world, treat
the world as ``its own best model'' and focus on embodiment and action
in the world.\cite[p. 256]{ekbia}

Rather than being built into a single integrated system, the
subsumption approach is by definition modular. More complex behaviors
are built out of simper ones: level zero competence might avoid objects,
while the next level would wander aimlessly, and the next would
attempt to wander to places it had not been before.\cite[p.
  351--352]{mobilebrooks} Each layer is separate (and can actually run
on its own processor), and can read data from
and write data into the layer below it (hence the term subsumption,
the higher levels subsume the lower). When a higher level fails or
cannot run, lower levels continue to operate, and basic behavior is
maintained.\cite[p. 355]{mobilebrooks} This approach has tempting biological connections as well:
accurately or not, one can imagine the human being as a robot with a subsumption
architecture, where breathing and heartbeat are lower than balance,
which is lower than voluntary motion, which is lower than logical
thought. And temptingly, it is an approach that can handle
uncertainties, as each layer is built to be robust in the inevitable
event of inaccuracies and lost messages. Theoretically, one could
imagine building systems of great complexity using these approaches,
but this architecture, the radical relativist response to the
structural realism of the physical-symbol system, did not alone make
it past robots of insect intelligence.

Brooks himself abandoned the project of building intelligence from
these humble, subsumptive blocks. Instead, his research on building
the Cog robot shifts focus from ``emergence'' to ``integration,'' and
reversed some of his initial fervor to avoid representation.\cite[p.
  258]{ekbia} He skipped the middle of the evolutionary tree, straight
to humanoid forms, because the evolutionary approach was too slow: 
Brooks reported, it was ``starting to look like, if I was really
lucky, I might be remembered as the guy who built the best artificial
cat,'' a distinction he apparently did not desire.\cite[p.
  65]{brooksflesh}\footnote{As cited in \cite[p. 258]{ekbia}.}
But to do this, bootstrapped knowledge from other sources was
necessary, bringing back some of the old approaches.

What the physical-symbol system and embodied cognition
have in common is their attempt to start from first-principles or zero
knowledge, and to build inexorably toward intelligence. The difficulty
of this road led Brooks to recombine some of the old manner of
knowledge representation into his robotic techniques. The new source
of bootstrapping information is data, ``Big'' and ``small.'' Current
techniques are primarily statistical in nature, leveraging these data
sets by training machine-learning systems; but as we have seen
machine-learning systems have properties that make them ill-suited for
safety-critical systems, and autonomous vehicles are designed with a
mix of approaches that allows for more introspection into their
workings.

As we have seen, the sort of ``understanding''
involved is limited, dealing only with static images and objects
captioned by humans in the training data. The systems would not be
able to answer other questions about the scenes that have to do with
the material properties of the objects, or likely results of various
actions.\cite{gomesJordan} It is important to understand why this technique is not a
panacea in order to understand the possibility for alternatives to the current
driverless car vision. Another recent slew of articles focuses on
the ``self-aware'' Mario created by researchers at the University of
T\"{u}bingen. Any pretense to worry about a ``self-aware AI . . . with
an insatiable desire for material wealth'' that knows ``how to kill''\cite{vincentMario}
is simply journalistic excess\footnote{As the
  researchers well know. This case is just a convenient example of how
such stories spiral out from the lab and acquire new meanings.}, and suggest
significantly more care must be taken in the use of such terms. Mario
is programmed with emotional states like a word-processing program is
programmed with different modes or display parameters. These states
are simply caricatures of emotions, and that plus the program's
natural-language interface makes it appear more eerily sci-fi than it
actually is.


\subsection{Human Supervisory Control}

—while Hutchins' cockpit
remembers its speeds through a combination of human activity and
physical cognitive aids,\cite{???} the NHTSA's “vehicle” may sometimes “not
perform a control function.”\cite{???}

ref back to NHTSA/SAE; starting from P. 9 of the DM paper: The documentation of the Google car's Nevada driving test
exposes fractures in the traditional perspectives on vehicle
automation.\cite{???} 

As a way to INTRODUCE HSC, actually

\subsection{Lessons from Autonomy Research}

\subsection{Whither Alternate Narratives?}



1) bring in the telerobotics/supervisory control literature
-why supervisory control? and where is it used? (5 p)
-examine the issue of deskilling vs. the ``irony of automation'' well
known in Human Factors, in which increased automation actually
increases human load (2 p)
-but SC allows for particular combinations of human skills and machine
competencies (1 p)
--used all over industry, aviation, undersea (2 p)


3) situate the moonshot approach (which may actually be easier in some
senses but less socially acceptable) and the mixed approach in a way
that does not support a teleology of autonomous vehicle development
(think Sheridan's graph of telerobotics, not the SAE's 5 stages) (4 p)
--in historical context of previous systems that are wholly or
partially autonomous (4 p)





