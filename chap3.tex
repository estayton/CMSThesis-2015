\chapter{Hybrid Controls, Hybrid Possibilities}
\label{chap:4}

%% 4. What is the alternative to the teleological progression implicit
%% in the for- and against- stories?
%% (start with story of planes [the backing off from full autonomy] or Mars rovers or something
%% -HSC
%% -Autonomy research from other areas
%% -envision some alternatives
%% -some stakes (data) are still applicable, but others may be
%% ameliorated

%%EDITS
%%SUCHMAN: plans and situated actions

Most automated vehicle narratives---and all those we have investigated
so far---rest on two primary, interlocking assumptions. First, the nature of the
ideal human-machine interaction for vehicle control is assumed to be
known. Second, an inevitable progression toward not only greater
autonomy but complete autonomy is assumed as the
starting point of these arguments. Rarely if ever does the question of
how much autonomy or supervision to provide to the automated system
enter the discussion as an engineering parameter over which designers
have control, and which should be responsive to larger goals. Instead,
advanced driver-assistance technologies and fully-self-driving
operation with no need for human supervision are recognized as
technologically connected, but perceived as fundamentally dichotomous
approaches. Fully-self-driving operation is considered---often via a
speculative or ideological basis---either good or not-good compared to
current vehicles with driver-assistance technologies; it is not generally
perceived as the upper bound on a spectrum of automation
approaches.\footnote{And as we will see in this chapter, even
  so-called ``full'' autonomy would not, in practice, be so simply
  disconnected from human oversight.} And through all this,
fully-automated operation is assumed to be the ultimate end goal, in a
realm of complete technological possibility: the ideal human-machine
interaction is, in a sense, no human-machine interaction. As I have
outlined, this perspective comes from particular roots (automation
history, artificial intelligence, science fiction), but is not the
only way to envision automated systems. There is a deep history of
work in human supervisory control (HSC) that has important implications
for the real-world design of automated systems, and which tells a
potentially very different story about what automated vehicle
operation will look like from a ``driver's'' or operator's point of view.

\subsection{Human Supervisory Control}

No current ``self-driving'' vehicles designed to operate on public
roadways (as opposed to in controlled conditions) operate in a fully
autonomous mode. The cars operate---and legally, can only
operate\cite{???}---in
a supervised mode, wherein a human driver is responsible for
overseeing the automated systems. Even if the vehicle is capable of
performing a maneuver ``on its own,'' that operation is monitored, at
least intermittently, by a person who can theoretically correct errors
made by the automation.\footnote{Of course there are serious
  limits to this capability, as HSC research shows, and as I will
  more fully describe later in this chapter.} This point is made not
only numerous articles describing vehicle operations in general terms,
but by Google's
own job postings, searching for ``vehicle safety specialists'' to join
the self-driving car team in Mountain View. The ideal operator will
develop ``a unique set of operational skills'' using the vehicles, and
operate ``comfortably in a fast-paced environment, sometimes managing
up to four communication channels simultaneously via various high- and
low-tech mediums.''\cite{???-googleJobPosting} This person's primary
duties include filing daily reports and monitoring operations of the
software with ``constant focus.''\footnote{While this constant focus
  is likely to be solely an artifact of the car as a device still in
  testing phases, the development of a ``unique set of operational
  skills'' is just the sort of learning one might expect to perform in
order to operate a car with a novel human-machine interface. While it
may be Google's intention that only test drivers need develop this
kind of expertise, requiring it of prospective users is not
necessarily a bad idea. While there is a reluctance to require drivers
to acquire new skills, people are already licensed before they are
allowed to drive, and the terms of that licensing may well need to
change with new technologies and new relationships to automotive
technology.} This role is clearly not one for a passive participant,
an observer who sits in the seat, lets the software run (and perhaps
self-diagnose failures), and only presses an abort button in an
emergency. The test driver is instead an active participant in the
complex process of 
vehicle operation. Further details of these drives, however,
are difficult to come by. Test drivers are, expectedly, required to
``keep all project details confidential.''\cite{???-googleJobPosting}

%%Use the Hireart job posting; looking for people who can monitor
%%comms and vehicle ops; a complex task; but the actual operations
%%shrouded in secrecy

However, in May of 2012,
Google actually tested one of these vehicles in Nevada, on public
roads, in the only government test yet conducted in the United States.
The documentation of this test---which occurred with engineers Chris
Urmson (the project lead) and Anthony Levandowski in the front
seats---exposes fractures in the traditional perspectives on vehicle
automation.\cite{harrisNevada} Even though the structure of the
checklist only breaks down operation into ``Autonomous,'' ``Driver
Assist,'' and ``Driver Only'' modes, it shows that the human driver
was required to assist, or to take control, at multiple points during
the test drive. The test records a mix of autonomous and
driver-assisted operation when the vehicle faced road construction,
switching into manual mode and requiring human assistance to continue.
These hand-overs were not limited to construction, however: 
``Wojcik [the examiner] also recorded that the car needed driver
assistance with some turns, although she did not note the
circumstances.''\cite{harrisNevada} This should not be taken
simplistically, as evidence that the system is not sufficiently
advanced. Instead, it is evidence that real operations are more
nuanced than narratives of them tend to allow for, involving mixes of
attention and control that change over time and varying road situations.

This operational mode, then, is more properly a human-supervisory mode
than an autonomous one. The study of HSC is by no means new, but
perhaps because it does not interact with human fears of obsolescense,
loss of agency, or robot apocalypse the way AI does---it doesn't stand
to damage our egos in the same way, in part because because it just
sounds rather staid and boring---it has not been as commonly
recognized or discussed in popular narratives. And yet, supervisory
control is implied any time an article mentions a human driver or
co-driver monitoring a system, or taking control at a critical moment.
However, these moments are generally implied weaknesses to the device,
as the self-driving vehicle narrative is organized around the ultimate
goal of fully autonomous robot cars. Supervisory control, however,
admits different goals and possibilities.

The core text of the supervisory control literature is Thomas
Sheridan's \emph{Telerobotics, Automation, and Human Supervisory
  Control} from 1992. Though work on such systems had been occurring
at the MIT Man-Machine Systems Laboratory---among other places---for
the previous 30 years, this book represents the first time many of the
concepts that were ``maturing'' over those years of dissertations were
brought together into one source.\cite[p. xix]{sheridan} Despite its
age, the book remains a great introduction to the field as well as a
much-needed counterpoint to other tendencies in the field of
automation.\footnote{And in fact, Sheridan is almost prescient in his
  identification of the vehicle automation technologies (Advanced
  Vehicle Control Systems, or AVCS) that would
  make it to market: all of the ones he lists we see today, and the
  last (like automatic lane keeping) are really only just now becoming
  available. Many of these technologies were in development at the
  time, as Sheridan's book came out during the middle of the Eureka
  PROMETHEUS project.} Human supervisory control history in fact
shares touch-points with the history of automation in its popular
form, but comes more out of theories of management and human factors
engineering. Frederick Winslow Taylor is identified as a key player in
the history, less for his ``dehumanizing'' approach to the worker as
his intent to generate ``a new interest in the sensorimotor aspects of
human performance''---in other words, the way that human capabilities
interact with the tools they use to accomplish the tasks they are
set\cite[p. 7]{sheridan}. Later human factors or \emph{ergonomics}
work continued to probe such questions, though many supervisory
control systems had already entered relatively common use, including
aircraft autopilots, automatic elevators, and even, perhaps arguably,
washers and dryers.\cite[p. 8]{sheridan}. Though control researchers
interested in the behaviors of humans in interactions with highly
automated systems were already pursuing similar work, supervisory
control, according to Sheridan, truly came into its own as part of
research on the teleoperation\footnote{Teleoperation means the
  extension of an operator's sensing and control capacity to a remote
  location, via an artificial assemblage.\cite[p. 4]{sheridan}} of vehicles under time delay,
specifically on the moon. The time delay enforced a fundamental
constraint on direct operation, as the results of any action require
three seconds to be reported back to Earth, and therefore made
apparent the great benefit of having the remotely-operated system
include its own internal control loop to allow it to perform simple
delegated tasks.\cite[p. 9]{sheridan}

As Sheridan describes, the concept of
supervisory control comes from
the idea of human supervision within management structures:  in an idealized
case, a human
supervisor instructs her subordinates, who carry out tasks, summarize
results, and report them back to the supervisor who decides what
further actions are required; the supervisor may
exercise various amounts of monitoring or direct control over the
actions of her subordinates, based on their skill and her trust in
their abilities. Replacing the human subordinates with computerized
components completes the basic analogy to supervisory control as it is
used here. Sheridan's definition of supervisory control in its
strictest sense is that
\begin{quote}one or more human operators are intermittently
  programming and continually receiving information from a computer
  that itself closes an autonomous control loop through artificial
  effectors and sensors to the controlled process or task
  environment\cite[p. 1]{sheridan}\end{quote}
The less-strict definition loosens the requirement that the device
close a control loop of its own, simply requiring it to interconnect
``through artificial effectors and sensors to the controlled process
or task environment'':  only in the strict case can the computer
operator without the human as an autonomous system ``for some
variables at least some of the time.''\cite[p. 1]{sheridan} This
emphasis on partial, gradiated control\footnote{The computer system
  may function primarily on the ``efferent or motor side'' to actually
implement directives from the supervisory, subject to its own
sensors.\cite[p. 3]{sheridan} Or it may act principally ``on the
display side,'' processing incoming sensory information into a form
digestible for the supervisor, or, as is usual, it may do some
both.\cite[p. 3]{sheridan} As Sheridan notes, the computer acts
independently ``at least for short periods of time,'' and the human
may assume direct control of the entire system or certain variables
within the system at various points.\cite[p. 3]{sheridan}} is emblematic of the entire HSC
project, and represents its fundamental ideological difference from
the AI-focused perspective on automated systems. This is neither a
weakness nor an unwillingness to be sufficiently bold, but a
well-considered engineering strategy. Sheridan identifes seven
motivations to develop supervisory control, of which
six\footnote{Number 5 is not relevant as stated, but is very relevant
  in principle to a distracted driver.} are
eminently relevant to self-driving cars and so I will include those
here in their entireity:
\begin{quote} (1) to achieve the accuracy and reliability of the
  machine without sacrificing the cognitive capability and
  adaptability of the human,
(2) to make control faster and unconstrained by the limited pace of
  the continuous human sensorimotor capability,
(3) to make control easier by letting the operator give instructions
  in terms of objects to be moved and goals to be met, rather than
  instructions to be used and control signals to be sent,
(4) to eliminate the demand for continuous human attention and reduce
  the operator's workload,
(5) to make control possible even where there are time delays in
  communication between human and teleoperator,
(6) to provide a ``fail-soft'' capability when failure in the
  operator's direct control would prove catastrophic\cite[p. 12]{sheridan}
\end{quote}

Though supervisory control, and human factors engineering by proxy, is
very interested in mathematically modeling the human operator in her
engagement with the control system---itself a fraught project in
several ways\footnote{People rarely behave as ideal mathematical
  functions. The black-boxing of the operator into a stimulus-response
system is something HF and HSC continue to struggle with, as it is a
tricky problem to get right. Human operators respond differently under
laboratory test conditions than they do under the stress of actual
operations, a fact that troubled X-15 designers attempting to tune
their fly-by-wire controls.\cite[p. ???]{???-digitalApollo} Though
improvements have been made in this area, as modeling operator
responses is a topic of great interest especially in aeronautical and
astronautical design, human operators continue to be
unpredictable.}---the specifics of such modeling are not necessary to
understand the supervisory control concept or the doors it opens in
understanding the operations of ``self-driving'' vehicles. To provide
a specific example of supervisory control, consider the following
situation: a highly-automated vehicle is set up to operate in a
supervised manner. The vehicle is capable of navigating traffic on its
own, but includes a map interface, a digital display a shifter that includes an
autonomous mode ``gear'' selection, a standard set of wheel and
pedals, a turn signal control,
and a cruise-control-like control stalk. The user may set a
destination on the map interface and engage the automation from a stop, or engage
the automation while the vehicle is in motion. The user is expected to
be available to assist the vehicle with maneuvers, and oversee its
behavior:  she may take the wheel at any time to direct the vehicle,
or use the pedals to force its speed to alter; she may request lane
changes using the lane signal stalk; she may use the cruise-control
stalk to subtly alter the vehicle's speed to suit traffic and
conditions. At any time when the automation is engaged, she may alter
the destination on the map. When the automation is engaged, the
vehicle will warn if it is encountering a situation it cannot handle,
and will revert to a minimal-risk condition if the operator does not
intervene (e.g. pull to the shoulder and slowly come to a stop).
Whether or not the automation is engaged,
certain ADAS or AVCS are operating in closed-loop mode, including
pedestrian collision detection. And vehicle data including detected
objects and planned paths through the environment are always being
presented on the digital display to assist the user in evaluating the
environment and determining vehicle intent.

This hypothetical vehicle functions in a clearly supervisory mode,
since high-level commands can be provided by the human operator, to be
carried out by the automation in accordance with its sensors.
Information from the environment is processed by the vehicle and
returned to the user via the display, providing the user with more
cues as to the environment and indications as to the status of the
automated system. Such a vehicle bears little resemblance to the
self-driving vehicle envisioned by Google, but looks quite like a
current vehicle might after a decade of evolutionary development of
driver assistance systems. Furthermore, it manages to address all of
Sheridan's supervisory control motivations. It combines the constant
vigilance of automated sensing with the judgment and perception of the
human operator (1 and 2); it allows the operator to provide high level
controls in autonomous mode, instructing the vehicle which lane to be
in or which turns to make without having to maneuver manually (3); it
allows the operator to pay intermittent attention to the vehicle by
including fail-safe modes and the ability to handle most driving
situations (4, 5); in the case of a time delay in response (e.g. due
to drowsyness) or an
operator's failure to control, the vehicle will attempt to behave
safely (5 and 6). While actually designing and engineering such a
system is by no means as simple as sketching it briefly as I have, it
should be clear that a vehicle of this description is not only
potentially very useful, but is at least as plausible as a fully
autonomous robotic car. It presents unique problems of human
interaction and attention which should not go unremarked upon (and
real-world examples of these problems will be dealt with later in this
section), but it also presents unique opportunities for blended
capabilities that may not only compensate for deficiencies in computer
vision, mapping, or automated sensing, but may do much to address
human discomfort with automation systems and concern with being
outside of the control loop of their automated vehicle.

With our attention on supervisory control, new questions and processes
come into focus. If the machine is clearly no
longer the sole component of analysis, what formerly neglected pieces
must be considered? How do we assess system design or performance, or
think about the ways these newly expanded devices operate? In this
pursuit it is worthwhile to expand our
view to encompass other related work. As Sheridan notes, much study of
supervisory control has gone on elsewhere under different names, but
with similar guiding concepts. Coming instead from Cognitive
Science\footnote{And, notably, responding to the canonical focus of
  cognitive science on the ``mental processes that organize the
  behavior'' of an individual, a position Hutchins cites as having its
standard statement in Newell and Simon's 1972 \emph{Human problem
  solving}.\cite[p. 265-266]{hutchinsCockpit} These AI pioneers show
up again here, and their individualist focus perhaps sheds some 
light on why the narratives of automated vehicle technology that are
inflected by AI are so different than they would be if told instead
through the lens of supervisory control.},
Edwin Hutchins and his anthropological work on group cognition
processes expands from the traditional focus on the individual agent
to systems of interacting agents and technologies. As he identifies,
the outcomes of tasks are not determined by individual components of a
system but by the overall dynamics of the system, and the behaviors of
a complex thinking system cannot be ``inferred from the properties of
individual agents, alone,'' irrespective of the detail to which those
individuals may be studied or modeled.\cite[p. 265]{hutchinsCockpit}
Hutchins's ``How a Cockpit Remembers Its Speeds'' lays out a framework
for understanding the cognitive activities of a human-machine system,
and uses the example of an airline cockpit to show how the cognitive
properties of a system as a whole may be very different than those of
individual human actors within that system.\footnote{This is also true
for systems that do not actually contain ``automation'' technologies
as part of their make-up. The speed-bugs discussed in ``How a Cockpit
Remembers Its Speeds'' are rudimentary in terms of other forms of
cockpit automation, but the issues they present do not disappear with
increased automation. Hutchins and Klausen co-wrote another
article in 1995 titled ``Distributed Cognition in an Airline
Cockpit,''\cite{???-hutchinsKlausen} which focuses on the interactions of the Captain, first
officer, second officer, and air-traffic control. This exchange,
though it involves control yokes, altitude alerts, and other
technological actants, is primarily focused on the joint capabilities
of the human crew.} Studying the humans alone,
or the automation alone, does not suffice to explain the overall
behavior of a joint human-machine cognitive system.\footnote{It is
  worth noting here that these are not the remarks of a maverick or a
  ``mere'' anthropologist working on his own. The research involved
  was supported by a grant from the NASA Ames Research Center, as part
  of the Aviation Safety/Automation Program.}

Hutchins is very particular about the point that the speed bugs and
other cockpit devices function as more than just memory aids for the
pilots. They do not na\"{\i}vely increase pilot memory. Instead, they
are information processing tools, responsible for changing the form of
information, giving it a new representation and altering the
interaction with that information: specifically, changing a task that
requires memory and comparison to a readout to a simpler task of
``judgments of spatial proximity'' of the needle to the speed
bugs.\cite[p. 282]{hutchinsCockpit}. The whole system can be said to
have a ``memory'' that is distinct from the pilot's memory: the
``\emph{cockpit system} remembers speeds'' by virtue of the pilots
within that system judging a needle against the bug.\cite[p.
  283]{hutchinsCockpit} I introduce this material here because
Hutchins, by looking at cognitive behaviors within this particular
setting, identifies a new unit of analysis: the cockpit as a whole.
Automation systems are more complex than the cockpit's
visual-mechanical aides, but are appropriate subjects of the same kind
of analysis.\footnote{As an aside, I would argue that if a number of
  modern information technologies, like smartphones, seem to have done
relatively little to alter human beings, it is because the individual
is still the unit of analysis. When the unit of analysis is instead
set at the individual plus his or her immediate technological
surroundings, much greater differences in capability become apparent.}
Automation likewise takes complicated jobs involving spatial
processing and reasoning (driving between the lines and avoiding other
cars), and transforms them through technological interfaces into other
tasks. It is precisely these changes in task to which we should attend
if we would like to properly understand (and design) the role of the
operator within an automated vehicle. But this perspective is not
represented in the prevailing guides for automated system development.
While Hutchins's cockpit remembers its speeds through a combination of
human activity and physical cognitive aids, the NHTSA's ``vehicle''
may sometimes ``not perform a control function''\cite[p. 3]{NHTSA} and
nevertheless remain on the road. The NHTSA's documentation instead
focuses simplistically on whether the human or the ``vehicle''
controls, rather than addressing what control involves or how it may
be shared between human and automated systems.

A more nuanced view of the cognitive tasks involved in interacting
with automated systems also goes quite a ways toward clarifying some
of the more difficult parts of system automation. Specifically,
significant research interest in human factors engineering focuses on
the problem of so-called ironies of automation, whereby increasing automation actually
increases the load on the operator, rather than decreasing it, and
leads to increased failure rather than increased
reliability.\cite{???-somethingin-hutchins} It might
not be clear at first glance how this would be, and it is in part an
interface problem---with how information is presented to the operator,
not with the concept of automation itself. But a distributed,
cognitive attention to automation highlights the problem:  an
automation system may transform a task from a complex visual task for the
operator (e.g. driving) to another complex visual task (e.g. monitoring a control
panel with many displays and lights, and switches to press) while the
overall processes within and results of the system remain
fundamentally congruent---the system still ``knows'' how to turn, for
example, but instead of the driver turning the wheel, she interacts
with the automation systems in cognitively-intense way. Attending to
the complex internal dynamics of
the joint system improves the analysis: looking at which tasks are
allocated where, and what is the demand on human perceptual and
decision-making systems, at any particular time. As one should expect,
people drive largely based on experience and instinct,\cite{article-http://www.telegraph.co.uk/news/science/science-news/11410261/Driverless-car-beats-racing-driver-for-first-time.html} not logical
thought, and similarly system performance should be expected to shift
over time as interactions with automation are internalized. As
Hutchins and Klausen describe, ``It is possible to design computer systems with open
interfaces (Hutchins, 1990) that support learning in joint action but
this can only be done when the designer goes beyond the conception of
the isolated individual user.''\cite[p. 13]{hutchinsKlausen}
Appropriately recognizing interactions is critical to successful design.

Not to belabor the point, however, the perspective I am describing is
not fundamentally new---simply, its importance is not often
recognized outside of particular disciplinary boundaries. Supervisory control and 
joint-cognitive systems approaches are actually in use all over
industry, even in cases where they are not necessarily cast in
those terms. 

<??? Describe some from Sheridan 10-11 or run paragraphs together>

 While it is also
implicit in much popular writing about autonomous systems, however,
supervisory control is not generally acknowledged as an important and
developed field. Human supervisory control is not even that well
considered in many engineering fields; even today, the human factors
engineer is often low in status within the engineering hierarchy, and
is too often brought in at the last minute to design an interface for
an already specified device,\footnote{This subtlety of the politics
  within engineering communities themselves was not something I was
  previously aware of, but was made clear to me by David Mindell.
  Discussion with the author, September 8, 2014.} rather than actually being involved in
device design from the beginning, as effective supervisory control
requires. While the related discipline of interaction design or
user-experience design is gaining in importance in the information
technology sector\footnote{This I can speak to personally, through my
  involvement with programming communities building web and mobile
  applications. Demand for UI/UX designers has increased dramatically,
following the success of design-first product development by IT
companies, most notably Apple.}, this too is often (though not always) a surface-level
consideration of the interface, without consideration of deeper
dynamics of human and joint cognitive processes. It may be
that this kind of engineering work gains greater recognition with the
entry of Google, Apple, Tesla, and other high-technology companies
into the autonomous vehicle space, but so far Google's seems to dream
of minimizing the interface as much as possible, rather than more
broadly considering what sort of interface makes sense.\cite{???}
%%Wrap up: Actual use all over industry (Sheridan 10-11), and implicit
%in much popular writing, 
%just not generally acknowledged as an important field in all its complexity
%HSC not even that well considered/thought out in many fields; HF
%engineers low on status and too often just brought in at the last
%minute to design an interface rather than actually being involved in
%device design from the beginning.

Hutchins and Klausen make it a particular point to highlight that
human-machine dynamics of information transfer are at the center of
the appropriate and safe operation of aircraft: ``if we step back and
look at the entire aviation system and ask how
it is that aircraft are kept separated from each other, we see that it
is through the propagation of representational state of descriptions
of flight paths into the state of the aircraft controls
themselves.''\cite[p. 14]{hutchinsKlausen} This situation is not quite
the same for cars, which are generally not orchestrated at a
system-level in the same fashion. But in principle, the roadway presents a related
information processing situation. The propagation of a ``plan'' of the
vehicle's path on the 
road into the state of the vehicle's controls keeps vehicles apart.
And even when that vehicle system involves computerized
displays, sensors, and people, the same point holds;  at a high-level of oversight, as
one would find represented in an employee monitoring an automated vehicle fleet remotely
from an office, the parallelism with aircraft operations returns in
full force. Given these underlying
similarities in terms of human-machine interaction, let us look at
other examples of automated technologies (in space, underwater, and in
the air) to see how previous sets of designers and users have
negotiated compromises of automation, autonomy, and human-control.



%% As a way to INTRODUCE HSC, actually

%% 1) bring in the telerobotics/supervisory control literature
%% -why supervisory control? and where is it used? (5 p)
%% -examine the issue of deskilling vs. the ``irony of automation'' well
%% known in Human Factors, in which increased automation actually
%% increases human load (2 p)
%% -but SC allows for particular combinations of human skills and machine
%% competencies (1 p)
%% --used all over industry, aviation, undersea (2 p)




\subsection{Lessons from Autonomy Research}

Spaceflight, both manned and unmanned, provides myriad reasons to
invest in automated systems: time delays prevent or at least greatly
inhibit remote control from Earth; conditions where humans are not
uniquely equipped to survive suggest the use of mechanical explorers
instead; and precise control functions with redundant backup systems
suggest the use of the unique capacities of computerized systems to
monitor constantly and act immediately in the even of an emergency.
And these are all senses with which the narrative of automation in
spaceflight is inflected today. Certainly unmanned spaceflight had a
strong interest in automation since at least the 1950s, with serious
research being done on simple automated probes to do a fly-by of
Mars.\cite{???-DMsclass} But the story of automation in spaceflight is
not nearly so simple as it appears. Rather than being the ultimate and
obvious endpoint of a progression of human engineering in space, the
roles and implementations of automation remained fraught, highly
contested by astronauts themselves due to professional pride, national
politics, evidence for the capability of the human being to be in
control, and the dogma among certain groups that such control is
necessarily more reliable than computerized automation---which has not
a little to do with the fragility of pre-integrated-circuit computer
technologies.

From 1952 to 1954, Colliers magazine published a series of articles
titled \emph{Man Will Conquer Space Soon} which described Wernher Von
Braun's vision for manned spaceflight.\cite{???-wiki} Numerous
articles focused, not surprisingly, on the people who would do the
voyaging, how they would be selected, tested, and kept alive. But
lurking behind this majestic picture of manned spaceflight was a dark
realization for prospective astronauts: to von Braun himself, the
astronaut would be a mere passenger, ferried into space by automated
rockets. In a speech to the Society of Experimental Test Pilots in
August of 1959, a hostile audience for this sort of rhetoric, von
Braun emphasized that human control in rocketry is ``actually
undesirable'' because human beings are ``outrageously slow and
cumbersome'' in missile terms.\cite[p. 66-67]{???-DM} While von Braun
may have been the most polemical, he was not alone in his cautions to
test pilots about the limits of their capacities.
In an earlier speech to the SETP (October 4, 1957) on automation and
the ``survival'' of the test pilot as a profession, Richard Horner
took a balanced and moderate view on human involvement, but warned
that technology would progress faster than human beings change:
the ``link'' that improves the least ``is the man himself''\cite[p. 19]{???-DM}>

Pilots---prospective astronauts---were not about to find themselves
cut out of the control loop, and for good reasons. Not only was their
personal pride at stake---as military test pilots, they possessed a
particular standing in part by virtue of being in control of dangerous
and cutting-edge vehicles like the X-15, which though it necessarily
incorporated significant automation to deal with hypersonic flight and
the transition from atmospheric to extra-atmospheric operation, kept
the pilot firmly in command\footnote{Kelly Johnson, the leading aircraft
designer in the US, working at Lockheed, objected to the manned
component of the X-15
project, but his objections were overruled by the NACA.\cite[p.
  46]{???-DM}} through fly-by-wire controls\cite{???}---they had 
every reason to doubt the capabilities of computerized guidance and
control systems.\footnote{Even the fly-by-wire control was somewhat
  controversial. Milt Thompson, a test pilot in the X-15 program, said
  of it: ``you would like him [the engineer responsible] to be in the
airplane with you to be exposed to any adverse results.''\cite[p.
  55]{???-DM}} Al Blackburn's response to Von Braun's speech to the
SETP was particularly empassioned, and recounted his many experiences with
``brain-dead autopilots, broken fire control systems, and failed cockpit
computers''\cite[p. 68]{???-DM} From
his admittedly partial perspective, the resourcefulness of human
pilots is the answer to automated technology's lack of reliability.

Such debates over control did not go away, but lasted through the each
of the Mercury, Gemini, and Apollo programs.Mercury, which began the
manned space program in the United States, had an ambivalent
relationship to its astronauts. They were pilots, rather than von
Braun's ``missile riders,'' but only just. While Mercury preserved an
aeronautical joystick, which controlled the reaction control thrusters
(RCS) and thereby allowed the astronauts attitude control over their
space ``capsule''---nomenclature that would soon find itself under
siege\cite{???-DMsclass}---they could not fly the Mercury craft in any
aeronautical sense. Like the warheads of the ballistic missiles from
which the program spawned---and which were in fact used to convey the
Mercury astronauts into space---the astronauts were launched by
automated rockets, and returned to the ground on a purely ballistic
trajectory. But the presence of the human astronaut was greatly
important to the Mercury project, even if he (they were all men)
largely flew as ``Spam in a tin can.''\cite{???} Putting a man in
space was a political venture more than a scientific one or a matter
of national defense.\footnote{Scientists held and continue to hold
  that unmanned missions are more effective in generating scientific
  discoveries than manned ones.} In the eyes of the world---or at
least as interpreted by Washington---the
reputation of the United States depended on it. The shibboleth
``third world man'' was waiting on the outcome of the space race to
decide whether to be communist or capitalist.\cite{???} Apollo was, among many
other things, a nationalist (and even perhaps imperial)  PR campaign.
Spurred on by Russian successes (Sputnik, Gargarin), which were
originally of little political interest until the media reporting on
the events presented them in a frantic light, as a possible sign of
Soviet technological dominance,\footnote{There was little coverage in
  Russian papers about the Sputnik launch, and even Gargarin was not
  greatly covered. However, the media frenzy that ensued in the United
States and other countries worldwide made clear the political
importance of being first in space.\cite{???}} Kennedy pushed for a greater
investment in space research. But he was ``not that interested in
space''\footnote{He continued: ``I think it's nice, I think we should
  find out about it,'' but balked at the level of funding necessary
  for the manned space program unless it led to a US political and PR
  victory.}---primarily, he wanted to beat the Russians to some big,
visible space project.\cite{???-whitehousetapes} This project had to
be manned, precisely because human spaceflight captured peoples
imaginations, and because the Soviets were certain to continue their
piloted space flight ventures. But Soviet spacecraft continued to be
more automated than their Western counterparts---Gargarin could not
operate manual backup functionality without unlocking a combination
lock, the
code to which was only reluctantly placed in the cockpit in a sealed
envelope rather than being radioed from the ground\cite[p. 89]{???-DM}---and
this automation was seen as a feminization of space:  Soviet
spacecraft required no skill to operate, and were therefore,
rhetorically at least, less impressive.\cite{???} This reading was
supported some years later in 1963 by the flight of Valentina
Tereshkova, the 
first woman in space (a milestone not followed by the United States
until the 1970s). This approach perhaps fit a communist ethos, sending
the representatives of every-man and every-woman into
space,\footnote{Neither Gargarin nor Tereshkova had aristocratic
  family backgrounds.} but did
not fit with the ideally meritocratic culture and the political aims
of the US program, not to mention the flying culture of the astronauts
ultimately selected for the program.\footnote{All those selected for
  the Mercury program were military test pilots, despite NASA's
  existence as a civilian agency.\cite{???-DM} Issues of secrecy alone
should not have disqualified other military personnel, and a greater
emphasis on science in the program would have suggested actually
including scientists on flight teams.\cite{???-DM} Though over the recruitment
rounds the emphasis on test-pilot status decreased in favor of greater
engineering knowledge (and higher degrees), and though one geologist
set foot on the moon during the final Apollo mission, the astronaut
culture was dominated by influences from the test pilot community.}

%% Mercury is a tin can with attitude control
%% -but, somewhat conversely, the position of the person in spaceflight
%% was very important politically
%% -Kennedy ``not that interested in space''; wanted to beat the Russians
%% with something BIG
%% -Russian space automation was a sort of feminiziation of space; their
%% astronauts required no skill
%% ---perhaps fit a communist ethos (sending the everyman (or woman)
%% into space), but not a meritocratic, capitalist one

%%%%WARNING CITATION STYLE CHANGE%%%%%%%%

The development of the Gemini spacecraft---actually started after the
Apollo project, intended to demonstrate capabilities necessary for
a mission to the moon to take place---set out to extend the
capabilities of Mercury, and incorporate lessons learned from the
precursor program. One way this occurred was specifically technical, a
re-engineering of the space capsule to allow systems to be installed
and serviced from the outside of the craft\cite{???}. But another
reorchestration of space flight in the Gemini program involves the
human's interaction with the spacecraft through control systems.
Pilots wanted a role in launch vehicle guidance, and a number of
simulator studies were commissioned to study the capacity of human to
actually guide boosters into orbit\cite{???}. Pilots, spun in a
centrifuge in Johnsville PA to simulate the immense acceleration of
takeoff, flew
simulated rockets: while some tests failed, with astronauts losing
control during stage changes, some were able to launch the vehicle
into orbit, and these tests served as evidence for the importance of
pilots to the ``reliability and flexibility'' of launch vehicles,
the same phrasing used to justify the pilot's role in the X-15
testing\cite[p. 72]{???-DM}. Some even argued that the lack of pilots
explained the high failure rate of automated rockets\cite[p.
  73]{???-DM}. Though no American would ever fly the launch stage of
the rocket, pilots continued to push for control. Robert Voas,
responsible for astronaut selection and training, was a firm believer
in the human role both for monitoring and managing automated systems,
but also 
navigation, communications, and performing research\cite[p.
  77]{???-DM}. Monitored automation, which prevailed in Mercury---with
the human ``more than secondary, if still less than primary''\cite[p.
  77]{???-DM}---continued into the Gemini program. The Gemini pilot
interfaced with an inertial navigation system, a digital computer, and
an optical star tracker, responsible for tasks that would be too heavy
or too difficult to automate, and remaining as a backup for automated
systems\cite[p. 83]{???-DM}. But Mindell holds that advancing
technology was seen to be served by the inclusion of more human
control, which Mercury (in association with simulator tests) had proved was
feasible\cite[p. 84]{???-DM}. The importance of orbital maneuvers,
rendezvous and docking, for the Gemini missions suggested returning
the role of piloting to the spacecraft pilot, and rendezvous was
indeed performed with manual control\cite[p. 84]{???-DM}. But, as
pilots (particularly, Jim McDivitt) soon discovered, orbital
rendezvous could not be achieved by the
traditional manner of flying: in orbit, flying toward an object is not
an appropriate way to achieve rendezvous, which requires not only
passing close to the target but remaining there for an extended
period. Matching velocity means matching orbits, and so ``catching
up'' to a target may require going slower to change orbits\cite[p.
  86]{???-DM} ``Numbers, equations, and calculations'' would be
required, and were bootstrapped to the pilot's senses with a new
readout, the IVI or Incremental Velocity Indicator, which could be
programmed with particular velocity changed in different axes and
would show visually when those particular burns had been achieved,
so that the human pilot would know when to stop accelerating\cite[p.
  86-87]{???-DM} Pilots did not fight the implementation of such
technology, realizing that anything that ``helped them manage the
chaos [of space flight] would extend their ability,'' and overall
liked piloting the Gemini craft, which they saw as expressing the
``national character'' through the importance of the individual pilot
to the vehicle's operations\cite[p. 88]{???-DM} Gemini is 
a triumph of manual control, aided of course by automated read-outs.

The Apollo program started too soon to learn the lessons of
Gemini---the Apollo system design was complete by the time Gemini
missions actually flew\cite[p. 93]{???-DM}---but despite the
ultimately high levels of automation present with the Apollo
spacecraft, arguments about human role shaped development on this
parallel program. One major locus of such controversy was the Apollo
guidance computer: Should it allow fully autonomous (from Earth)
operation? What functions should be fully automated? The idea of an
autonomous navigation system was not fundamentally new. The Apollo
project pulled from previous experience in digital computer design and
development, both from military ICBM programs which often used
self-contained inertial guidance units, and from work by Laning and
Trageser from the Instrumentation Lab at MIT on a small Mars probe,
driven by ``self-dependent'' navigation, which could ideally use
complex logic to respond autonomously to problems\cite[p.
  99-100]{???-DM} Though self-contained navigation capacity was a goal
for the engineers at MIT responsible for the guidance system---working
under Charles Stark Draper, who was also involved in developing
guidance systems for ICBMs---much of this functionality was ultimately
removed due to computer memory restrictions\cite{???}. The Apollo
spacecraft would have to be in contact with Earth for guidance
corrections and instructions over the long term. Human beings were
again counted on for sextant readings and for initiating appropriate
program modes and monitoring automation systems, and the spacecraft
could operate in a self-contained fashion for some length of
time\cite{???}. While the initial idea for lunar landing was that the
computer would guide the spacecraft down, given appropriate landmark
fixes from pilots, this was not how the LM was flown: despite
possessing an autopilot capable of automatic landing, pilots used
their prerogative of ultimate control to take over at various stages
of landing: Armstrong, famously, took manual control on the Apollo 11
landing to seek out a cleaner landing spot and avoid a potentially
dangerous crater\cite[p. 3]{???-DM} While the Apollo engineers perhaps
got away with a lot of automation, in part because the astronauts were
busy with Gemini instead, manual control still proved critical to the
overall operation of the system. The mission depended on human and
automated systems both, and the successful integration of their roles
and capabilities. When conceived of as a model for automated vehicle
engineering, the history of manned space programs does not fit the
popular narrative of automation, instead presenting a clear example of
joint human-machine systems in action.

While these manned missions were occurring, Jerome Wienser,
Presidential science advisor and later President of MIT, and other
scientists were perennially against manned
spaceflight.\footnote{Wiesner not only disagreed with the importance
  of manned space flight, he took issues with the decision to perform
  Lunar Orbit Rendezvous, LOR, as the Apollo mission mode. He
  continued to oppose LOR after the official NASA announcement, and
  argued with von Braun in front of the
  press at Marshall Space Flight Center, until President Kennedy
  stepped in to end the argument\cite{???}.} Unmanned exploration is
cheaper by orders of magnitude\cite[p. 66]{???-coxMurray}. To serve
scientific goals, humans are not required, and in his role as chair of
the Presidential Science Advisory Committee, Wiesner felt that his
duty was to argue against the work on scientific
grounds\cite{???}.\footnote{While Killian's list of goals for the
  space program included the human need to explore, national defense,
  national pride, and scientific experiment, and while the
  congressional hearings about the space program said little about
  defense, trading it for discussion of propaganda value and a
  ``vertiginous rhetoric'' of scientific exploration, manned space
  exploration is in general not justifiable by scientific ends\cite[p.
    194-197]{smithSelling}.} Instead of a manned program, scientists
have continued to push for unmanned probes,\footnote{And these probes
  actually get leveraged in the popular narrative as sources for
  automated vehicle design inspiration: `` And so Google’s new vehicle
  design 
takes a leaf out of NASA’s design book to cope with such
eventualities. 'It doesn’t have a fallback to human—it has redundant
systems,' said Fairfield. 'It has two steering motors, and we have
various ways we can bring it to a stop'''\cite{simonite}. This is a
very reductive view even of the most remote of NASA's unmanned space
systems, let alone manned systems with which the Google vehicles must share
the critical characteristic of containing human occupants.} but even for
unmanned exploration systems in space, autonomy is not the
end-all-be-all goal. More important is the link between human
operators and scientists on the ground, and the remote science
platform responsible for carrying out instructions. 

While people may
think of the Mars rovers \emph{Spirit} and \emph{Opportunity} as
autonomous robots remotely carrying out science experiments---as
Clancey describes them often showing up anthropomorphized as a ``robot
geologist'' or 
``explorer'' in news coverage\cite[p. 7]{???-clancey}---they are
in reality telerobotic systems in Sheridan's mold. The rovers are not
sent out to wander or find goals by themselves. They drive mostly
blind, with only their immediate obstacle detection, following manual
waypoints entered by human navigators on Earth: their autonomous
path-finding systems are much slower, and use more power, and are
therefore generally avoided\cite[p. 118]{???-clancey} While the rovers
are out of contact for the span of two weeks during solar
conjunctions, the instruments lie dormant and the rovers sit
still\cite[p. 25]{???-clancey}. Though the rovers are technically capable of
carrying out pre-planned science sequences during this time, including
automatic navigation, this capability is not used. Human observation,
human teleoperation, is too important a part of the process, and
autonomous operation without monitoring presents too many risks. Human
scientists define sites of interest, locations to take samples, and
the paths to reach them most safely and effectively. Scientists
contacted the rovers at least once per day to relay plans, and again
to retrieve results, with more intense schedules during the early
parts of the program\cite[p. 58]{???-clancey}. Small sets of
operations are requested, and the outcomes monitored, with new plans
being made by humans to account for new data at each step.

This in some ways
represents greater engagement between scientists and the remote
machine than during the earlier \emph{Viking} mission, in which weeks
were spent writing programs to run the robot.\cite[p. 58]{???-clancey}
The recent missions step back
from programmatic operation, and toward closely-coupled human
supervised control. The Mars rovers could
certainly have been made more autonomous, but this would have opposed
their function. The quality of the mission depends on ``aspects of the
MER's design that promote the \emph{agency} of the scientists''
themselves, rather than automated operation specifically\cite[p.
  xii]{???-clancey}. Scientific work in the field is ``opportunistic,
serendipitous, and incremental,''\cite[p. 32]{???-clancey} yielding
not so much to \emph{a priori}
plans of great detail, but Suchman's ``situated
action''\cite{???-suchman}. Researchers on the ground actually
experience a sense of ``telepresence'' through these ``synergistic'' machines, created
by virtue of their closely-coupled operations and the MER's
semi-anthropomorphic bodies\cite[p. 55]{???-clancey}. Scientists see
as if they are the rover, and have to ``retool'' their thinking, to
become the ``mind'' of the rover and plan its work in a ``symbiotic''
way\cite[p. 106, 110, 118]{???-clancey}. The system's autonomy does not replace the
scientists, but allows them to do more of what they want to do and are
best at, and less of what they don't: some types of automation could reduce the
autonomy of the scientists, and their ability to act creatively and
spontaneous to capitalize on new findings\cite[p.
  118-119]{???-clancey}. 

As Clancey suggests, autonomy is not an ``inherent property of
technology'' but ``a relation between people, technology, and a
task-environment'' and should be considered in those terms\cite[p.
  119]{???-clancey}. While we might say, colloquially, that
``\emph{Opportunity} encountered'' something,\cite[p. 8]{???-clancey} this
is really just convenient shorthand for scientists on Earth encountering it
through and in concert with the telerobotic platform. Human knowledge,
perception, and common sense is integral to rover missions, and
organizations operating expensive 
technology in a high-risk environment quite reasonably want humans to
be responsible
for the well-being of the equipment. The same issues play out with
robotic vehicles for underwater exploration. The most famous of these,
Alvin and Jason, deeply involve human scientists as operators in real
time, either directly from within the vehicle or through a tethered
link.\cite{???} Most AUVs (or Autonomous Underwater Vehicles) are not
truly autonomous, but are operated remotely using low-bandwidth data
links via acoustic communications.\footnote{David Mindell, discussion
  with the author, September 3, 2014} This is largely an issue of risk
aversion. While it has often been assumed, even sometimes by system
designers, that long duration missions will be performed autonomously,
this is not the primary mode in which AUVs are operated: the last
thing you want to happen, as a scientist presiding over an expensive
piece of equipment, is to lose the vehicle irretrievably or have an
instrument fail on the first day and not know about it for weeks. So
instead of long, autonomous missions, most operations involve many
shorter missions with acoustic links coupling the device with
shipboard researchers.\footnote{David Mindell, discussion with the
  author, September 3, 2014} While self-driving cars would not
be much like Mars rovers or underwater robots, telerobotic exploration
provides potential lessons
to be learned in terms of human and machine roles, and the level of
autonomy one wants from a machine in a human-machine relationship.
Considering what type of interaction promotes the agency
of human owners and occupants---allowing them to do more of what they
want and less of what they don't---is particularly important,
especially while recognizing that the answer to what humans want to do
is not necessarily ``nothing.'' And while local streets are not as
remote and inaccessible as Mars or the bottom of the ocean, having
one's vehicle go missing on the way to or from a parking garage while
operating autonomously would be a significant issue, and the risk of
this kind of failure must be considered in the human-machine system
design. Cars are still capital-intensive pieces of equipment, and we
generally want to know where they are at all times. Whether vehicles
are monitored
from within or without, it seems unlikely designers will be able to
escape the need to loop the human in to the decision-making process.

But there is reason to suggest that even this picture of supervision
and monitoring on a long timescale may be too ambitious for automated
vehicles of the near future. As we have discussed, a number of major
engineering problems remain to be solved in order to build cars that
are capable of fully automated operation in all reasonable situations,
which implies more than occasional supervision is required: human engagement in
sensing and reacting, on reasonable time scales, may be necessary. The
specter that we have been dodging so far, throughout this chapter and
indeed through much of the thesis, is the issue of human attention.
Issues with attention have always been implicit within the discussion,
since inattention is precisely the human quality that makes
self-driving vehicles most necessary, at least rhetorically. But human
supervisory control, in its practical applications, has much to teach
us about the limits of human attention, and indeed such human-factors
research is the source of the NHTSA's claim that users of
higher-levels of automation will not, and will not be expected to,
monitor its operation. An often-used justification
for entirely eliminating the human from the vehicle control loop is
related to this tendency not to monitor the automation system:
research shows that human operators who become reliant on the
automation to perform a task are ill-equipped to take that task back
in a crisis situation, and task hand-over is likely to be
catastrophic.\cite{???} A transportation systems researcher I spoke to
even cited this as the key reason he felt that highway-only
self-driving systems would not work, and that companies would have to
design for full automation everywhere.\footnote{How much do I say,
  safely?} 

This issue has so far played out most visibly in airplane cockpit
automation, and especially given the role that autopilots have as
models of ground vehicle automation, it is worth examining human
supervisory control in this practical
context. As we have found previously when examining automation
technologies, however, actual autopilot design and use differs 
significantly from its popular representation. It is not a fully automated
system that controls the entire aircraft, but a set of specific tools
leveraged by pilots to increase their agency, similarly to how
geologists use the automated capabilities of the Mars rovers to
increase theirs. Autopilots are also complex systems, with many modes
that require deep technical knowledge and training to use
appropriately\cite{???}---and which, when poorly designed from a human factors
angle, as they have often been, can behave in ways that are unexpected
by and antithetical to the wishes of the pilots. Stories of crashes
due to autopilot mode confusion abound, but one of the most important
incidents to consider in our context is the Air France Flight 447 crash. AF
447 took off from Rio de Janiero, Brazil on its way to Paris, crashing
over the Atlantic Ocean at 02:14 UTC on June 1, 2009. The official
report, published in 2012, identifies the cause of the accident as a
failed hand-over of control from the autopilot system to the pilots:
when the autopilot system shut down due to a failure of its airspeed
indicators, the pilots were not prepared to so suddenly resume the
task of flying a large airliner in poor conditions.\cite{BEA}
Seemingly confused by the situation they were in, the pilot and
co-pilot provided contradictory control inputs to the aircraft and
stalled the plane, causing it to crash. 

This crash brings a number of human supervisory control questions to
the fore. What is the proper role of pilots? And how quickly can they
be expected to take back control authority over particular aspects of
aircraft operation that have been previously delegated to an automated
system? Are human pilots so unreliable that we should immediately push
toward automating all aircraft operations? Or are autopilots so
finicky that we should emphasize manual flying skills and reduce or
eliminate the use of autopilots. Most likely, the correct answer is
somewhere in between. But this concern with cockpit automation is not
new, and pilots and aircraft designers have been negotiating human
roles in flying machines since the very early days of aviation. The
tension between stable and unstable aircraft design---will the
aircraft ``fly itself'' in a given orientation or does it require
constant control inputs to maintain course---goes back as far as the
Wright Flyer, as do the competing professional identities that
accompany them: are you simply a chauffeur, or a ``true
airman''\cite[p. 21]{???-DM}? The Wright's focus on the operator's
skill created the unique profession of the
``pilot,'' and unstable aircraft were the standard for years, until
human fatigue over long periods of flight changed the importance of
stability to aircraft operation\cite[p. 22-24]{???-DM}. Stability,
this subsequent virtue of a well-engineered airframe had to be again
renegotiated in the context of supersonic flight, where stability
problems related to supersonic airflow required electronic
solutions\cite[p. 33]{???-DM}. But there was deep concern among the
flying community about black boxes that are out of the pilot's
control, and some saw these devices as ``band-aids'' for engineers not
``skilled'' enough to make the aircraft stable\cite[p. 35]{???-DM}. J.
O. Roberts' ``The Case Against Automation in Manned Fighter Aircraft''
argued for more emphasis on displaying information to pilots, rather
than using automation to assuming control\cite[p. 35]{???-DM}.
Nevertheless, cockpit automation has continued to increase. Today,
many airlines 
have guidelines that require automation systems to be used
whenever possible\cite{???}. This parallels some developments
in the automotive space: deep controversies exist among automotive
enthusiasts about the proper roles of all-wheel-drive, traction
control, and automatic transmissions in the driving process. Even ABS,
generally accepted today as a positive technology that enhances safety and
performance, is in some senses controversial, driven by the idea that
a professional driver utilizing fully manual ``threshold braking,''
developed through deep experience and human skill, can outperform the
computerized system\cite{???}. New performance vehicles---like the
highly-computerized Nissan GT-R, with a plethora of all-wheel-drive
systems---are criticized by some as ``too easy'' to drive quickly\cite{???}.
An overly-computerized vehicle is to some soulless, unexciting, too much like
a video game: these vehicles may derisively be said to be for the
PlayStation generation, even while others argue for the superior
abilities that advanced technologies convey upon human
drivers,\footnote{This community has stumbled upon the point of
  human-supervisory control, and a rudimentary sort of cyborg
  identity, perhaps without even realizing it. Rather than holding the
machine or the human as prime, the human gains through their
interaction with the technology.}
turning an average driver into a hero or track-star\cite{???}.

Computerized aids are sites of contradictory feelings and pressures.
Cruise control, when first introduced commercially as Chrysler's ``Auto-Pilot''
was described as ``faintly
ominious'' by \emph{Popular Science}, but nevertheless seemed like a
``genuine help'' for reducing
fatigue\cite{???http://www.brookings.edu/research/papers/2014/04/products-liability-driverless-cars-villasenor}.
It is well enshrined within legal principles that drivers using
cruise control and even automated driver aids are legally responsible
for controlling their vehicles at all times, and yet these same aids
may reduce attention and ability to react in an emergency
situation---the other side of reducing fatigue.
Traction and stability control are now required for all cars in the US
market,\cite{???-brookings} while drivers' identities still modulate the
extent to which such
advancements are seen as valuable, or how often the features get
turned off in day-to-day use. 

While the AF 447 example shows the
perils of human interaction with automated systems, an issue which will
affect automated vehicles as well, its significance
within aircraft development does not seem to be broadly appreciated
outside of that field. It gets taken---including by one of the
automated vehicle researchers I interviewed---as meaning that human
interaction should \emph{never} be expected, and that until automated
vehicles can be fully self-driving in all circumstances, they will not
be sufficiently safe. The risks of a hand-over of control at high
speed would negate, by this reasoning, the benefits of computer
control on the highway. But aircraft companies seem to be learning a
different lesson from the same incidents. Boeing and Airbus, the two
most prominent manufacturers of commercial airliners, are stepping
back from complete automation, while increasing the computerization of
their cockpits through digital displays and tools to assist pilots in
the task of flying\cite{???}. The focus is on adaptive or adaptible automation
rather than complete computer control. Both types of systems allow for
balancing the operator's load---adaptive systems automatically,
adaptible systems by operator request---taking on tasks during
high-stress, busy situations, but handing back tasks during periods of
limited load in order to keep the operator informed of and engaged in
the operation of the system\cite{???}. Such systems have their own
engineering challenges, but represent a very different answer to the
question ``what should the role of pilots be?'' than does the further
complete automation of aircraft operations. Sustained human engagement
with highly-automated systems is possible, but has to be designed into
the system from the beginning. Asking people to be mere machine
tenders, present only to ensure the continued operation of the
machinery, is indeed untenable, as it tends to produce boredom,
inattention, and the risk of catastrophic failures like the AF 447
crash. However, joint human-machine systems research provides avenues
to engage operators in the operation of the system in ways that
increase overall system safety, without necessitating that the human
be entirely eliminated. There is certainly a difference in the
professionalization of pilots compared with the ordinary status of an
everyday driver, and this difference in status and roles certainly
changes the amount of training we expect operators to have and the
sorts of interactions that may appeal to them, but there is no
fundamental reason why human-supervisory control approaches and
experience are not applicable to automated vehicles, if appropriately
moderated for the role and status of the driver. 

\subsection{Whither Alternate Narratives?}

While there is a perhaps pleasing sort of narrative arc to the idea
that complete and total automation is the ultimate end product of
technological development---restrained only when nasty luddites, backwards lawyers, and
risk-averse policy makers get in the way of natural progress---this
simplistic story of automation
teleology is not supported by history. While what has happened in the
past is not destined to repeat, and the regime of ground vehicle
automation could come to be very different from that of aerospace,
this history at least needs to be taken seriously as evidence for what
can and may occur. There are lessons here we cannot afford to forget.
While it may be just as difficult, or even more difficult, to engineer
an appropriate joint human-machine system that takes the capabilities
and desires of people seriously, rather than trying to minimize human
involvement to the greatest extent possible, that approach might
result in vehicles that 
come to be more socially acceptable in certain ways---notably, in
terms of autonomy and ethics. Furthermore, as I have stated
repeatedly, complete autonomy is fundamentally a chimera.

%% 3) situate the moonshot approach (which may actually be easier in some
%% senses but less socially acceptable) and the mixed approach in a way
%% that does not support a teleology of autonomous vehicle development
%% (think Sheridan's graph of telerobotics, not the SAE's 5 stages) (4 p)
%% --in historical context of previous systems that are wholly or
%% partially autonomous (4 p)

%%Defense Science board report (DSB)
NASA, and the Defense Science Board in their report \emph{The Role of
Autonomy in DoD Systems}, describe the proliferation of attempts at
automation taxonomies as developing out of Sheridan's work, the very
book cited above as the bible for human supervisory control.\cite{???}
However, the DSB takes a far more nuanced position than many do,
perhaps do to many years worth of accumulated experience within the
military in terms of how to understand and apply autonomy in practical
systems. The DSB
identifies that these levels formulations are ``often incorrectly
interpreted as implying that autonomy is simply a delegation of a
complete task to a computer, that a vehicle operates at a single level
of autonomy and that these levels are discrete and represent scaffolds
of increasing difficulty,''\cite{???} all interpretations made by both the
NHTSA and SAE taxonomies that may be expected to guide self-driving
vehicle research and policy, and the very interpretations I have been
attempting to critique. Instead, autonomy in practice generally
involves sub-tasks, and novel mixes of human and computational effort,
not the wholesale transfer of a complete task or process. Levels
formulations lose sight of the fact that autonomy and control are
already fractured and contingent:  what ``level'' is my vehicle
operating in when I drive an automatic transmission in semi-manual
mode, or engage cruise control, or allow a turn signal stalk to
automatically return to neutral after a turn of the wheel is detected,
or stand on the brake pedal to engage ABS in an attempt to bring the
vehicle to a stop as quickly as I have requested. These currently
everyday situations already defy levels, and represent overlapping, shifting
delegations of authority that may change on a second-by-second basis.
I also claim, to add to their critique, that the
levels seem to also suggest scaffolds of increasing desirability,
which, as we have seen in air and space flight, is a matter of perspective.

In addition, the DSB highlights two main negative consequences of the popularity of
levels of autonomy: that it ``deflects focus from the fact that all
autonomous systems are joint human-machine cognitive systems''\cite{???} and
that it ``reinforces fears about unbounded autonomy'' while obscuring
the fact that no systems are fully autonomous\cite{???}. Humans are always
involved somewhere along the line, in their programming, production,
and use, though it makes sense that the military, focused as it is on
human command hierarchies, would find it especially important to make
this apparent. Even so-called ``full autonomy'' is not likely to be so
simply full. At the very least, a human-rated system will need to have
a stop or abort button. And having such an option implies some human
oversight, which means a number of additional questions soon arise:
How much monitoring? When does it happen? And how is it
regulated or supported by the device? We should recognize that this
monitoring is a valid and valuable component of vehicle operations,
rather than discount it as a stopgap measure. New tools and interfaces
may be necessary to facilitate this process, to allow for the human to
take on new roles in the human-machine relationship: currently, engineers
co-piloting Google's driverless vehicles sit with laptops to monitor
the vehicle telemetry\cite{???-article-from-DM-class}. Data displays
like this, or like the pretty LIDAR visualizations that get used to
explain and dramatize sensor data, could be needed to form the core of the
instrumentation for the automated vehicle. More discussion should be
happening around the dashboard of the future; in the dominant
narrative of the driverless vehicle, the dashboard is vestigial, but
it could be a vibrant space presenting unparalleled new possibilities
for human-machine interaction.

Attention, and how to preserve, regulate, monitor, and transact it,
must be at the center of vehicle development. And the parameters of
these interactions are in part defined by the end goal. Much popular
discussion centers around texting as a need for automated cars. But as
David Mindell describes,``wanting to text is different
from needing a fully automated vehicle,''\footnote{David Mindell,
  discussion with the author, September 8, 2014} and it seems quite possible
to create a vehicle that will allow distractions on the order of the
time span of writing and sending a text even if it is not feasible, or
not culturally acceptable, to build one that is fully automated. In
all this, we should avoid the clich\'{e}d question ``People: sinners
or saints?'' which seems so popular in media discussions: it represents
a ``false position'' that does not adequately reflect the complex
nature of operating automated
systems\cite[p. ??]{???-woods\&hollnagel-JCS-ch1}. As operators, we are
sources of both success and failure. But ultimate, as Woods and
Hollnagel point out, all cognitive systems are finite and will
therefore also have errors, lapses, and
failures\cite[p. ??]{???-woodshollnagel}. Success in automation is not
a given that the human presence degrades,\footnote{And it is not fair
  to say that increasing automation necessarily decreases the
  importance of the human. The NRC (National
  Research Council) report on aviation, claims the following: 
``For example, antilock braking systems and airbag systems on cars are
fully automatic, deciding on their own when to act. However, fully
automatic systems as well as fully
autonomous systems depend on humans to define and limit the scope of
their authority and the range of possible actions. Movement along the
continuum typically does not eliminate or diminish the importance
of the humans to the operations of the system, but it does change
their role. In fact, the human’s role
becomes more rather than less important when moving toward the
autonomous end of the spectrum
because it is so important to assure that the systems are
properly designed, tested, deployed, and monitored to ensure the
aircraft’s continued airworthiness''\cite[p. 14-15]{???-NRCaviation}}
but something to be  
achieved through careful engineering of automated components and their
human interactions. 


We should return here to the myth of the ``personless'' factory or the
self-teaching computer program: the ``autonomous'' vehicle is a
similar sort of myth; always a 
product of people, responsible to people, and involving people,
however deep in the margins of our vision. Writing this off as
anomalous or a failure, and focusing on ``autonomy'' or ``self-driving,''
obscures real complexities of operation that encounters with the world
will inevitably involve. The important question is when/how are people
involved, and this should be empirically not ideologically driven.
Perhaps there is hope in this altered narrative to ameliorate some of
the issues facing automated vehicles today. 

It seems highly unlikely that a supervisory control perspective will
fix the potential contribution of automated vehicles to the
surveillance state:  data gathering technologies will still be
available, and comprehensive mapping may still be necessary for
operations. In any case, the potential still exists to use hardware
and software to try to restrain and control people's actions and
movements. Supervisory control will also certainly not fix the
reliance on projections of risk for decision making about vehicles:
while reducing certain risks, this approach may increase others, and
any estimates of such risk will still be estimates only, derived from
models that may be incomplete.

However, human supervisory control stands to alter the dynamics of use
and therefore change the sort of urban planning necessary to account
for these vehicles, as well as the environmental effects of their use.
Systems designed with supervision in mind may operate in a different
way than those designed primarily with autonomy\footnote{This is a
  tricky distinction. In this case
  I mean autonomy over long timescales, and, as major companies seem
  to be working on, autonomy from the human in the vehicle. Even an
  ``autonomous'' car will be supervised at some level, but the
  question here is on what timescale the human is involved in
  monitoring vehicle actions. The ``autonomous'' model really means very
  long timescale supervision, such as from a remote data center. In
  the supervisory control case I am considering here, though such
  supervision could come remotely, it would be more frequent and more
  deeply involved in the vehicle's actions---a military UAV operator
  would be one example of this, or a supervisory operator within the
  vehicle.} in mind. Operational
differences will change the behaviors favored by these systems:  Can
they operate on their own and drive to remote parking garages? Or be
sent to pick up children from school? How much attention is required
to operate the vehicle, and if that attention comes from the human
within the vehicle, how will that affect how likely people are to
drive rather than take other forms of transport that could reduce,
rather than increase, road congestion? 

But where supervisory control considerations come to totally change
the landscape is in the field of ethics. The ethical operation of
vehicles involving computer code will always depend to an extent on
that code, and the previous discussion about programmatic ethics still
holds. But what changes here is that the vehicle is no longer free to
operate based only on its programmed ethics for long periods of time.
Instead, the human operator is involved in decision making processes
at a much more granular level. Will some emergency situations likely
be handled in a largely automated fashion, just as today's pedestrian
detection systems can perform an emergency stop to avoid pedestrians a
driver does not see? Almost certainly. But these issues, though
perhaps ethically tricky, are much closer to situations we are already
working through. While one positive effect of the autonomous vehicle
might be to estrange us from vehicle operations, and thereby draw us
to consider more seriously the ethics programmed into software
systems, the effect in the dominant narrative seems to be drawing a
clear distinction between current and future automation systems:  in
the NHTSA's policy document, current automation systems like ABS,
traction control, and stability control can override the human, but
``driverless'' systems must have overrides that can be used at any
time\cite[p. ??]{NHTSA}. This distinction is artificial. However,
consistent human involvement in vehicle operations seems to open new
doors for human acceptance, by putting these systems closer to what we
already know. What we should do is consider carefully why we feel that
ABS and traction control can be allowed to override human inputs, and
what separates them from systems that should not be allowed to do so, and
keep this in mind as we continue to develop more complex automation systems.

Additionally, supervisory control presents the possibility for a renegotiation of car
culture and its troubled relationship with the automated vehicle. In
my interviews and research work at conferences, I noted especially the
contradictions within common characterizations of human
drivers and their future. One interviewee basically asserted that
humans are bad drivers and that perhaps we should ``all have our
licenses taken away,'' but later stated that ``car nuts'' should
welcome self-driving cars because they will free up the roads for
enthusiast use. This combination does not actually make sense. The
question of whether people will be allowed to drive also loomed large
in industry conferences, with further comments that don't seem to hold
together: Daniela Rus, a computer science researcher, stated at an MIT
conference that people will still be able to drive normally, sharing
the roads with automated vehicles, in response to concerns about being
forced to use automated vehicles. She is most likely correct, but this
does little to allay fears in the face of the statistic that prompted
the question: that 90 percent of accidents are caused by human
error.\footnote{FIND ESTIMATES OF THIS AND COMMENT ON ACCURACY} While
human driving isn't going away in the near future, the idea that
autonomous vehicles will free up the roads for people who want to
drive manually seems like a platitude. 

-these questions are deeply embedded in the idea of progress: does
-greater safety imply taking licenses away? a question that really
-only makes sense with a particular vision of autonomy (perhaps a
-better question is what sort of licensing/training is necessary)
-->difference in airlines (professional) and cars (non-professional)
BUT people are licensed to drive: no reason to say new training may
-not be necessary or good; or that some continuing education would not
-be a net good thing
--perhaps a middle ground here in the hybrid narrative; future cars
may be vulnerable to the same critiques as the Nissan GT-R, but still
provide a role for the human and a place for enthusiasts who want to
be involved in vehicle operations

In the end, whether you are an operator in a vehicle trying to monitor that you
are on the right path, and not about to be killed by an undetected
obstacle; or an operator waiting
for their car to come pick them up, who wants to ensure that it is en route
and not in an accident; or an operator and system-technician at a
multinational corporation administering a fleet of automated vehicles,
attempting to ensure they are not being 
hijacked or stolen, the situation you are in is one of supervisory
control. Not recognizing it as such does not make it not so, it only means we are
likely to neglect the most important parts of the system design---in
terms of its adoption and long-term use---by blindly following a
narrative based on a warped view of history. And we are likely to
ignore new skills and competencies that may be important for
operation (or even vital to operation, and perhaps which ought to be
licensed and required), or which may be valuable in other ways, for
human reasons. Cars---and transportation systems as a whole---are
designed to serve human needs, and must ultimately answer to those
needs, even those that might seem, to some, to be mere ethical squeamishness.


