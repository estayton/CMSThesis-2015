\chapter{Where To?}

5. Where to?
-new epistemology?
-orienting toward the right questions?
-???

Impacts / New Epistemologies (21 p)

Somewhere in here:
bring in the qualitative observations of actually driving, the
components we interact with on a daily basis
Our cars are already complex human/machine systems, and it is time our
viewpoint shifts to begin to recognize this fact; this is not a move
from total human operation to total machine operation, and therefore
from total human agency to total machine agency, but a change in the
blend of capacities and the technologies involved in getting there.



>>>This is where I weave in automotive history 
>>>Return to streetcar and the re-shaping of public space: 

1) process by which new technologies become entwined with new legal
principles; new epistemologies?
-historical example (3 p)
-what is the new epistemology needed to make sense of devices like
this in the everyday (because we haven't seen them outside of extreme
environments, except in very weak forms)
-the street stands to be re-shaped again, subtly, by the capabilities
of our vehicles (1 p)
-this is the place to bring together the AI, Human Factors,
Law/Policy, and STS/Anthro background (specifically on technology \&
people, edges of HCI, Andy Clark on natural-born cyborgs) to paint the
whole picture of the forces in play (2 p, v. philosophical)

2) VERY different potential impacts if we are looking at fully
autonomous vs. partially autonomous 
--in terms of human role and our understanding of our own role
----referring back to HF/SC (3 p)
--in terms of how the road itself may have to change (not going to
spend a lot of time on this, but it is worth at least a couple
paragraphs) (2 p)
--in terms of legal framework
----it is important to note that ``responsibility'' is not binary
(which is made clear in these examples), and torts may be distributed
across human and nonhuman actors (operator, owner, seller,
manufacturer, distributors) but distribution of torts has an ethical
and ideological stance (2 p)
>>>>BUT: problems with the liability framework: expensive for the
litigant to get expert witnesses; harder to determine whose fault it
is and the human tends to get scapegoated (see industrial accidents,
TMI, or oil rig disasters) (3 p)
>>>>>What can we actually ask the human to do and hold them to task
for? (3 p)
---important to note that issues of litigation for full autonomy are
not even part of the debate yet, we just aren't at the stage where it
is worth discussing (2 p)

--------------------

Resisting Teleology / Conclusion (14 p)

1) what should be clear from the above, but must be made clear in
public discourse, is that human->hybrid->autonomous is not inevitable,
it is not a requirement, it is conditional on our acceptance AS WELL
as on technological progress (3 p)
--the hybrid system is a very difficult engineering problem, but this
is valuable for systems that are tasked with keeping us safe (see
Apollo etc.); especially as we will use them every day and we cannot
afford to alienate ourselves from them (4 p)
-This discussion has tried to frame the invisible history, the
ideologies flowing into these systems, the contingency of the plans
--and contrast with the simplistic depictions provided to most via
media
---which form the visible picture as of today, for MOST people (1 p)
>>> possibly split up <<<




Conclusion:
The pro-side: millions of deaths if we don't do something now, so we
have to accept these
The con-side: (regulator) current deaths not on my hands, but if one
little girl is killed by a robot I've lost my job

But the statistics are matters of faith. Nobody has the answer, they
can just point to 90\% of accidents involving people. Computers can fix
                     some, will cause other problems. I believe
                     computers can do better than us, but it is still
                     just guesses at future stats. So this is about
                     stories and faith as much as math.

Our choice shouldn't be to accept this driverless vision or take
responsibility for the deaths of thousands. That's abusive, coercive,
and deeply wrong. This isn't a question of whether fully automated
cars come to exist, even as noe envisioned. I am not saying this dream
is possible or impossible. But there are other choices to be made!

The choice should be between options, between visions, choices of how
we integrate computers to make our driving safer. The google model,
this model, the other model, or innumerable newly imagined
possibilities. NOT a take it or leave it; not the choice of the google
car or a big fuck you to all the prospective road fatalities. That is
no choice. There is more on heaven and earth than is dreamt of in this
philosophy. The real question we need to ask is what form the tech
should take, and what benefits and acceptable sacrifices adhere to
that choice.

(Evan, on Hippocratic oath: why is it ``do no harm'' not ``save more
lives''?
there may be hidden consequences to a philosophy of saving more lives)


EDIT THE BELOW

2) wrap up with the big questions we are faced with, that we have to
grapple with instead of accepting teleology/common media framings: (1
p)
--1) what is the appropriate role for the human in the human/machine
system? (1 p)
--2) what is the appropriate role of statistics and risk in governing
policy on the issue? can you opt-out? (0.5 p)
--3) how safe is safe enough? for whom? and how is this regulated? (1
p)
--4) how do we feel about AI systems that cannot really be said to
``think'', ``know'', or ``understand'' but which nevertheless ``hold
our lives in their hands'' (0.5 p)
----This may well imply we need the human involved, for that very
reason
----and we should KEEP the human involved as a matter of design
principle (2 p)

