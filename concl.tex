\chapter{Conclusion: Driving the Future}

%%Resisting Teleology / Conclusion (14 p)

1) what should be clear from the above, but must be made clear in
public discourse, is that human->hybrid->autonomous is not inevitable,
it is not a requirement, it is conditional on our acceptance AS WELL
as on technological progress (3 p)
--the hybrid system is a very difficult engineering problem, but this
is valuable for systems that are tasked with keeping us safe (see
Apollo etc.); especially as we will use them every day and we cannot
afford to alienate ourselves from them (4 p)
-This discussion has tried to frame the invisible history, the
ideologies flowing into these systems, the contingency of the plans
--and contrast with the simplistic depictions provided to most via
media
---which form the visible picture as of today, for MOST people (1 p)
>>> possibly split up <<<


Conclusion:
Can't get positive social effects by just adding autonomy!
2) VERY different potential impacts if we are looking at fully
autonomous vs. partially autonomous 

The pro-side: millions of deaths if we don't do something now, so we
have to accept these
The con-side: (regulator) current deaths not on my hands, but if one
little girl is killed by a robot I've lost my job

But the statistics are matters of faith. Nobody has the answer, they
can just point to 90\% of accidents involving people. Computers can fix
                     some, will cause other problems. I believe
                     computers can do better than us, but it is still
                     just guesses at future stats. So this is about
                     stories and faith as much as math.

Our choice shouldn't be to accept this driverless vision or take
responsibility for the deaths of thousands. That's abusive, coercive,
and deeply wrong. This isn't a question of whether fully automated
cars come to exist, even as noe envisioned. I am not saying this dream
is possible or impossible. But there are other choices to be made!

The choice should be between options, between visions, choices of how
we integrate computers to make our driving safer. The google model,
this model, the other model, or innumerable newly imagined
possibilities. NOT a take it or leave it; not the choice of the google
car or a big fuck you to all the prospective road fatalities. That is
no choice. There is more on heaven and earth than is dreamt of in this
philosophy. The real question we need to ask is what form the tech
should take, and what benefits and acceptable sacrifices adhere to
that choice.

(Evan, on Hippocratic oath: why is it ``do no harm'' not ``save more
lives''?
there may be hidden consequences to a philosophy of saving more lives)

{Dumit, companies unlikely to back a system that will result in fewer
  cars on the road; just against their economic interests)


EDIT THE BELOW

2) wrap up with the big questions we are faced with, that we have to
grapple with instead of accepting teleology/common media framings: (1
p)
--1) what is the appropriate role for the human in the human/machine
system? (1 p) (and what skills do they need to acquire)
--2) what is the appropriate role of statistics and risk in governing
policy on the issue? can you opt-out? (0.5 p)
--3) how safe is safe enough? for whom? and how is this regulated? (1
p)
--4) how do we feel about AI systems that cannot really be said to
``think'', ``know'', or ``understand'' but which nevertheless ``hold
our lives in their hands'' (0.5 p)
----This may well imply we need the human involved, for that very
reason
----and we should KEEP the human involved as a matter of design
principle (2 p)

